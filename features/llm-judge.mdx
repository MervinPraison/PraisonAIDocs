---
title: "LLM as Judge"
description: "Evaluate agent performance with AI-powered analysis for context, memory, and knowledge utilization"
icon: "gavel"
---

## Overview

LLM as Judge evaluates your agent workflows using AI to analyze performance across three dimensions: **context flow**, **memory utilization**, and **knowledge retrieval**.

```mermaid
flowchart LR
    A[ðŸš€ Recipe Run] -->|--save| B[ðŸ“„ Trace File]
    B --> C{ðŸ” Judge Mode}
    C -->|--context| D[Context Analysis]
    C -->|--memory| E[Memory Analysis]
    C -->|--knowledge| F[Knowledge Analysis]
    D --> G[ðŸ“Š Judge Report]
    E --> G
    F --> G
    G --> H[ðŸ“‹ Plan File]
    H -->|apply| I[âœ… Improved YAML]
    
    style A fill:#4CAF50,color:#fff
    style G fill:#2196F3,color:#fff
    style I fill:#9C27B0,color:#fff
```

## Evaluation Modes

<CardGroup cols={3}>
  <Card title="Context Mode" icon="arrows-rotate">
    Evaluates how context flows between agents in multi-agent workflows
  </Card>
  <Card title="Memory Mode" icon="brain">
    Evaluates how agents store and retrieve memories effectively
  </Card>
  <Card title="Knowledge Mode" icon="book">
    Evaluates how agents search and use knowledge bases
  </Card>
</CardGroup>

## Quick Start

<Steps>
  <Step title="Run with Trace">
    ```bash
    praisonai recipe run my-recipe --save --name my-trace
    ```
  </Step>
  <Step title="Judge the Trace">
    ```bash
    # Context evaluation (default)
    praisonai recipe judge my-trace --yaml agents.yaml

    # Memory evaluation
    praisonai recipe judge my-trace --memory

    # Knowledge evaluation
    praisonai recipe judge my-trace --knowledge
    ```
  </Step>
  <Step title="Apply Improvements">
    ```bash
    praisonai recipe apply judge_plan.yaml --confirm
    ```
  </Step>
</Steps>

## Context Evaluation Criteria

The default **context mode** evaluates each agent on **6 criteria**:

| Criterion | Score | Description |
|-----------|-------|-------------|
| **Task Achievement** | 1-10 | Did the agent accomplish its goal? |
| **Context Utilization** | 1-10 | Was the context effectively used? |
| **Output Quality** | 1-10 | Is the output high quality and relevant? |
| **Instruction Following** | 1-10 | Did the agent follow specific instructions? |
| **Hallucination** | 1-10 | 10=no fabrication, 1=severe hallucination |
| **Error Handling** | 1-10 | How well did the agent handle errors? |

## Memory Evaluation Criteria

The **memory mode** (`--memory`) evaluates how agents use memory:

```mermaid
flowchart LR
    subgraph Memory Operations
        S[ðŸ§  Store] --> DB[(Memory DB)]
        DB --> R[ðŸ” Search]
    end
    
    subgraph Evaluation
        R --> E1[Retrieval Relevance]
        S --> E2[Storage Quality]
        R --> E3[Recall Effectiveness]
        S --> E4[Memory Efficiency]
    end
    
    style S fill:#4CAF50,color:#fff
    style R fill:#2196F3,color:#fff
```

| Criterion | Score | Description |
|-----------|-------|-------------|
| **Retrieval Relevance** | 1-10 | Did the agent search for relevant memories? |
| **Storage Quality** | 1-10 | Did the agent store useful information? |
| **Recall Effectiveness** | 1-10 | Was retrieved memory used in the response? |
| **Memory Efficiency** | 1-10 | No redundant stores/searches? |

## Knowledge Evaluation Criteria

The **knowledge mode** (`--knowledge`) evaluates RAG and knowledge retrieval:

```mermaid
flowchart LR
    subgraph Knowledge Operations
        Q[â“ Query] --> K[(Knowledge Base)]
        K --> D[ðŸ“„ Documents]
    end
    
    subgraph Evaluation
        Q --> E1[Retrieval Accuracy]
        D --> E2[Source Coverage]
        D --> E3[Citation Quality]
        D --> E4[Knowledge Integration]
    end
    
    style Q fill:#9C27B0,color:#fff
    style D fill:#FF9800,color:#fff
```

| Criterion | Score | Description |
|-----------|-------|-------------|
| **Retrieval Accuracy** | 1-10 | Did the agent find relevant documents? |
| **Source Coverage** | 1-10 | Were all relevant sources used? |
| **Citation Quality** | 1-10 | Were sources properly attributed? |
| **Knowledge Integration** | 1-10 | Was knowledge well-integrated? |

## How It Works

```mermaid
flowchart TB
    subgraph Input
        T[Trace Events]
        Y[YAML Config]
    end
    
    subgraph Analysis
        E[Extract Agent Data]
        TC[Tool Call Analysis]
        CF[Context Flow Analysis]
        CL[Content Loss Detection]
    end
    
    subgraph Evaluation
        LLM[LLM Judge]
        S[Score Calculation]
    end
    
    subgraph Output
        R[Judge Report]
        P[Plan File]
        REC[Recommendations]
    end
    
    T --> E
    Y --> E
    E --> TC
    E --> CF
    E --> CL
    TC --> LLM
    CF --> LLM
    CL --> LLM
    LLM --> S
    S --> R
    R --> P
    R --> REC
```

## CLI Commands

### Judge a Trace

```bash
praisonai recipe judge <trace-id> [OPTIONS]
```

**Options:**
| Option | Description |
|--------|-------------|
| `--yaml FILE` | YAML file for context-aware evaluation |
| `--output FILE` | Save plan to specific file |
| `--model MODEL` | LLM model for judging (default: gpt-4o-mini) |
| `--context` | Evaluate context flow between agents (default) |
| `--memory` | Evaluate memory utilization |
| `--knowledge` | Evaluate knowledge retrieval |

### Apply Improvements

```bash
praisonai recipe apply <plan-file> [OPTIONS]
```

**Options:**
| Option | Description |
|--------|-------------|
| `--confirm` / `-c` | Apply without confirmation |
| `--dry-run` | Preview changes without applying |
| `--fix-ids IDS` | Apply specific fixes only |
| `--no-backup` | Skip creating backup |

## Example Output

### Judge Report

```bash
praisonai recipe judge run-abc123 --yaml agents.yaml
```

```
============================================================
  LLM JUDGE REPORT: run-abc123
============================================================

  Timestamp: 2026-01-24T06:01:33
  Agents Evaluated: 3
  Overall Score: 7.86/10

  AGENT SCORES:
    Research Agent:
      Task Achievement: 8.0/10
      Context Utilization: 7.0/10
      Output Quality: 8.0/10
      Instruction Following: 9.0/10
      Hallucination: 8.0/10
      Error Handling: 10.0/10
      Overall: 8.33/10
      Tool Calls (1):
        - tavily_search: completeness=9.0/10

    Writer Agent:
      Task Achievement: 8.0/10
      Context Utilization: 9.0/10
      Output Quality: 8.0/10
      Instruction Following: 9.0/10
      Hallucination: 9.0/10
      Error Handling: 10.0/10
      Overall: 8.83/10

  CONTEXT FLOW:
    Research Agent â†’ Writer Agent: 10.0/10 âœ“

  RECOMMENDATIONS:
    - Research Agent: Include more specific citations
    - Writer Agent: Ensure all sections are complete

============================================================
```

### Plan Preview

```bash
praisonai recipe apply judge_plan.yaml --dry-run
```

```
============================================================
  PREVIEW: Changes to be applied
============================================================
  File: agents.yaml
  Fixes: 4

  [MEDIUM] fix_abc123
    Agent: Research Agent
    Type: append_instruction
    Path: agents.Research Agent.instructions
    New: IMPROVEMENT: Include specific citations...

  [LOW] fix_def456
    Agent: Writer Agent
    Type: append_instruction
    Path: agents.Writer Agent.instructions
    New: IMPROVEMENT: Ensure all sections complete...

============================================================
ðŸ” Dry run - no changes applied
```

## Tool Evaluation

Each tool call is evaluated for:

- **Input Quality** - Was the input well-formed?
- **Output Utilization** - Was the output fully used?
- **Result Completeness** - Was the full result captured?
- **Error Detection** - Were there any errors?

```
Tool Calls (2):
  - tavily_search: completeness=9.0/10
  - create_wp_post: completeness=8.0/10
    Issues: unexpected keyword argument
```

## Context Flow Analysis

The judge tracks how context flows between agents:

```mermaid
flowchart LR
    A[Agent 1] -->|"Output: keywords"| B[Agent 2]
    B -->|"Output: topics"| C[Agent 3]
    
    style A fill:#90EE90
    style B fill:#90EE90
    style C fill:#90EE90
```

**Scores:**
- `10/10 âœ“` - Full context passed
- `5-9/10` - Partial context
- `<5/10 âš ï¸` - Content loss detected

## Content Loss Detection

The judge detects when important content is lost:

```
âš ï¸  CONTENT LOSS DETECTED:
  - Tool 'tavily_search' result was truncated
  - LLM response was truncated for agent 'Writer'
```

## Fix Types

The judge generates different fix types:

| Fix Type | Description |
|----------|-------------|
| `append_instruction` | Add guidance to existing instructions |
| `add_expected_output` | Specify expected output format |
| `modify_context_config` | Adjust context settings |
| `suggestion` | General improvement suggestion |

## Best Practices

<CardGroup cols={2}>
  <Card title="Run Multiple Times" icon="repeat">
    Judge the same trace multiple times to get consistent scores
  </Card>
  <Card title="Use YAML Context" icon="file-code">
    Always provide `--yaml` for context-aware evaluation
  </Card>
  <Card title="Review Before Apply" icon="eye">
    Use `--dry-run` to preview changes before applying
  </Card>
  <Card title="Backup Your Files" icon="copy">
    The apply command creates backups by default
  </Card>
</CardGroup>

## Integration with Recipes

```bash
# Full workflow: run â†’ judge â†’ apply
praisonai recipe run my-recipe --save --name test-run
praisonai recipe judge test-run --yaml agents.yaml --output plan.yaml
praisonai recipe apply plan.yaml --dry-run
praisonai recipe apply plan.yaml --confirm
```

## Programmatic Usage

<CodeGroup>
```python Context Mode
from praisonai.replay import ContextTraceReader, ContextEffectivenessJudge
from praisonai.replay.judge import format_judge_report

# Load trace
reader = ContextTraceReader()
events = reader.read_trace("my-trace")

# Judge with context mode (default)
judge = ContextEffectivenessJudge(mode="context")
report = judge.judge_trace(events, session_id="my-trace")
print(format_judge_report(report))
```

```python Memory Mode
from praisonai.replay import ContextTraceReader, ContextEffectivenessJudge

# Load trace
reader = ContextTraceReader()
events = reader.read_trace("my-trace")

# Judge with memory mode
judge = ContextEffectivenessJudge(mode="memory")
report = judge.judge_trace(events, session_id="my-trace")
print(f"Memory Score: {report.overall_score}/10")
```

```python Knowledge Mode
from praisonai.replay import ContextTraceReader, ContextEffectivenessJudge

# Load trace
reader = ContextTraceReader()
events = reader.read_trace("my-trace")

# Judge with knowledge mode
judge = ContextEffectivenessJudge(mode="knowledge")
report = judge.judge_trace(events, session_id="my-trace")
print(f"Knowledge Score: {report.overall_score}/10")
```
</CodeGroup>
