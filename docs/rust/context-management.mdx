---
title: "Context Management"
sidebarTitle: "Context"
description: "Token budgeting and context optimization in the PraisonAI Rust SDK"
icon: "chart-pie"
---

# Context Management

The Context module provides token estimation, budget allocation, and context optimization for managing LLM context windows.

## Quick Start

```rust
use praisonai::context::{ContextManager, ContextConfig, OptimizerStrategy};

let config = ContextConfig::new()
    .model("gpt-4")
    .max_tokens(8192)
    .strategy(OptimizerStrategy::Smart);

let manager = ContextManager::new(config);

// Build context
let context = manager.compose(messages, tools, system_prompt)?;
println!("Total tokens: {}", context.total_tokens);
```

## OptimizerStrategy

Strategies for handling context overflow:

| Strategy | Description |
|----------|-------------|
| `Truncate` | Remove oldest messages |
| `SlidingWindow` | Keep most recent N messages |
| `Summarize` | Summarize older messages |
| `PruneTools` | Remove tool-related messages first |
| `NonDestructive` | Fail if over limit |
| `Smart` | Intelligent combination (default) |

```rust
use praisonai::context::OptimizerStrategy;

let strategy = OptimizerStrategy::Smart;
let truncate = OptimizerStrategy::Truncate;
let window = OptimizerStrategy::SlidingWindow;
```

## ContextSegment

A segment of context with token count:

```rust
use praisonai::context::ContextSegment;

let segment = ContextSegment::new("system_prompt", 150)
    .priority(10)  // Higher = more important
    .content("You are a helpful assistant...");

println!("Name: {}", segment.name);
println!("Tokens: {}", segment.tokens);
println!("Priority: {}", segment.priority);
```

## ContextLedger

Track token usage across segments:

```rust
use praisonai::context::{ContextLedger, ContextSegment};

let mut ledger = ContextLedger::new(8192);

// Add segments
ledger.add(ContextSegment::new("system", 200));
ledger.add(ContextSegment::new("history", 3000));
ledger.add(ContextSegment::new("tools", 500));

// Check budget
println!("Used: {}", ledger.total_tokens);
println!("Remaining: {}", ledger.remaining());
println!("Utilization: {:.1}%", ledger.utilization() * 100.0);
println!("Over budget: {}", ledger.is_over_budget());
```

## BudgetAllocation

Allocate token budget across components:

```rust
use praisonai::context::BudgetAllocation;

// Default 60-30-10 split
let budget = BudgetAllocation::new(8192);
println!("System: {}", budget.system_budget);
println!("History: {}", budget.history_budget);
println!("Tools: {}", budget.tools_budget);

// Custom ratios
let custom = BudgetAllocation::with_ratios(
    8192,
    0.2,  // 20% system
    0.6,  // 60% history
    0.2   // 20% tools
);
```

## ContextConfig

Configuration for context management:

```rust
use praisonai::context::{ContextConfig, OptimizerStrategy};

let config = ContextConfig::new()
    .model("gpt-4")              // Model name
    .max_tokens(8192)            // Max context tokens
    .strategy(OptimizerStrategy::Smart)
    .with_monitoring()           // Enable stats
    .output_reserve_pct(0.2);    // Reserve 20% for output

println!("Model: {}", config.model);
println!("Max tokens: {}", config.max_tokens);
println!("Output reserve: {}", config.output_reserve_pct);
```

### Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `model` | String | "gpt-4" | Model name for token estimation |
| `max_tokens` | usize | 8192 | Maximum context window |
| `strategy` | OptimizerStrategy | Smart | Overflow strategy |
| `monitoring` | bool | false | Enable usage stats |
| `output_reserve_pct` | f64 | 0.15 | Reserve for output |

## ContextManager

Central manager for context operations:

```rust
use praisonai::context::{ContextManager, ContextConfig, ContextSegment};

let config = ContextConfig::new().max_tokens(4096);
let manager = ContextManager::new(config);

// Estimate tokens
let tokens = manager.estimate_tokens("Hello, world!");
println!("Estimated: {}", tokens);

// Check if fits
let fits = manager.fits_in_context(&messages, 1000)?;
println!("Fits: {}", fits);

// Optimize messages
let optimized = manager.optimize(messages)?;
println!("Optimized to {} messages", optimized.len());

// Get context stats
let stats = manager.get_stats();
println!("Total contexts: {}", stats.total_contexts);
println!("Optimizations: {}", stats.optimization_count);
```

### Token Estimation

```rust
use praisonai::context::ContextManager;

let manager = ContextManager::default();

// Estimate single text
let tokens = manager.estimate_tokens("Hello, how are you?");

// Estimate messages
let msg_tokens = manager.estimate_message_tokens(&messages);

// Estimate tools
let tool_tokens = manager.estimate_tool_tokens(&tools);
```

### Context Composition

```rust
use praisonai::context::{ContextManager, ComposedContext};

let manager = ContextManager::new(config);

// Compose context from parts
let context: ComposedContext = manager.compose(
    messages,
    tools,
    system_prompt
)?;

println!("Total tokens: {}", context.total_tokens);
println!("Messages: {}", context.messages.len());
println!("System tokens: {}", context.system_tokens);
println!("Was optimized: {}", context.was_optimized);
```

## ContextOptimizer

Low-level optimization utilities:

```rust
use praisonai::context::{ContextOptimizer, OptimizerStrategy};

let optimizer = ContextOptimizer::new()
    .strategy(OptimizerStrategy::SlidingWindow)
    .window_size(10)  // Keep last 10 messages
    .preserve_system(true);  // Always keep system message

// Optimize
let optimized = optimizer.optimize(messages, 4000)?;
```

### Optimization Methods

| Method | Description |
|--------|-------------|
| `truncate(messages, limit)` | Remove from beginning |
| `sliding_window(messages, n)` | Keep last N messages |
| `prune_tools(messages)` | Remove tool messages first |
| `summarize(messages, limit)` | Summarize older messages |
| `smart_optimize(messages, limit)` | Intelligent combination |

## Example: Agent with Context Management

```rust
use praisonai::{Agent, Result};
use praisonai::context::{ContextManager, ContextConfig, OptimizerStrategy};

#[tokio::main]
async fn main() -> Result<()> {
    let ctx_config = ContextConfig::new()
        .model("gpt-4")
        .max_tokens(8192)
        .strategy(OptimizerStrategy::Smart)
        .output_reserve_pct(0.2);
    
    let ctx_manager = ContextManager::new(ctx_config);
    
    let agent = Agent::new()
        .instructions("You are a helpful assistant")
        .build()?;
    
    // Simulate long conversation
    let mut messages = vec![];
    for i in 0..50 {
        messages.push(format!("Message {}: Hello!", i));
    }
    
    // Check context size
    let tokens = ctx_manager.estimate_tokens(&messages.join("\n"));
    println!("Conversation tokens: {}", tokens);
    
    // Optimize if needed
    if tokens > 6000 {
        let optimized = ctx_manager.optimize_text(&messages.join("\n"), 4000)?;
        println!("Optimized to: {} tokens", ctx_manager.estimate_tokens(&optimized));
    }
    
    Ok(())
}
```

## Related

<CardGroup cols={2}>
  <Card title="Memory" icon="brain" href="/docs/rust/memory">
    Conversation memory
  </Card>
  <Card title="Agent" icon="robot" href="/docs/rust/agent">
    Agent API
  </Card>
  <Card title="Configuration" icon="gear" href="/docs/rust/configuration">
    SDK configuration
  </Card>
  <Card title="LLM" icon="microchip" href="/docs/rust/llm">
    LLM providers
  </Card>
</CardGroup>
