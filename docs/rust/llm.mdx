---
title: "LLM Providers"
sidebarTitle: "LLM"
description: "Configure and use LLM providers in the PraisonAI Rust SDK"
icon: "brain"
---

# LLM Providers

The Rust SDK provides a flexible LLM abstraction layer with support for OpenAI-compatible APIs and custom providers.

## Quick Start

```rust
use praisonai::{Agent, LlmConfig};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Uses OPENAI_API_KEY from environment
    let agent = Agent::new("assistant")
        .instructions("You are a helpful assistant")
        .build()?;
    
    let response = agent.run("Hello!").await?;
    println!("{}", response);
    Ok(())
}
```

## LlmConfig

Configure model, temperature, and other parameters:

```rust
use praisonai::LlmConfig;

let config = LlmConfig::new("gpt-4o")
    .temperature(0.7)
    .max_tokens(1000)
    .api_key("sk-...")  // Optional: defaults to OPENAI_API_KEY env var
    .base_url("https://api.openai.com/v1");  // Optional: for custom endpoints
```

### Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `model` | `String` | `"gpt-4o-mini"` | Model name (e.g., `"gpt-4o"`, `"claude-3-sonnet"`) |
| `temperature` | `f32` | `0.7` | Sampling temperature (0.0 - 2.0) |
| `max_tokens` | `Option<u32>` | `None` | Maximum tokens in response |
| `api_key` | `Option<String>` | `None` | API key (falls back to env var) |
| `base_url` | `Option<String>` | `None` | Custom API endpoint |

## Message Types

Create conversation messages with helper methods:

```rust
use praisonai::llm::{Message, Role};

// Create different message types
let system = Message::system("You are a helpful assistant");
let user = Message::user("What is Rust?");
let assistant = Message::assistant("Rust is a systems programming language...");

// Tool response message
let tool_response = Message::tool("call_123", "Search results: ...");
```

### Role Enum

```rust
pub enum Role {
    System,     // System instructions
    User,       // User input
    Assistant,  // LLM response
    Tool,       // Tool/function result
}
```

## LlmProvider Trait

Implement custom LLM providers:

```rust
use praisonai::llm::{LlmProvider, Message, LlmResponse};
use praisonai::tools::ToolDefinition;
use async_trait::async_trait;

#[async_trait]
pub trait LlmProvider: Send + Sync {
    /// Send a chat completion request
    async fn chat(
        &self,
        messages: &[Message],
        tools: Option<&[ToolDefinition]>,
    ) -> Result<LlmResponse>;
    
    /// Stream a chat completion
    async fn chat_stream(
        &self,
        messages: &[Message],
        tools: Option<&[ToolDefinition]>,
    ) -> Result<Box<dyn Stream<Item = Result<String>> + Send + Unpin>>;
    
    /// Get the model name
    fn model(&self) -> &str;
}
```

## Built-in Providers

### OpenAiProvider

Works with OpenAI and compatible APIs (Azure, Groq, Together, etc.):

```rust
use praisonai::llm::{OpenAiProvider, LlmConfig, Message, LlmProvider};

let config = LlmConfig::new("gpt-4o")
    .temperature(0.5);

let provider = OpenAiProvider::new(config);

// Use with tool support
let response = provider.chat(
    &[Message::user("Search for Rust tutorials")],
    Some(&tools),
).await?;

println!("Response: {}", response.content);
println!("Tool calls: {:?}", response.tool_calls);
```

### MockLlmProvider

For testing without API calls:

```rust
use praisonai::llm::{MockLlmProvider, ToolCall};

// Create with a preset response
let mock = MockLlmProvider::with_response("Hello from mock!");

// Or queue multiple responses
let mock = MockLlmProvider::new();
mock.add_response("First response");
mock.add_response("Second response");

// Add tool call responses
mock.add_tool_calls(vec![
    ToolCall::new("call_1", "search", r#"{"query": "rust"}"#),
]);

let response = mock.chat(&[Message::user("Search")], None).await?;
assert_eq!(response.tool_calls.len(), 1);
```

## LlmResponse

Response structure from LLM calls:

```rust
pub struct LlmResponse {
    pub content: String,              // Text response
    pub tool_calls: Vec<ToolCall>,    // Tool calls (if any)
    pub finish_reason: Option<String>, // "stop", "tool_calls", etc.
    pub usage: Option<Usage>,         // Token usage stats
}

pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}
```

## Tool Calls

Handle tool/function calls from the LLM:

```rust
use praisonai::llm::ToolCall;

let call = ToolCall::new(
    "call_abc123",
    "get_weather",
    r#"{"city": "London"}"#,
);

// Access fields
println!("Tool: {}", call.name());         // "get_weather"
println!("Args: {}", call.arguments());    // {"city": "London"}
println!("ID: {}", call.id);               // "call_abc123"
```

## Custom Endpoints

Use with OpenAI-compatible providers:

<CodeGroup>

```rust Groq
let config = LlmConfig::new("llama-3.1-70b-versatile")
    .base_url("https://api.groq.com/openai/v1")
    .api_key(std::env::var("GROQ_API_KEY")?);
```

```rust Azure OpenAI
let config = LlmConfig::new("gpt-4o")
    .base_url("https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT")
    .api_key(std::env::var("AZURE_OPENAI_API_KEY")?);
```

```rust Ollama
let config = LlmConfig::new("llama3")
    .base_url("http://localhost:11434/v1");
```

</CodeGroup>

## Related

<CardGroup cols={2}>
  <Card title="Agent" icon="robot" href="/docs/rust/agent">
    Create agents with LLM configuration
  </Card>
  <Card title="Tools" icon="wrench" href="/docs/rust/tools">
    Add tools for function calling
  </Card>
  <Card title="Memory" icon="brain" href="/docs/rust/memory">
    Add conversation memory
  </Card>
  <Card title="Failover" icon="rotate" href="/docs/rust/failover">
    Automatic provider failover
  </Card>
</CardGroup>
