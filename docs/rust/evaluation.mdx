---
title: "Evaluation"
sidebarTitle: "Evaluation"
description: "Agent evaluation and testing in the PraisonAI Rust SDK"
icon: "flask-vial"
---

# Evaluation

The Evaluation module provides comprehensive evaluation capabilities for testing AI agent performance, accuracy, and behavior.

## Quick Start

```rust
use praisonai::eval::{EvaluationScore, AccuracyEvaluator, EvaluationResult};

// Create an evaluator
let evaluator = AccuracyEvaluator::new();

// Evaluate a response
let score = evaluator.evaluate(
    "What is the capital of France?",
    "The capital of France is Paris.",
    "Paris"
)?;

println!("Score: {:.2}", score.value);
println!("Passing: {}", score.is_passing(0.8));
```

## EvaluationScore

Score from an evaluation:

```rust
use praisonai::eval::EvaluationScore;

let score = EvaluationScore::new(0.85)
    .with_reasoning("Response correctly identified the capital")
    .with_confidence(0.95);

println!("Score: {:.2}", score.value);
println!("Percentage: {:.1}%", score.as_percentage());
println!("Passing (>0.8): {}", score.is_passing(0.8));
println!("Reasoning: {:?}", score.reasoning);
println!("Confidence: {:?}", score.confidence);
```

### Score Properties

| Property | Type | Description |
|----------|------|-------------|
| `value` | f64 | Score (0.0-1.0) |
| `reasoning` | Option | Explanation |
| `confidence` | Option | Confidence level |

## PerformanceMetrics

Performance metrics from evaluation:

```rust
use praisonai::eval::PerformanceMetrics;
use std::time::Duration;

let metrics = PerformanceMetrics::new(Duration::from_millis(2500))
    .with_ttft(Duration::from_millis(800))
    .with_tokens_per_second(50.0)
    .with_tokens(100, 250);

println!("Duration: {:?}", metrics.duration);
println!("TTFT: {:?}", metrics.ttft);
println!("Tokens/sec: {:.1}", metrics.tokens_per_second.unwrap_or(0.0));
println!("Input tokens: {:?}", metrics.input_tokens);
println!("Output tokens: {:?}", metrics.output_tokens);
```

## CriteriaScore

Score for specific evaluation criteria:

```rust
use praisonai::eval::CriteriaScore;

let accuracy = CriteriaScore::new("accuracy", 0.9)
    .with_weight(1.5)
    .with_feedback("Excellent factual accuracy");

let fluency = CriteriaScore::new("fluency", 0.85)
    .with_weight(1.0)
    .with_feedback("Natural language flow");

println!("Accuracy weighted: {:.2}", accuracy.weighted_score());
println!("Fluency weighted: {:.2}", fluency.weighted_score());
```

## ToolCallResult

Result of tool call evaluation:

```rust
use praisonai::eval::ToolCallResult;

let result = ToolCallResult::new("web_search", true, true);

println!("Tool: {}", result.name);
println!("Expected: {}", result.expected);
println!("Called: {}", result.called);
println!("Correct: {}", result.is_correct());
```

## EvaluationResult

Complete evaluation result:

```rust
use praisonai::eval::{EvaluationResult, EvaluationScore, PerformanceMetrics, CriteriaScore};
use std::time::Duration;

let result = EvaluationResult::new(
    EvaluationScore::new(0.88)
)
    .with_metrics(PerformanceMetrics::new(Duration::from_secs(2)))
    .with_criteria_score(CriteriaScore::new("accuracy", 0.9))
    .with_criteria_score(CriteriaScore::new("helpfulness", 0.85))
    .with_metadata("model", serde_json::json!("gpt-4"));

println!("Overall: {:.2}", result.score.value);
println!("Criteria scores: {}", result.criteria_scores.len());
```

## Evaluators

### AccuracyEvaluator

Evaluate response accuracy:

```rust
use praisonai::eval::AccuracyEvaluator;

let evaluator = AccuracyEvaluator::new()
    .case_insensitive(true)
    .partial_match(true);

let score = evaluator.evaluate(
    "What year was Python created?",
    "Python was created in 1991 by Guido van Rossum.",
    "1991"
)?;

println!("Accuracy: {:.2}", score.value);
```

### RelevanceEvaluator

Evaluate response relevance:

```rust
use praisonai::eval::RelevanceEvaluator;

let evaluator = RelevanceEvaluator::new();

let score = evaluator.evaluate(
    "Explain machine learning",
    "Machine learning is a subset of AI that enables systems to learn from data..."
)?;

println!("Relevance: {:.2}", score.value);
```

### ToolUseEvaluator

Evaluate tool usage correctness:

```rust
use praisonai::eval::{ToolUseEvaluator, ToolCallResult};

let evaluator = ToolUseEvaluator::new();

let results = vec![
    ToolCallResult::new("calculator", true, true),
    ToolCallResult::new("web_search", false, false),
    ToolCallResult::new("code_runner", true, false),  // Should have called but didn't
];

let score = evaluator.evaluate(&results);
println!("Tool use score: {:.2}", score.value);
println!("Correct: {}/{}", 
    results.iter().filter(|r| r.is_correct()).count(),
    results.len()
);
```

### CriteriaEvaluator

Evaluate against multiple criteria:

```rust
use praisonai::eval::{CriteriaEvaluator, Criterion};

let evaluator = CriteriaEvaluator::new()
    .add_criterion(Criterion::new("accuracy")
        .weight(2.0)
        .description("Factual correctness"))
    .add_criterion(Criterion::new("completeness")
        .weight(1.5)
        .description("Covers all aspects"))
    .add_criterion(Criterion::new("clarity")
        .weight(1.0)
        .description("Easy to understand"));

let result = evaluator.evaluate(
    "Explain photosynthesis",
    "Photosynthesis is the process by which plants convert sunlight..."
)?;

for criteria_score in result.criteria_scores {
    println!("{}: {:.2} (weight: {})", 
        criteria_score.name, 
        criteria_score.score,
        criteria_score.weight);
}
```

## EvaluationSuite

Run multiple evaluations:

```rust
use praisonai::eval::{EvaluationSuite, TestCase, EvaluationReport};

let suite = EvaluationSuite::new("Agent QA Suite")
    .add_case(TestCase::new("capital_question")
        .input("What is the capital of Japan?")
        .expected_output("Tokyo"))
    .add_case(TestCase::new("math_question")
        .input("What is 15 + 27?")
        .expected_output("42"))
    .add_case(TestCase::new("tool_usage")
        .input("Search for recent AI news")
        .expected_tools(vec!["web_search"]));

// Run suite
let report = suite.run(&agent).await?;

println!("Suite: {}", report.suite_name);
println!("Passed: {}/{}", report.passed_count, report.total_count);
println!("Average score: {:.2}", report.average_score);
```

## Example: Agent Evaluation

```rust
use praisonai::{Agent, Result};
use praisonai::eval::{
    EvaluationSuite, TestCase, AccuracyEvaluator, 
    PerformanceMetrics, EvaluationScore
};
use std::time::Instant;

#[tokio::main]
async fn main() -> Result<()> {
    let agent = Agent::new()
        .instructions("You are a helpful assistant")
        .build()?;
    
    let evaluator = AccuracyEvaluator::new();
    
    let test_cases = vec![
        ("What is 2+2?", "4"),
        ("Capital of Germany?", "Berlin"),
        ("Largest planet?", "Jupiter"),
    ];
    
    let mut total_score = 0.0;
    
    for (question, expected) in test_cases {
        let start = Instant::now();
        let response = agent.chat(question).await?;
        let duration = start.elapsed();
        
        let score = evaluator.evaluate(question, &response, expected)?;
        
        println!("\nQ: {}", question);
        println!("A: {}", response);
        println!("Score: {:.2} | Time: {:?}", score.value, duration);
        
        total_score += score.value;
    }
    
    println!("\nAverage: {:.2}", total_score / 3.0);
    
    Ok(())
}
```

## Related

<CardGroup cols={2}>
  <Card title="Agent" icon="robot" href="/docs/rust/agent">
    Agent API
  </Card>
  <Card title="Criteria" icon="check-double" href="/docs/rust/criteria">
    Evaluation criteria
  </Card>
  <Card title="Telemetry" icon="chart-line" href="/docs/rust/telemetry">
    Performance monitoring
  </Card>
  <Card title="Tools" icon="wrench" href="/docs/rust/tools">
    Tool system
  </Card>
</CardGroup>
