---
title: "Trafilatura Web Extraction (Custom Tool Example)"
description: "Example implementation showing how to create a custom web extraction tool using Trafilatura"
icon: "file-export"
---

# Trafilatura Web Extraction - Custom Tool Example

<Note>
**Important**: Trafilatura is NOT part of the core PraisonAI Agents package. This documentation shows how to implement a custom tool using Trafilatura for web content extraction. You'll need to create your own implementation based on the example provided in the `/examples/tools/trafilatura/` directory.
</Note>

This example demonstrates how to create a custom tool that provides advanced web content extraction capabilities, allowing AI agents to extract clean, structured text from web pages while removing boilerplate content like navigation, ads, and footers.

## Overview

Trafilatura is a Python library and command-line tool designed to extract meaningful content from web pages. It focuses on main text extraction, metadata parsing, and content quality assessment, making it ideal for creating clean datasets from web sources.

## Installation

Since this is a custom tool, you'll need to:

1. Install the required dependencies:
   ```bash
   pip install trafilatura lxml
   ```

2. Copy the example implementation from the PraisonAI repository:
   ```bash
   # Download the example file
   wget https://raw.githubusercontent.com/MervinPraison/praisonaiagents/main/examples/tools/trafilatura/custom_trafilatura_tools.py
   ```

3. For enhanced extraction capabilities, install additional dependencies:
   ```bash
   pip install trafilatura[all] newspaper3k beautifulsoup4
   ```

## Core Functions

### Creating the Custom Tool

First, import and initialize the custom Trafilatura tool:

```python
# Import the custom tool
from custom_trafilatura_tools import TrafilaturaTools, create_trafilatura_tools

# Create an instance
tools = create_trafilatura_tools()
```

### `extract_content`

Extracts clean text content from web pages with various configuration options.

```python
# Basic extraction from URL
content = tools.extract_content(
    url="https://example.com/article",
    output_format="text"
)

# Extract with metadata
result = tools.extract_content(
    url="https://example.com/article",
    include_metadata=True,
    output_format="json"
)
print(result['title'])
print(result['author'])
print(result['date'])
print(result['text'])
```

### `extract_text_only`

Extract only the main text content from a URL.

```python
# Extract plain text
text = tools.extract_text_only(
    url="https://example.com/article",
    include_comments=False
)
```

## Usage Examples

### Basic Web Content Extraction

```python
from praisonaiagents import Agent, Task
from custom_trafilatura_tools import create_trafilatura_tools

# Create the custom tool instance
trafilatura_tools = create_trafilatura_tools()

# Create content extraction agent with custom tool
extractor_agent = Agent(
    name="Content Extractor",
    instructions="""Extract and analyze web content. 
    Use extract_content() method from the trafilatura_tools to get content from URLs.""",
    tools=[trafilatura_tools.extract_content]
)

# Extract article content
task = Task(
    description="Extract the main content from https://example.com/blog-post",
    agent=extractor_agent
)

result = task.execute()
print(result)
```

### Advanced Extraction with Options

```python
from custom_trafilatura_tools import create_trafilatura_tools

# Create tool instance
tools = create_trafilatura_tools()

# Extract with all options
content = tools.extract_content(
    url="https://example.com/article",
    output_format="json",  # json, text, or xml
    include_metadata=True,
    include_comments=True,
    include_links=True,
    target_language="en"  # Target language
)

# Extract metadata separately
metadata = tools.extract_metadata(url="https://example.com/article")
print(f"Title: {metadata.get('title')}")
print(f"Author: {metadata.get('author')}")
print(f"Language: {metadata.get('language')}")
```

### Batch Processing Multiple URLs

```python
from custom_trafilatura_tools import create_trafilatura_tools
import concurrent.futures

urls = [
    "https://example.com/article1",
    "https://example.com/article2",
    "https://example.com/article3"
]

# Create tool instance
tools = create_trafilatura_tools()

def extract_url(url):
    try:
        content = tools.extract_content(url, include_metadata=True)
        return {
            'url': url,
            'content': content
        }
    except Exception as e:
        return {'url': url, 'error': str(e)}

# Parallel extraction
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    results = list(executor.map(extract_url, urls))

# Process results
for result in results:
    if 'error' not in result and 'text' in result['content']:
        print(f"Extracted from {result['url']}: {len(result['content']['text'])} characters")
```

## Configuration Options

### Extraction Parameters

The custom tool supports these parameters:

```python
from custom_trafilatura_tools import create_trafilatura_tools

tools = create_trafilatura_tools()

# Full configuration example
content = tools.extract_content(
    url='https://example.com/article',
    include_comments=False,      # Include comment sections
    include_links=True,          # Preserve hyperlinks
    output_format='json',        # 'text', 'json', or 'xml'
    include_metadata=True,       # Extract title, author, date, etc.
    target_language='en'         # Target language for extraction
)
```

### Working with Extracted Data

```python
# Extract and process metadata
metadata = tools.extract_metadata("https://example.com/article")
if 'error' not in metadata:
    print(f"Title: {metadata.get('title', 'N/A')}")
    print(f"Author: {metadata.get('author', 'N/A')}")
    print(f"Date: {metadata.get('date', 'N/A')}")
    print(f"Language: {metadata.get('language', 'N/A')}")

# Compare extraction methods
comparison = tools.compare_extraction(
    url="https://example.com/article",
    include_newspaper=True,
    include_spider=True
)
print("Extraction comparison:", comparison)
```

### Creating a Custom Wrapper with Additional Features

```python
# You can extend the TrafilaturaTools class for custom behavior
from custom_trafilatura_tools import TrafilaturaTools

class EnhancedTrafilaturaTools(TrafilaturaTools):
    def extract_with_summary(self, url: str, max_summary_length: int = 200):
        """Extract content and generate a summary."""
        content = self.extract_content(url, output_format='json')
        
        if 'error' not in content and 'text' in content:
            # Simple summary: first N characters
            text = content['text']
            summary = text[:max_summary_length] + '...' if len(text) > max_summary_length else text
            content['summary'] = summary
            
        return content
    
    def extract_with_keywords(self, url: str, num_keywords: int = 5):
        """Extract content and identify keywords."""
        content = self.extract_text_only(url)
        
        if isinstance(content, str) and content:
            # Simple keyword extraction (word frequency)
            import re
            from collections import Counter
            
            words = re.findall(r'\b\w+\b', content.lower())
            # Filter common words
            stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
            words = [w for w in words if w not in stopwords and len(w) > 3]
            
            keywords = Counter(words).most_common(num_keywords)
            return {
                'text': content,
                'keywords': [kw[0] for kw in keywords]
            }
        
        return {'error': 'Failed to extract keywords'}

# Use the enhanced tool
enhanced_tools = EnhancedTrafilaturaTools()
result = enhanced_tools.extract_with_summary("https://example.com/article")
```

## Integration with AI Agents

### Content Analysis Pipeline

```python
from praisonaiagents import Agent, Task, Process
from custom_trafilatura_tools import create_trafilatura_tools

# Create the custom tool
trafilatura_tools = create_trafilatura_tools()

# Content extractor with custom tool
extractor = Agent(
    name="Web Extractor",
    instructions="""Extract main content from web pages using the extract_content method.
    Return the extracted content in JSON format with metadata.""",
    tools=[trafilatura_tools.extract_content]
)

# Content analyzer
analyzer = Agent(
    name="Content Analyzer",
    instructions="Analyze extracted content for key insights, themes, and important information"
)

# Summarizer
summarizer = Agent(
    name="Summarizer",
    instructions="Create concise summaries of extracted content"
)

# Tasks
extract_task = Task(
    description="Extract content from https://example.com/important-article",
    agent=extractor
)

analyze_task = Task(
    description="Analyze the extracted content for main themes and key points",
    agent=analyzer
)

summarize_task = Task(
    description="Create a 3-paragraph summary of the content",
    agent=summarizer
)

# Process
content_pipeline = Process(
    agents=[extractor, analyzer, summarizer],
    tasks=[extract_task, analyze_task, summarize_task]
)

result = content_pipeline.run()
```

### Research Assistant

```python
from praisonaiagents import Agent, Task
from praisonaiagents.tools import searxng_search
from custom_trafilatura_tools import create_trafilatura_tools

# Create custom tool
trafilatura_tools = create_trafilatura_tools()

research_agent = Agent(
    name="Research Assistant",
    instructions="""You are a research assistant that:
    1. Searches for relevant sources using searxng_search
    2. Extracts content from found URLs using the extract_content method
    3. Compiles comprehensive research reports""",
    tools=[searxng_search, trafilatura_tools.extract_content]
)

research_task = Task(
    description="Research recent developments in quantum computing, extract content from top 5 sources, and compile a report",
    agent=research_agent
)

report = research_task.execute()
```

## Advanced Features

### Content Quality Assessment

```python
def assess_content_quality(url, tools):
    content = tools.extract_content(
        url=url,
        include_metadata=True,
        output_format="json"
    )
    
    if not content:
        return {"quality": "low", "reason": "No content extracted"}
    
    text_length = len(content.get('text', ''))
    has_metadata = all(content.get(field) for field in ['title', 'author', 'date'])
    
    quality_score = {
        'text_length': text_length,
        'has_metadata': has_metadata,
        'quality': 'high' if text_length > 500 and has_metadata else 'medium'
    }
    
    return quality_score
```

### Incremental Web Scraping

```python
import time
from datetime import datetime

def incremental_scrape(url_list, checkpoint_file='scrape_checkpoint.json'):
    import json
    
    # Load checkpoint
    try:
        with open(checkpoint_file, 'r') as f:
            checkpoint = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        checkpoint = {'processed': [], 'last_run': None}
    
    results = []
    
    for url in url_list:
        if url in checkpoint['processed']:
            continue
            
        try:
            content = tools.extract_content(url, include_metadata=True)
            results.append({
                'url': url,
                'content': content,
                'extracted_at': datetime.now().isoformat()
            })
            
            checkpoint['processed'].append(url)
            
            # Save checkpoint after each successful extraction
            checkpoint['last_run'] = datetime.now().isoformat()
            with open(checkpoint_file, 'w') as f:
                json.dump(checkpoint, f)
                
            time.sleep(1)  # Rate limiting
            
        except Exception as e:
            print(f"Error extracting {url}: {e}")
            
    return results
```

### Content Deduplication

```python
from praisonaiagents.tools import trafilatura_extract
import hashlib

def extract_unique_content(urls, tools):
    seen_hashes = set()
    unique_content = []
    
    for url in urls:
        content = tools.extract_content(url, output_format="text")
        
        if content:
            # Create content hash
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_content.append({
                    'url': url,
                    'content': content,
                    'hash': content_hash
                })
            else:
                print(f"Duplicate content found: {url}")
                
    return unique_content
```

## Best Practices

1. **Rate Limiting**: Always implement delays between requests to avoid overwhelming servers
2. **Error Handling**: Wrap extraction calls in try-except blocks
3. **Content Validation**: Verify extracted content meets minimum quality standards
4. **Metadata Preservation**: Always extract metadata when available
5. **Language Filtering**: Use language detection for multilingual sites
6. **Caching**: Cache extracted content to avoid redundant requests
7. **User Agent**: Set appropriate user agent strings

## Creating a Complete Custom Tool Module

Here's how to create a reusable Trafilatura tool module for your project:

```python
# File: my_trafilatura_tool.py

from typing import List, Dict, Any
from praisonaiagents import Tool
import trafilatura

class TrafilaturaWebExtractor(Tool):
    """Custom web extraction tool using Trafilatura."""
    
    name = "trafilatura_extractor"
    description = "Extract clean content from web pages"
    
    def __init__(self):
        super().__init__()
        self.tools_instance = self._create_tools()
    
    def _create_tools(self):
        """Create the TrafilaturaTools instance."""
        from custom_trafilatura_tools import create_trafilatura_tools
        return create_trafilatura_tools()
    
    def execute(self, url: str, **kwargs) -> Dict[str, Any]:
        """Execute the extraction."""
        return self.tools_instance.extract_content(url, **kwargs)
    
    def extract_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Extract content from multiple URLs."""
        results = []
        for url in urls:
            try:
                content = self.execute(url, include_metadata=True)
                results.append({'url': url, 'content': content, 'status': 'success'})
            except Exception as e:
                results.append({'url': url, 'error': str(e), 'status': 'failed'})
        return results

# Usage in agents
from praisonaiagents import Agent

# Create custom tool
web_extractor = TrafilaturaWebExtractor()

# Use with agent
agent = Agent(
    name="Web Analyst",
    instructions="Extract and analyze web content",
    tools=[web_extractor]
)
```

## Troubleshooting

### Common Issues and Solutions

1. **Empty Extraction Results**
   ```python
   # Fallback extraction strategies
   def robust_extract(url, tools):
       # Try different extraction strategies
       strategies = [
           {'include_comments': False, 'output_format': 'text'},
           {'include_metadata': True, 'output_format': 'json'},
           {'include_links': True, 'output_format': 'json'}
       ]
       
       for strategy in strategies:
           content = tools.extract_content(url, **strategy)
           if content and 'error' not in content:
               return content
               
       return None
   ```

2. **Encoding Issues**
   ```python
   # Handle various encodings
   def extract_with_encoding_detection(url):
       import chardet
       import requests
       
       response = requests.get(url)
       detected = chardet.detect(response.content)
       encoding = detected['encoding']
       
       html = response.content.decode(encoding)
       # You'll need to implement HTML extraction in the custom tool
       # or use trafilatura directly
       import trafilatura
       return trafilatura.extract(html)
   ```

3. **JavaScript-Heavy Sites**
   ```python
   # For JS-rendered content, combine with browser automation
   from selenium import webdriver
   
   def extract_js_content(url):
       driver = webdriver.Chrome()
       driver.get(url)
       
       # Wait for content to load
       time.sleep(3)
       
       html = driver.page_source
       driver.quit()
       
       # Use trafilatura directly for HTML extraction
       import trafilatura
       return trafilatura.extract(html)
   ```

## Summary

The Trafilatura custom tool example demonstrates how to:

1. **Create custom tools** for PraisonAI agents
2. **Implement web extraction** with security features (URL validation)
3. **Integrate external libraries** into your agent workflows
4. **Handle errors gracefully** with proper error messages
5. **Compare different extraction methods** for quality assessment

### Key Features of the Example Implementation

- ✅ URL validation to prevent SSRF attacks
- ✅ Multiple extraction methods (content, metadata, text-only)
- ✅ Comparison with other extraction tools
- ✅ Proper error handling and logging
- ✅ Type hints and documentation

### When to Use This Custom Tool

- When you need high-quality text extraction from web pages
- When built-in web scraping tools don't meet your requirements
- When you need to extract structured metadata along with content
- When you want to compare extraction quality across different methods

For more examples of custom tools, check the `/examples/tools/` directory in the PraisonAI repository.