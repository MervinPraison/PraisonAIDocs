---
title: "Deep Research Agent"
sidebarTitle: "Deep Research"
description: "Automated research using OpenAI or Gemini Deep Research APIs with real-time streaming and citations."
icon: "flask-vial"
---

```mermaid
flowchart LR
    In[Research Query] --> Agent[Deep Research Agent]
    Agent --> Search[Web Search]
    Search --> Reason[Reasoning]
    Reason --> Report[Research Report]
    Report --> Out[Citations + Report]
    
    style In fill:#8B0000,color:#fff
    style Agent fill:#2E8B57,color:#fff
    style Search fill:#2E8B57,color:#fff
    style Reason fill:#2E8B57,color:#fff
    style Report fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

The Deep Research Agent automates comprehensive research using OpenAI or Gemini Deep Research APIs with real-time streaming, web search, and structured citations.

**Agents: 1** â€” Specialized agent using provider deep research APIs.

## Workflow

1. Receive research query
2. Execute web searches via provider API
3. Perform multi-step reasoning
4. Generate comprehensive report with citations

## Setup

```bash
pip install praisonaiagents praisonai
export OPENAI_API_KEY="your-key"  # or GEMINI_API_KEY
```

## Run â€” Python

```python
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="o4-mini-deep-research",
    verbose=True
)

result = agent.research("What are the latest AI trends in 2025?")
print(result.report)
print(f"Citations: {len(result.citations)}")
```

## Run â€” CLI

```bash
# Deep research mode
praisonai research "What are the latest AI trends?"

# With save option
praisonai research --save "Research quantum computing advances"
```

## Run â€” agents.yaml

```yaml
framework: praisonai
topic: Deep Research
roles:
  researcher:
    role: Deep Research Specialist
    goal: Conduct comprehensive research with citations
    backstory: You are an expert researcher
    llm: o4-mini-deep-research
    tasks:
      research:
        description: Research the latest AI trends in 2025
        expected_output: Comprehensive report with citations
```

```bash
praisonai agents.yaml
```

## Serve API

```python
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="o4-mini-deep-research",
    verbose=True
)

# Note: DeepResearchAgent uses .research() method
# For API serving, wrap in standard agent
```

## OpenAI Deep Research

```python
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="o4-mini-deep-research",  # or "o3-deep-research"
    verbose=True
)

result = agent.research("What are the latest AI trends?")
print(result.report)
print(f"Citations: {len(result.citations)}")
```

## Gemini Deep Research

```python
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="deep-research-pro",
    verbose=True
)

result = agent.research("Research quantum computing advances")
print(result.report)
```

## Features

<CardGroup cols={2}>
  <Card title="Multi-Provider" icon="layer-group">
    Supports OpenAI, Gemini, and LiteLLM providers.
  </Card>
  <Card title="Real-time Streaming" icon="signal-stream">
    See reasoning summaries and web searches as they happen.
  </Card>
  <Card title="Structured Citations" icon="quote-left">
    Get citations with titles and URLs.
  </Card>
  <Card title="Auto Detection" icon="wand-magic-sparkles">
    Provider automatically detected from model name.
  </Card>
</CardGroup>

## Streaming Output

Streaming is enabled by default. You will see:
- ðŸ’­ Reasoning summaries
- ðŸ”Ž Web search queries
- Final report text

```python
# Streaming is ON by default
result = agent.research("Research topic")

# Disable streaming
result = agent.research("Research topic", stream=False)
```

## Response Structure

```python
result.report           # Full research report
result.citations        # List of citations with URLs
result.web_searches     # Web searches performed
result.reasoning_steps  # Reasoning steps captured
result.interaction_id   # Session ID (for Gemini follow-ups)
```

## Available Models

| Provider | Models |
|----------|--------|
| OpenAI | `o3-deep-research`, `o4-mini-deep-research` |
| Gemini | `deep-research-pro` |

## Configuration Options

```python
agent = DeepResearchAgent(
    name="Researcher",
    model="o4-mini-deep-research",
    instructions="Focus on data-rich insights",
    verbose=True,
    poll_interval=5,      # Gemini polling interval (seconds)
    max_wait_time=3600    # Max research time (seconds)
)
```

## With Custom Instructions

```python
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="o4-mini-deep-research",
    instructions="""
    You are a professional researcher. Focus on:
    - Data-rich insights with specific figures
    - Reliable sources and citations
    - Clear, structured responses
    """,
    verbose=True
)

result = agent.research("Economic impact of AI on healthcare")
```

## Accessing Citations

```python
result = agent.research("Research topic")

for citation in result.citations:
    print(f"Title: {citation.title}")
    print(f"URL: {citation.url}")
    print(f"Snippet: {citation.snippet}")
    print("---")
```

---

## Monitor / Verify

```bash
praisonai research "test query" --verbose
```

## Cleanup

```bash
# No cleanup needed - uses provider APIs
```

## Features Demonstrated

| Feature | Implementation |
|---------|----------------|
| Workflow | Multi-step reasoning with web search |
| Observability | `--verbose` flag, streaming output |
| Tools | Built-in web search via provider API |
| Resumability | `interaction_id` for Gemini follow-ups |
| Structured Output | Citations with titles and URLs |

## Next Steps

- [Research Agent](/agents/research) for custom research workflows
- [RAG](/features/rag) for document-based research
- [Memory](/features/advanced-memory) for persistent research context
