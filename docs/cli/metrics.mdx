---
title: "Metrics"
sidebarTitle: "Metrics"
description: "Track token usage and cost metrics for agent executions"
icon: "chart-bar"
---

The `--metrics` flag displays token usage and cost information after agent execution.

## Quick Start

```bash
praisonai "Analyze this data" --metrics
```

## Usage

### Basic Metrics

```bash
praisonai "Explain quantum computing" --metrics
```

**Expected Output:**
```
Metrics enabled - will display token usage and costs

â•­â”€ Agent Info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  ğŸ‘¤ Agent: DirectAgent                                                       â”‚
â”‚  Role: Assistant                                                             â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Quantum computing is a type of computation that harnesses quantum mechanical â”‚
â”‚ phenomena like superposition and entanglement...                             â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ğŸ“Š Metrics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Value        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model               â”‚ gpt-4o-mini  â”‚
â”‚ Prompt Tokens       â”‚ 45           â”‚
â”‚ Completion Tokens   â”‚ 312          â”‚
â”‚ Total Tokens        â”‚ 357          â”‚
â”‚ Estimated Cost      â”‚ $0.0021      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Combine with Other Features

```bash
# Metrics with planning mode
praisonai "Complex analysis task" --metrics --planning

# Metrics with guardrail (shows combined usage)
praisonai "Generate code" --metrics --guardrail "Include tests"

# Metrics with router (shows selected model)
praisonai "Simple question" --metrics --router
```

## Metrics Displayed

| Metric | Description |
|--------|-------------|
| **Model** | The LLM model used for the task |
| **Prompt Tokens** | Tokens in the input/prompt |
| **Completion Tokens** | Tokens in the response |
| **Total Tokens** | Sum of prompt + completion tokens |
| **Estimated Cost** | Approximate cost based on model pricing |

## Use Cases

### Cost Monitoring

Track costs across different prompts:

```bash
# Short prompt
praisonai "What is 2+2?" --metrics
# Expected: ~50 tokens, ~$0.0001

# Long prompt
praisonai "Write a detailed analysis of AI trends in 2025" --metrics
# Expected: ~2000 tokens, ~$0.012
```

### Model Comparison

Compare token usage across models:

```bash
# GPT-4o-mini (cheaper)
praisonai "Explain AI" --metrics --llm openai/gpt-4o-mini

# GPT-4o (more capable)
praisonai "Explain AI" --metrics --llm openai/gpt-4o

# Claude (different pricing)
praisonai "Explain AI" --metrics --llm anthropic/claude-3-haiku-20240307
```

### Planning Mode Metrics

See total tokens across all planning steps:

```bash
praisonai "Research and write a report" --metrics --planning
```

**Expected Output:**
```
ğŸ“Š Metrics (Planning Mode):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Value        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Planning Tokens     â”‚ 523          â”‚
â”‚ Execution Tokens    â”‚ 1,847        â”‚
â”‚ Total Tokens        â”‚ 2,370        â”‚
â”‚ Estimated Cost      â”‚ $0.0142      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cost Estimation

<Note>
Cost estimates are approximate and based on publicly available pricing. Actual costs may vary based on your API plan.
</Note>

### Typical Costs by Model

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| gpt-4o-mini | $0.15 | $0.60 |
| gpt-4o | $2.50 | $10.00 |
| claude-3-haiku | $0.25 | $1.25 |
| claude-3-sonnet | $3.00 | $15.00 |

## Best Practices

<Tip>
Use `--metrics` during development to optimize prompts and reduce costs before production deployment.
</Tip>

<CardGroup cols={2}>
  <Card title="Optimize Prompts">
    Monitor token counts to identify verbose prompts that can be shortened
  </Card>
  <Card title="Choose Right Model">
    Use metrics to compare cost/quality tradeoffs between models
  </Card>
  <Card title="Budget Tracking">
    Track cumulative costs across multiple runs for budget planning
  </Card>
  <Card title="Debug Issues">
    High token counts may indicate prompt issues or infinite loops
  </Card>
</CardGroup>

## Related

- [Telemetry CLI](/docs/cli/telemetry)
- [Router CLI](/docs/cli/router)
- [Models](/models)
