---
title: "Ollama Provider"
description: "Use local Ollama models with PraisonAI TypeScript"
icon: "server"
---

# Ollama Provider

Run models locally with Ollama.

## Environment Variables

```bash
export OLLAMA_BASE_URL=http://localhost:11434
```

## Supported Modalities

| Modality | Supported |
|----------|-----------|
| Text/Chat | ✅ |
| Embeddings | ✅ |
| Tools | ✅ |

## Prerequisites

1. Install Ollama: https://ollama.ai
2. Pull a model: `ollama pull llama3.2`

## Quick Start

```typescript
import { Agent } from 'praisonai';

const agent = new Agent({
  name: 'LocalAgent',
  instructions: 'You are a helpful assistant.',
  llm: 'ollama/llama3.2'
});

const response = await agent.chat('Hello!');
```

## Available Models

| Model | Description |
|-------|-------------|
| `llama3.2` | Llama 3.2 |
| `llama3.1` | Llama 3.1 |
| `mistral` | Mistral 7B |
| `codellama` | Code Llama |
| `phi3` | Phi-3 |

## Related

- [Ollama CLI Usage](/docs/js/providers/ollama-cli)
- [Local Providers](/docs/js/local-providers)
