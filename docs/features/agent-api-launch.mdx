---
title: "Agent API Launch"
description: "Deploy AI agents as HTTP APIs or MCP servers for integration with external applications."
icon: "rocket"
---

PraisonAI Agents can be deployed as HTTP APIs or MCP (Model Context Protocol) servers, enabling seamless integration with web applications, microservices, and other systems.

## Quick Start

<Steps>
    <Step title="Install Package">
        Install PraisonAI Agents with API support:
        ```bash
        pip install praisonaiagents[api]
        ```
    </Step>

    <Step title="Create an Agent">
        Create an agent to deploy as an API:
        ```python
        from praisonaiagents import Agent

        # Create your agent
        agent = Agent(
            name="API Assistant",
            role="API helper",
            goal="Answer questions and perform tasks via API",
            backstory="An AI assistant accessible through HTTP endpoints",
            llm="gpt-4o-mini"
        )
        ```
    </Step>

    <Step title="Launch as API">
        Deploy the agent as an HTTP API:
        ```python
        # Launch as HTTP API
        agent.launch(
            type="api",
            host="0.0.0.0",
            port=8000,
            path="/assistant"  # Available at http://localhost:8000/assistant
        )
        ```
    </Step>

    <Step title="Test the API">
        Test your deployed agent API:
        ```bash
        curl -X POST http://localhost:8000/assistant/chat \
          -H "Content-Type: application/json" \
          -d '{"message": "Hello, how can you help me?"}'
        ```
    </Step>
</Steps>

## Launch Methods

### HTTP API Server

The most common deployment method is as an HTTP API server:

```python
from praisonaiagents import Agent

# Create agent
agent = Agent(
    name="Customer Support",
    role="Support specialist",
    goal="Help customers with their inquiries",
    tools=["web_search", "knowledge_base"]  # Optional tools
)

# Launch as HTTP API
agent.launch(
    type="api",
    host="0.0.0.0",      # Listen on all interfaces
    port=8000,           # Port number
    path="/support",     # API endpoint path
    cors=True,           # Enable CORS for web apps
    auth_token="secret"  # Optional authentication
)

# API will be available at:
# POST http://localhost:8000/support/chat
# GET  http://localhost:8000/support/health
# GET  http://localhost:8000/support/info
```

### MCP Server

For Model Context Protocol integration:

```python
# Launch as MCP server
agent.launch(
    type="mcp",
    transport="stdio",    # or "websocket"
    name="my-agent",
    version="1.0.0"
)
```

## API Endpoints

When launched as an HTTP API, agents expose several endpoints:

### Chat Endpoint
`POST /path/chat`

Send messages to the agent and receive responses.

```python
# Request
{
    "message": "What's the weather like?",
    "context": {  # Optional
        "user_id": "123",
        "session_id": "abc"
    },
    "stream": false  # Set to true for streaming responses
}

# Response
{
    "response": "I'd be happy to help with weather information...",
    "agent_id": "agent-123",
    "timestamp": "2024-01-15T10:30:00Z",
    "metadata": {
        "tokens_used": 150,
        "model": "gpt-4o-mini"
    }
}
```

### Health Check
`GET /path/health`

Check if the agent API is running.

```python
# Response
{
    "status": "healthy",
    "agent": "Customer Support",
    "uptime": 3600,
    "version": "1.0.0"
}
```

### Agent Info
`GET /path/info`

Get information about the agent.

```python
# Response
{
    "name": "Customer Support",
    "role": "Support specialist",
    "capabilities": ["chat", "tools"],
    "tools": ["web_search", "knowledge_base"],
    "model": "gpt-4o-mini"
}
```

## Complete Examples

### Example 1: Multi-Agent API System

```python
from praisonaiagents import Agent
import asyncio

# Create multiple specialized agents
sales_agent = Agent(
    name="Sales Assistant",
    role="Sales specialist",
    goal="Help with product information and purchases"
)

support_agent = Agent(
    name="Support Assistant",
    role="Technical support",
    goal="Resolve technical issues"
)

general_agent = Agent(
    name="General Assistant",
    role="General helper",
    goal="Answer general questions"
)

# Launch all agents on different ports
async def launch_agents():
    tasks = [
        asyncio.create_task(sales_agent.launch(
            type="api",
            host="0.0.0.0",
            port=8001,
            path="/sales"
        )),
        asyncio.create_task(support_agent.launch(
            type="api",
            host="0.0.0.0",
            port=8002,
            path="/support"
        )),
        asyncio.create_task(general_agent.launch(
            type="api",
            host="0.0.0.0",
            port=8003,
            path="/general"
        ))
    ]
    
    await asyncio.gather(*tasks)

# Run all agents
asyncio.run(launch_agents())
```

### Example 2: Authenticated Agent API

```python
from praisonaiagents import Agent
import secrets

# Generate secure token
auth_token = secrets.token_urlsafe(32)
print(f"Authentication token: {auth_token}")

# Create secure agent
secure_agent = Agent(
    name="Private Assistant",
    role="Confidential advisor",
    goal="Handle sensitive information securely"
)

# Launch with authentication
secure_agent.launch(
    type="api",
    host="127.0.0.1",  # Local only for security
    port=8443,
    path="/secure",
    auth_token=auth_token,
    ssl_cert="cert.pem",    # Optional SSL
    ssl_key="key.pem"
)

# Client must include auth header:
# Authorization: Bearer <auth_token>
```

### Example 3: Streaming Agent API

```python
from praisonaiagents import Agent

# Create agent with streaming support
streaming_agent = Agent(
    name="Story Teller",
    role="Creative writer",
    goal="Generate engaging stories",
    streaming=True  # Enable streaming
)

# Launch with streaming configuration
streaming_agent.launch(
    type="api",
    host="0.0.0.0",
    port=8000,
    path="/storyteller",
    enable_streaming=True,
    stream_timeout=30  # Seconds
)

# Client code for streaming
import requests
import json

def stream_chat(message):
    response = requests.post(
        "http://localhost:8000/storyteller/chat",
        json={"message": message, "stream": True},
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            data = json.loads(line.decode('utf-8'))
            print(data['chunk'], end='', flush=True)
```

### Example 4: Agent with Custom Middleware

```python
from praisonaiagents import Agent
from datetime import datetime
import json

# Create agent with request logging
class LoggingAgent(Agent):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.request_log = []
    
    async def process_request(self, request):
        # Log request
        self.request_log.append({
            "timestamp": datetime.now().isoformat(),
            "message": request.get("message"),
            "ip": request.get("client_ip")
        })
        
        # Process normally
        return await super().process_request(request)

# Create logging agent
logging_agent = LoggingAgent(
    name="Monitored Assistant",
    role="Logged helper",
    goal="Provide assistance with full audit trail"
)

# Launch with custom middleware
logging_agent.launch(
    type="api",
    host="0.0.0.0",
    port=8000,
    path="/logged",
    middleware=[
        "rate_limiting",  # Built-in middleware
        "request_id",
        "cors"
    ],
    rate_limit="100/hour"  # Rate limiting config
)
```

## Advanced Configuration

### Launch Parameters

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `type` | str | Launch type: "api" or "mcp" | "api" |
| `host` | str | Host to bind to | "127.0.0.1" |
| `port` | int | Port number | 8000 |
| `path` | str | API endpoint path | "/agent" |
| `cors` | bool | Enable CORS | False |
| `auth_token` | str | Authentication token | None |
| `ssl_cert` | str | SSL certificate path | None |
| `ssl_key` | str | SSL key path | None |
| `enable_streaming` | bool | Enable streaming responses | True |
| `max_connections` | int | Maximum concurrent connections | 100 |
| `timeout` | int | Request timeout in seconds | 30 |

### Environment Variables

```bash
# API configuration
export PRAISONAI_API_HOST="0.0.0.0"
export PRAISONAI_API_PORT="8000"
export PRAISONAI_API_AUTH_TOKEN="your-secret-token"

# SSL configuration
export PRAISONAI_SSL_CERT="/path/to/cert.pem"
export PRAISONAI_SSL_KEY="/path/to/key.pem"

# Performance tuning
export PRAISONAI_MAX_WORKERS="4"
export PRAISONAI_REQUEST_TIMEOUT="60"
```

## Client Integration

### Python Client

```python
import requests
from typing import Optional, Dict, Any

class AgentAPIClient:
    def __init__(self, base_url: str, auth_token: Optional[str] = None):
        self.base_url = base_url.rstrip('/')
        self.headers = {"Content-Type": "application/json"}
        if auth_token:
            self.headers["Authorization"] = f"Bearer {auth_token}"
    
    def chat(self, message: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        response = requests.post(
            f"{self.base_url}/chat",
            headers=self.headers,
            json={"message": message, "context": context or {}}
        )
        response.raise_for_status()
        return response.json()
    
    def health(self) -> Dict[str, Any]:
        response = requests.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()
    
    def info(self) -> Dict[str, Any]:
        response = requests.get(f"{self.base_url}/info")
        response.raise_for_status()
        return response.json()

# Usage
client = AgentAPIClient("http://localhost:8000/assistant")
response = client.chat("Hello!")
print(response["response"])
```

### JavaScript Client

```javascript
class AgentAPIClient {
    constructor(baseUrl, authToken = null) {
        this.baseUrl = baseUrl.replace(/\/$/, '');
        this.headers = {
            'Content-Type': 'application/json'
        };
        if (authToken) {
            this.headers['Authorization'] = `Bearer ${authToken}`;
        }
    }
    
    async chat(message, context = {}) {
        const response = await fetch(`${this.baseUrl}/chat`, {
            method: 'POST',
            headers: this.headers,
            body: JSON.stringify({ message, context })
        });
        
        if (!response.ok) {
            throw new Error(`API error: ${response.statusText}`);
        }
        
        return response.json();
    }
    
    async streamChat(message, onChunk) {
        const response = await fetch(`${this.baseUrl}/chat`, {
            method: 'POST',
            headers: this.headers,
            body: JSON.stringify({ message, stream: true })
        });
        
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        
        while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const data = JSON.parse(chunk);
            onChunk(data.chunk);
        }
    }
}

// Usage
const client = new AgentAPIClient('http://localhost:8000/assistant');
const response = await client.chat('Hello!');
console.log(response.response);
```

## Deployment Best Practices

<CardGroup cols={2}>
  <Card title="Security" icon="shield">
    - Always use authentication in production
    - Enable SSL/TLS for encrypted communication
    - Implement rate limiting
    - Validate and sanitize inputs
    - Use environment variables for secrets
  </Card>
  <Card title="Performance" icon="gauge">
    - Use connection pooling
    - Enable response caching where appropriate
    - Implement request queuing
    - Monitor resource usage
    - Use load balancing for multiple agents
  </Card>
</CardGroup>

## Production Deployment

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install praisonaiagents[api]

# Copy agent code
COPY agent.py .

# Run agent API
CMD ["python", "agent.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  agent-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PRAISONAI_API_AUTH_TOKEN=${AUTH_TOKEN}
    restart: unless-stopped
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: agent-api
  template:
    metadata:
      labels:
        app: agent-api
    spec:
      containers:
      - name: agent
        image: your-registry/agent-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: agent-secrets
              key: openai-api-key
---
apiVersion: v1
kind: Service
metadata:
  name: agent-api-service
spec:
  selector:
    app: agent-api
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

## Monitoring and Logging

```python
from praisonaiagents import Agent
import logging
from prometheus_client import Counter, Histogram, start_http_server

# Setup metrics
request_count = Counter('agent_requests_total', 'Total requests')
request_duration = Histogram('agent_request_duration_seconds', 'Request duration')

# Create monitored agent
class MonitoredAgent(Agent):
    @request_duration.time()
    async def process_request(self, request):
        request_count.inc()
        return await super().process_request(request)

# Start Prometheus metrics server
start_http_server(9090)

# Launch agent with monitoring
agent = MonitoredAgent(
    name="Monitored Agent",
    role="Observable assistant",
    goal="Provide assistance with full observability"
)

agent.launch(
    type="api",
    port=8000,
    log_level="INFO",
    access_log=True
)
```

## Troubleshooting

<AccordionGroup>
  <Accordion title="Agent won't start">
    - Check if port is already in use
    - Verify all dependencies are installed
    - Check for syntax errors in agent code
    - Ensure API key is set correctly
  </Accordion>

  <Accordion title="Authentication issues">
    - Verify auth token matches on client and server
    - Check Authorization header format
    - Ensure token is properly encoded
    - Verify SSL certificates if using HTTPS
  </Accordion>

  <Accordion title="Performance problems">
    - Monitor CPU and memory usage
    - Check for blocking operations in agent
    - Implement caching for repeated queries
    - Consider horizontal scaling
  </Accordion>

  <Accordion title="Connection errors">
    - Verify firewall rules
    - Check CORS configuration
    - Ensure proper network connectivity
    - Monitor rate limiting
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="API Reference" icon="book" href="/api-reference">
    Detailed API documentation
  </Card>
  <Card title="MCP Integration" icon="plug" href="/integrations/mcp">
    Learn about Model Context Protocol
  </Card>
</CardGroup>