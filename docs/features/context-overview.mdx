---
title: "Context Management Overview"
sidebarTitle: "Overview"
description: "Complete visual guide to context management architecture, terminology, and optimization strategies"
icon: "diagram-project"
---

# Context Management Overview

This page provides a comprehensive visual guide to how context management works in PraisonAI Agents.

## Architecture

```mermaid
flowchart TB
    subgraph Input["üì• Context Sources"]
        SYS["System Prompt<br/>~2000 tokens"]
        HIST["Chat History<br/>Variable"]
        TOOLS["Tool Schemas<br/>~2000 tokens"]
        TOOL_OUT["Tool Outputs<br/>~20000 tokens"]
        MEM["Memory/RAG<br/>~4000 tokens"]
    end

    subgraph Manager["‚öôÔ∏è Context Manager"]
        EST["Token Estimator"]
        BUD["Budget Allocator"]
        DEDUP["Deduplication"]
        OPT["Optimizer"]
    end

    subgraph Strategies["üîß Optimization Strategies"]
        TRUNC["Truncate"]
        SLIDE["Sliding Window"]
        PRUNE["Prune Tools"]
        SUMM["Summarize"]
        SMART["Smart"]
    end

    subgraph Output["üì§ Optimized Context"]
        LLM["LLM API Call"]
    end

    SYS --> EST
    HIST --> EST
    TOOLS --> EST
    TOOL_OUT --> EST
    MEM --> EST

    EST --> BUD
    BUD --> DEDUP
    DEDUP --> OPT

    OPT --> TRUNC
    OPT --> SLIDE
    OPT --> PRUNE
    OPT --> SUMM
    OPT --> SMART

    TRUNC --> LLM
    SLIDE --> LLM
    PRUNE --> LLM
    SUMM --> LLM
    SMART --> LLM
```

## Multi-Agent Context Flow

```mermaid
sequenceDiagram
    participant W as Workflow
    participant A1 as Agent 1
    participant A2 as Agent 2
    participant A3 as Agent 3
    participant SC as Session Cache

    W->>SC: Create shared session cache
    W->>A1: Start with input
    A1->>SC: Add content hash
    A1->>W: Return output
    
    W->>A2: Pass output as input
    A2->>SC: Check hash (deduplicate)
    A2->>SC: Add new content hash
    A2->>W: Return output
    
    W->>A3: Pass output as input
    A3->>SC: Check hash (deduplicate)
    A3->>W: Return final output
```

## Terminology Reference

### Core Concepts

| Term | Definition | Default |
|------|------------|---------|
| **Context Window** | Maximum tokens an LLM can process in one request | Model-specific (128K for GPT-4o) |
| **Token Budget** | Allocated tokens for each context segment | Auto-calculated |
| **Compact Threshold** | Usage % that triggers optimization | 80% |
| **Output Reserve** | Tokens reserved for LLM response | 8000-16000 |

### Optimization Strategies

| Strategy | How It Works | Best For |
|----------|--------------|----------|
| **Truncate** | Removes oldest messages first | Simple chatbots |
| **Sliding Window** | Keeps N most recent messages | Long conversations |
| **Prune Tools** | Truncates old tool outputs | Tool-heavy agents |
| **Summarize** | Replaces old messages with summary | Critical context |
| **Smart** | Combines all strategies intelligently | Production use |

### Token Segments

```mermaid
pie title Token Budget Allocation (128K model)
    "System Prompt" : 2000
    "Tool Schemas" : 2000
    "Tool Outputs" : 20000
    "Memory/RAG" : 4000
    "History" : 84000
    "Output Reserve" : 16000
```

## Defaults Reference

### ContextConfig Defaults

```python
ContextConfig(
    auto_compact=True,           # Auto-optimize when threshold reached
    compact_threshold=0.8,       # Trigger at 80% usage
    strategy="smart",            # Use smart optimization
    output_reserve=8000,         # Reserve for LLM response
    history_ratio=0.6,           # 60% of usable for history
    tool_output_max=10000,       # Max tokens per tool output
    prune_after_tokens=40000,    # Start pruning after 40K
    keep_recent_turns=5,         # Keep last 5 turns intact
    tool_limits={},              # Per-tool output limits
)
```

### Per-Tool Limits

Configure different limits for different tools:

```yaml
context:
  auto_compact: true
  tool_limits:
    tavily_search: 2000      # Search results: 2000 chars
    tavily_extract: 5000     # Full page: 5000 chars
    code_executor: 10000     # Code output: 10000 chars
```

## Overflow Handling

```mermaid
stateDiagram-v2
    [*] --> Normal: < 70%
    Normal --> Warning: 70-80%
    Warning --> Critical: 80-90%
    Critical --> Emergency: 90-95%
    Emergency --> Overflow: > 95%
    
    Warning --> Normal: Optimization
    Critical --> Normal: Auto-compact
    Emergency --> Normal: Aggressive truncation
    Overflow --> Normal: Emergency truncation
```

| Level | Usage | Action |
|-------|-------|--------|
| Normal | < 70% | No action |
| Warning | 70-80% | Monitor |
| Critical | 80-90% | Auto-compact triggers |
| Emergency | 90-95% | Aggressive optimization |
| Overflow | > 95% | Emergency truncation |

## Session Deduplication

Prevents duplicate content across agents in multi-agent workflows:

```mermaid
flowchart LR
    subgraph Agent1["Agent 1"]
        C1["Content A"]
    end
    
    subgraph Cache["Session Cache"]
        H1["Hash A ‚úì"]
        H2["Hash B ‚úì"]
    end
    
    subgraph Agent2["Agent 2"]
        C2["Content A (skip)"]
        C3["Content B"]
    end
    
    subgraph Agent3["Agent 3"]
        C4["Content A (skip)"]
        C5["Content B (skip)"]
        C6["Content C"]
    end
    
    C1 --> H1
    C3 --> H2
    C2 -.->|"Duplicate"| H1
    C4 -.->|"Duplicate"| H1
    C5 -.->|"Duplicate"| H2
```

## CLI Commands

### Analytics Dashboard

```bash
# View analytics for recent sessions
praisonai replay dashboard

# Analyze specific session
praisonai replay dashboard <session_id>

# JSON output
praisonai replay dashboard --json
```

### Session Statistics

```bash
# View session stats
praisonai replay context <session_id> --stats
```

Output:
```
TOKEN USAGE BY AGENT:
  deep_researcher      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    956,146 (75.5%)
  content_writer       ‚ñà‚ñà‚ñà                               113,023 (8.9%)

CONTEXT EFFICIENCY:
  ‚ö†Ô∏è  Duplicates Found:  236
  ‚ö†Ô∏è  Wasted Tokens:     135,279 (10.7%)
```

## Quick Start Examples

<CodeGroup>
```python Enable Context
from praisonaiagents import Agent

agent = Agent(
    instructions="You are helpful",
    context=True  # Enable with defaults
)
```

```python Custom Config
from praisonaiagents import Agent, ManagerConfig

agent = Agent(
    instructions="You are helpful",
    context=ManagerConfig(
        auto_compact=True,
        compact_threshold=0.7,
        strategy="smart",
        tool_limits={"tavily_search": 2000}
    )
)
```

```yaml YAML Config
context:
  auto_compact: true
  compact_threshold: 0.7
  strategy: smart
  tool_limits:
    tavily_search: 2000
    tavily_extract: 5000
```
</CodeGroup>

## Related Pages

- [Context Strategies](/features/context-strategies) - Detailed strategy reference
- [Context Budgeter](/features/context-budgeter) - Token budgeting
- [Context Optimizer](/features/context-optimizer) - Optimization details
- [Context Replay](/features/replay) - Debugging and analysis
