---
title: "Context Management Overview"
sidebarTitle: "Overview"
description: "Complete visual guide to context management architecture, terminology, and optimization strategies"
icon: "diagram-project"
---

# Context Management Overview

This page provides a comprehensive visual guide to how context management works in PraisonAI Agents.

## What Makes PraisonAI Context Management Best-in-Class

<CardGroup cols={3}>
  <Card title="Smart Defaults" icon="wand-magic-sparkles">
    Auto-enables when agents have tools. Zero overhead when not needed.
  </Card>
  <Card title="Lazy Loading" icon="feather">
    No performance impact until actually used. 0ms creation overhead.
  </Card>
  <Card title="Session Deduplication" icon="clone">
    Cross-agent duplicate detection in multi-agent workflows.
  </Card>
  <Card title="LLM Summarization" icon="brain">
    Intelligent compression using agent's own LLM for quality preservation.
  </Card>
  <Card title="Per-Tool Limits" icon="sliders">
    Fine-grained control over individual tool output sizes.
  </Card>
  <Card title="6 Strategies" icon="gears">
    Truncate, Sliding Window, Prune Tools, Summarize, Smart, Non-Destructive.
  </Card>
  <Card title="Comprehensive Monitoring" icon="chart-line">
    Snapshots, analytics dashboard, and replay capabilities.
  </Card>
</CardGroup>

<Tip>
**Zero Performance Impact**: Context management uses lazy loading throughout. Setting `context=True` adds only 1 boolean assignment at creation time (0ms). The ContextManager is only instantiated when `.context_manager` is first accessed.
</Tip>

### Feature Comparison

| Feature | PraisonAI | LangChain | CrewAI | Agno |
|---------|:---------:|:---------:|:------:|:----:|
| Smart Defaults | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Lazy Loading (0ms overhead) | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Session Deduplication | ‚úÖ | ‚ùå | ‚ùå | ‚ö†Ô∏è |
| LLM Summarization | ‚úÖ | ‚ö†Ô∏è | ‚ùå | ‚ùå |
| Per-Tool Limits | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| 6 Compaction Strategies | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Benefit Checking | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Auto-Compaction | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Snapshot Replay | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Multi-Memory Aggregation | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå |

---

## Architecture

```mermaid
flowchart TB
    subgraph Input["üì• Context Sources"]
        SYS["System Prompt<br/>~2000 tokens"]
        HIST["Chat History<br/>Variable"]
        TOOLS["Tool Schemas<br/>~2000 tokens"]
        TOOL_OUT["Tool Outputs<br/>~20000 tokens"]
        MEM["Memory/RAG<br/>~4000 tokens"]
    end

    subgraph Manager["‚öôÔ∏è Context Manager"]
        EST["Token Estimator"]
        BUD["Budget Allocator"]
        DEDUP["Deduplication"]
        OPT["Optimizer"]
    end

    subgraph Strategies["üîß Optimization Strategies"]
        TRUNC["Truncate"]
        SLIDE["Sliding Window"]
        PRUNE["Prune Tools"]
        SUMM["Summarize"]
        SMART["Smart"]
    end

    subgraph Output["üì§ Optimized Context"]
        LLM["LLM API Call"]
    end

    SYS --> EST
    HIST --> EST
    TOOLS --> EST
    TOOL_OUT --> EST
    MEM --> EST

    EST --> BUD
    BUD --> DEDUP
    DEDUP --> OPT

    OPT --> TRUNC
    OPT --> SLIDE
    OPT --> PRUNE
    OPT --> SUMM
    OPT --> SMART

    TRUNC --> LLM
    SLIDE --> LLM
    PRUNE --> LLM
    SUMM --> LLM
    SMART --> LLM
```

## Multi-Agent Context Flow

```mermaid
sequenceDiagram
    participant W as Workflow
    participant A1 as Agent 1
    participant A2 as Agent 2
    participant A3 as Agent 3
    participant SC as Session Cache

    W->>SC: Create shared session cache
    W->>A1: Start with input
    A1->>SC: Add content hash
    A1->>W: Return output
    
    W->>A2: Pass output as input
    A2->>SC: Check hash (deduplicate)
    A2->>SC: Add new content hash
    A2->>W: Return output
    
    W->>A3: Pass output as input
    A3->>SC: Check hash (deduplicate)
    A3->>W: Return final output
```

## Terminology Reference

### Core Concepts

| Term | Definition | Default |
|------|------------|---------|
| **Context Window** | Maximum tokens an LLM can process in one request | Model-specific (128K for GPT-4o) |
| **Token Budget** | Allocated tokens for each context segment | Auto-calculated |
| **Compact Threshold** | Usage % that triggers optimization | 80% |
| **Output Reserve** | Tokens reserved for LLM response | 8000-16000 |

### Optimization Strategies

| Strategy | How It Works | Best For |
|----------|--------------|----------|
| **Truncate** | Removes oldest messages first | Simple chatbots |
| **Sliding Window** | Keeps N most recent messages | Long conversations |
| **Prune Tools** | Truncates old tool outputs | Tool-heavy agents |
| **Summarize** | Replaces old messages with summary | Critical context |
| **Smart** | Combines all strategies intelligently | Production use |

### Token Segments

```mermaid
pie title Token Budget Allocation (128K model)
    "System Prompt" : 2000
    "Tool Schemas" : 2000
    "Tool Outputs" : 20000
    "Memory/RAG" : 4000
    "History" : 84000
    "Output Reserve" : 16000
```

## Defaults Reference

### ContextConfig Defaults

```python
ContextConfig(
    auto_compact=True,           # Auto-optimize when threshold reached
    compact_threshold=0.8,       # Trigger at 80% usage
    strategy="smart",            # Use smart optimization
    output_reserve=8000,         # Reserve for LLM response
    history_ratio=0.6,           # 60% of usable for history
    tool_output_max=10000,       # Max tokens per tool output
    prune_after_tokens=40000,    # Start pruning after 40K
    keep_recent_turns=5,         # Keep last 5 turns intact
    tool_limits={},              # Per-tool output limits
)
```

### Per-Tool Limits

Configure different limits for different tools:

```yaml
context:
  auto_compact: true
  tool_limits:
    tavily_search: 2000      # Search results: 2000 chars
    tavily_extract: 5000     # Full page: 5000 chars
    code_executor: 10000     # Code output: 10000 chars
```

## Overflow Handling

```mermaid
stateDiagram-v2
    [*] --> Normal: < 70%
    Normal --> Warning: 70-80%
    Warning --> Critical: 80-90%
    Critical --> Emergency: 90-95%
    Emergency --> Overflow: > 95%
    
    Warning --> Normal: Optimization
    Critical --> Normal: Auto-compact
    Emergency --> Normal: Aggressive truncation
    Overflow --> Normal: Emergency truncation
```

| Level | Usage | Action |
|-------|-------|--------|
| Normal | < 70% | No action |
| Warning | 70-80% | Monitor |
| Critical | 80-90% | Auto-compact triggers |
| Emergency | 90-95% | Aggressive optimization |
| Overflow | > 95% | Emergency truncation |

## Session Deduplication

Prevents duplicate content across agents in multi-agent workflows:

```mermaid
flowchart LR
    subgraph Agent1["Agent 1"]
        C1["Content A"]
    end
    
    subgraph Cache["Session Cache"]
        H1["Hash A ‚úì"]
        H2["Hash B ‚úì"]
    end
    
    subgraph Agent2["Agent 2"]
        C2["Content A (skip)"]
        C3["Content B"]
    end
    
    subgraph Agent3["Agent 3"]
        C4["Content A (skip)"]
        C5["Content B (skip)"]
        C6["Content C"]
    end
    
    C1 --> H1
    C3 --> H2
    C2 -.->|"Duplicate"| H1
    C4 -.->|"Duplicate"| H1
    C5 -.->|"Duplicate"| H2
```

## CLI Commands

### Analytics Dashboard

```bash
# View analytics for recent sessions
praisonai replay dashboard

# Analyze specific session
praisonai replay dashboard <session_id>

# JSON output
praisonai replay dashboard --json
```

### Session Statistics

```bash
# View session stats
praisonai replay context <session_id> --stats
```

Output:
```
TOKEN USAGE BY AGENT:
  deep_researcher      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    956,146 (75.5%)
  content_writer       ‚ñà‚ñà‚ñà                               113,023 (8.9%)

CONTEXT EFFICIENCY:
  ‚ö†Ô∏è  Duplicates Found:  236
  ‚ö†Ô∏è  Wasted Tokens:     135,279 (10.7%)
```

## Smart Default Context

**New in v1.0**: Context management is now **automatically enabled** when your agent has tools:

```python
from praisonaiagents import Agent

# Context auto-enabled because tools are present
agent = Agent(
    instructions="You are helpful",
    tools=[my_search_tool]  # context=True automatically
)

# No tools = no context overhead
simple_agent = Agent(
    instructions="You are helpful"
    # context=None (disabled, zero overhead)
)

# Explicitly disable even with tools
agent = Agent(
    instructions="You are helpful",
    tools=[my_search_tool],
    context=False  # Override smart default
)
```

## Quick Start Examples

<CodeGroup>
```python Enable Context
from praisonaiagents import Agent

agent = Agent(
    instructions="You are helpful",
    context=True  # Enable with defaults (or auto-enabled with tools)
)
```

```python Custom Config
from praisonaiagents import Agent, ManagerConfig

agent = Agent(
    instructions="You are helpful",
    context=ManagerConfig(
        auto_compact=True,
        compact_threshold=0.7,
        strategy="smart",
        tool_limits={"tavily_search": 2000}
    )
)
```

```yaml YAML Config
context:
  auto_compact: true
  compact_threshold: 0.7
  strategy: smart
  tool_limits:
    tavily_search: 2000
    tavily_extract: 5000
```
</CodeGroup>

## Process Flow Diagrams

### Sequential Process Flow

How context flows through agents in sequential execution:

```mermaid
sequenceDiagram
    participant U as User Input
    participant W as Workflow
    participant A1 as Agent 1
    participant CM1 as Context Manager 1
    participant SC as Session Cache
    participant A2 as Agent 2
    participant CM2 as Context Manager 2
    participant A3 as Agent 3
    participant CM3 as Context Manager 3

    U->>W: Start task
    W->>SC: Create shared session cache
    
    W->>A1: Execute with input
    A1->>CM1: Process context
    CM1->>CM1: Estimate tokens
    CM1->>CM1: Apply budget
    CM1->>SC: Add content hashes
    A1->>W: Return output_1
    
    W->>A2: Execute with output_1
    A2->>CM2: Process context
    CM2->>SC: Check for duplicates
    Note over CM2,SC: Skip duplicate content
    CM2->>SC: Add new hashes
    A2->>W: Return output_2
    
    W->>A3: Execute with output_2
    A3->>CM3: Process context
    CM3->>SC: Check for duplicates
    A3->>W: Return final output
    
    W->>U: Return result
```

### Hierarchical Process Flow

How context flows in manager-worker hierarchies:

```mermaid
sequenceDiagram
    participant U as User
    participant M as Manager Agent
    participant MCM as Manager Context
    participant SC as Session Cache
    participant W1 as Worker 1
    participant W2 as Worker 2
    participant W3 as Worker 3

    U->>M: Complex task
    M->>MCM: Process context
    MCM->>SC: Initialize cache
    
    M->>M: Decompose task
    
    par Parallel Workers
        M->>W1: Subtask 1
        W1->>SC: Check/add hashes
        W1->>M: Result 1
    and
        M->>W2: Subtask 2
        W2->>SC: Check/add hashes
        W2->>M: Result 2
    and
        M->>W3: Subtask 3
        W3->>SC: Check/add hashes
        W3->>M: Result 3
    end
    
    M->>MCM: Aggregate results
    MCM->>MCM: Deduplicate
    M->>U: Final synthesis
```

### Workflow Process Flow

How context flows through workflow steps:

```mermaid
flowchart TB
    subgraph Workflow["Workflow Execution"]
        direction TB
        START([Start]) --> CACHE[Create Session Cache]
        CACHE --> STEP1
        
        subgraph STEP1["Step 1: Research"]
            A1[Agent 1] --> CM1[Context Manager]
            CM1 --> OPT1{Over threshold?}
            OPT1 -->|Yes| COMPACT1[Auto-compact]
            OPT1 -->|No| PASS1[Pass through]
            COMPACT1 --> OUT1[Output 1]
            PASS1 --> OUT1
        end
        
        OUT1 --> STEP2
        
        subgraph STEP2["Step 2: Analyze"]
            A2[Agent 2] --> CM2[Context Manager]
            CM2 --> DEDUP2[Deduplicate vs Cache]
            DEDUP2 --> OPT2{Over threshold?}
            OPT2 -->|Yes| COMPACT2[Auto-compact]
            OPT2 -->|No| PASS2[Pass through]
            COMPACT2 --> OUT2[Output 2]
            PASS2 --> OUT2
        end
        
        OUT2 --> STEP3
        
        subgraph STEP3["Step 3: Write"]
            A3[Agent 3] --> CM3[Context Manager]
            CM3 --> DEDUP3[Deduplicate vs Cache]
            DEDUP3 --> OUT3[Final Output]
        end
        
        OUT3 --> END([End])
    end
```

## Optimization Strategy Diagrams

### Truncate Strategy

```mermaid
flowchart LR
    subgraph Before["Before (150K tokens)"]
        M1[Msg 1] --> M2[Msg 2] --> M3[Msg 3] --> M4[Msg 4] --> M5[Msg 5]
    end
    
    Before --> TRUNC[Truncate Oldest]
    
    subgraph After["After (100K tokens)"]
        M3A[Msg 3] --> M4A[Msg 4] --> M5A[Msg 5]
    end
    
    TRUNC --> After
    
    style M1 fill:#ff6b6b
    style M2 fill:#ff6b6b
    style M3A fill:#51cf66
    style M4A fill:#51cf66
    style M5A fill:#51cf66
```

### Sliding Window Strategy

```mermaid
flowchart LR
    subgraph Before["Before (20 messages)"]
        direction LR
        OLD[Msgs 1-15<br/>Old] --> RECENT[Msgs 16-20<br/>Recent]
    end
    
    Before --> WINDOW[Keep Last N]
    
    subgraph After["After (5 messages)"]
        KEPT[Msgs 16-20<br/>Preserved]
    end
    
    WINDOW --> After
    
    style OLD fill:#ff6b6b
    style RECENT fill:#51cf66
    style KEPT fill:#51cf66
```

### Prune Tools Strategy

```mermaid
flowchart TB
    subgraph Before["Before Pruning"]
        T1["Tool Output 1<br/>50K tokens"] 
        T2["Tool Output 2<br/>30K tokens"]
        T3["Tool Output 3<br/>20K tokens"]
    end
    
    Before --> PRUNE[Truncate to Limit]
    
    subgraph After["After Pruning"]
        T1A["Tool Output 1<br/>10K tokens ‚úÇÔ∏è"]
        T2A["Tool Output 2<br/>10K tokens ‚úÇÔ∏è"]
        T3A["Tool Output 3<br/>10K tokens ‚úÇÔ∏è"]
    end
    
    PRUNE --> After
    
    style T1 fill:#ff6b6b
    style T2 fill:#ffa94d
    style T3 fill:#51cf66
    style T1A fill:#51cf66
    style T2A fill:#51cf66
    style T3A fill:#51cf66
```

### Summarize Strategy

```mermaid
flowchart TB
    subgraph Before["Before (50 messages)"]
        OLD[Messages 1-45<br/>Old History]
        RECENT[Messages 46-50<br/>Recent]
    end
    
    OLD --> LLM[LLM Summarization]
    LLM --> SUMMARY["[Summary]<br/>Key points from<br/>45 messages"]
    
    subgraph After["After (6 messages)"]
        SUMMARY
        RECENT2[Messages 46-50<br/>Preserved]
    end
    
    RECENT --> RECENT2
    
    style OLD fill:#ff6b6b
    style SUMMARY fill:#74c0fc
    style RECENT fill:#51cf66
    style RECENT2 fill:#51cf66
```

### Smart Strategy (Combined)

```mermaid
flowchart TB
    START[Over Budget] --> STEP1
    
    subgraph STEP1["Step 1: Prune Tools"]
        PRUNE[Truncate tool outputs]
    end
    
    STEP1 --> CHECK1{Under budget?}
    CHECK1 -->|Yes| DONE[Done ‚úì]
    CHECK1 -->|No| STEP2
    
    subgraph STEP2["Step 2: Sliding Window"]
        WINDOW[Keep recent messages]
    end
    
    STEP2 --> CHECK2{Under budget?}
    CHECK2 -->|Yes| DONE
    CHECK2 -->|No| STEP3
    
    subgraph STEP3["Step 3: Summarize"]
        SUMM[LLM summarization]
    end
    
    STEP3 --> DONE
    
    style STEP1 fill:#74c0fc
    style STEP2 fill:#ffa94d
    style STEP3 fill:#ff6b6b
    style DONE fill:#51cf66
```

## Context Overflow Handling

### What Happens When Context Exceeds Limits

```mermaid
flowchart TD
    INPUT[New Message] --> ESTIMATE[Estimate Total Tokens]
    ESTIMATE --> CHECK{Usage Level?}
    
    CHECK -->|"< 70%"| NORMAL[Normal: No Action]
    CHECK -->|"70-80%"| WARNING[Warning: Monitor]
    CHECK -->|"80-90%"| CRITICAL[Critical: Auto-Compact]
    CHECK -->|"90-95%"| EMERGENCY[Emergency: Aggressive Optimization]
    CHECK -->|"> 95%"| OVERFLOW[Overflow: Emergency Truncation]
    
    CRITICAL --> SMART[Apply Smart Strategy]
    EMERGENCY --> AGGRESSIVE[Prune + Window + Summarize]
    OVERFLOW --> TRUNCATE[Remove Oldest Until Safe]
    
    SMART --> CONTINUE[Continue Processing]
    AGGRESSIVE --> CONTINUE
    TRUNCATE --> CONTINUE
    NORMAL --> CONTINUE
    WARNING --> CONTINUE
    
    style NORMAL fill:#51cf66
    style WARNING fill:#ffd43b
    style CRITICAL fill:#ffa94d
    style EMERGENCY fill:#ff6b6b
    style OVERFLOW fill:#c92a2a
```

### Tool Output Overflow Handling

```mermaid
flowchart TD
    TOOL[Tool Execution] --> OUTPUT[Tool Output]
    OUTPUT --> CHECK{Output Size?}
    
    CHECK -->|"< tool_limit"| PASS[Pass Through]
    CHECK -->|"> tool_limit"| TRUNCATE[Truncate to Limit]
    
    TRUNCATE --> MARKER["Add [truncated] marker"]
    
    PASS --> CONTEXT[Add to Context]
    MARKER --> CONTEXT
    
    CONTEXT --> BUDGET{Within Budget?}
    BUDGET -->|Yes| DONE[Continue]
    BUDGET -->|No| OPTIMIZE[Apply Optimization Strategy]
    OPTIMIZE --> DONE
    
    style PASS fill:#51cf66
    style TRUNCATE fill:#ffa94d
    style OPTIMIZE fill:#74c0fc
```

## LLM Summarization

Enable intelligent summarization using the agent's LLM:

```python
from praisonaiagents import Agent, ContextConfig

agent = Agent(
    instructions="You are helpful",
    context=ContextConfig(
        auto_compact=True,
        llm_summarize=True,  # Enable LLM-powered summarization
        strategy="smart"
    )
)
```

When `llm_summarize=True`:
- Old messages are summarized by the LLM instead of truncated
- Key facts, decisions, and context are preserved
- More intelligent compression than simple truncation

## Related Pages

- [Context Strategies](/features/context-strategies) - Detailed strategy reference
- [Context Budgeter](/features/context-budgeter) - Token budgeting
- [Context Optimizer](/features/context-optimizer) - Optimization details
- [Context Replay](/features/replay) - Debugging and analysis
