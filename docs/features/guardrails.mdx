---
title: "Guardrails"
description: "Implement validation and quality assurance for agent outputs"
icon: "shield"
---

# Guardrails

Guardrails provide validation and quality assurance mechanisms for agent outputs, supporting both function-based and LLM-based validation to ensure outputs meet specific criteria.

## Overview

Guardrails allow you to:
- Validate agent outputs before they're returned
- Implement custom validation logic
- Use LLM-based validation with natural language criteria
- Retry operations when validation fails
- Transform outputs to meet requirements

## Quick Start

```python
from praisonaiagents import Agent, Task
from praisonaiagents.agents.guardrails import LLMGuardrail

# Create LLM-based guardrail
guardrail = LLMGuardrail(
    description="Ensure the response is professional and under 100 words",
    llm=None  # Uses agent's LLM
)

# Create agent with guardrailed task
agent = Agent(
    name="Customer Service",
    role="Support representative"
)

task = Task(
    description="Respond to angry customer email",
    agent=agent,
    guardrail=guardrail,
    max_retries=3  # Retry up to 3 times if validation fails
)
```

## Guardrail Types
### Function-Based Guardrails
Function guardrails provide programmatic validation:

```python
from praisonaiagents import TaskOutput

def length_guardrail(output: TaskOutput) -> tuple[bool, any]:
    """Ensure output is within length limits"""
    if len(output.raw) > 500:
        return False, "Output too long, please summarize"
    if len(output.raw) < 50:
        return False, "Output too short, please elaborate"
    return True, output

task = Task(
    description="Write a product description",
    guardrail=length_guardrail
)
```

### LLM-Based Guardrails
LLM guardrails use natural language validation:

```python
from praisonaiagents.agents.guardrails import LLMGuardrail

# Simple string description
task = Task(
    description="Generate SQL query",
    guardrail="Ensure the SQL query only reads data and contains no DELETE, DROP, or UPDATE statements"
)

# Or using LLMGuardrail class
guardrail = LLMGuardrail(
    description="Validate that the response contains no personally identifiable information (PII) such as names, addresses, phone numbers, or email addresses",
    llm=agent.llm
)
```

## Validation Process
### How It Works
1. Agent executes the task and produces output
2. Guardrail validates the output
3. If validation fails:
   
   * Task retries (up to `max_retries` times)
   * Agent receives feedback about what to fix
4. If validation passes:
   
   * Output is returned as final result

### Validation Response Format
Function guardrails return a tuple:

```python
(success: bool, result: any)
```
Where:

* `success`: Whether validation passed
* `result`: Modified output or error message

## Advanced Examples
### Content Moderation
```python
def content_guardrail(output: TaskOutput) -> tuple[bool, any]:
    """Ensure content is appropriate"""
    prohibited_words = ["spam", "scam", "illegal"]
    content_lower = output.raw.lower()
    
    for word in prohibited_words:
        if word in content_lower:
            return False, f"Content contains prohibited word: {word}"
    
    return True, output

agent = Agent(
    name="Content Creator",
    role="Blog writer"
)

task = Task(
    description="Write article about online safety",
    agent=agent,
    guardrail=content_guardrail
)
```

### Data Format Validation
```python
import json

def json_guardrail(output: TaskOutput) -> tuple[bool, any]:
    """Ensure output is valid JSON with required fields"""
    try:
        data = json.loads(output.raw)
        required_fields = ["id", "name", "price"]
        
        for field in required_fields:
            if field not in data:
                return False, f"Missing required field: {field}"
        
        if not isinstance(data["price"], (int, float)):
            return False, "Price must be a number"
            
        return True, output
    except json.JSONDecodeError:
        return False, "Output must be valid JSON"

task = Task(
    description="Generate product data in JSON format",
    guardrail=json_guardrail,
    max_retries=5
)
```

### Combining Multiple Validations
```python
def combined_guardrail(output: TaskOutput) -> tuple[bool, any]:
    """Apply multiple validation checks"""
    checks = [
        (lambda o: len(o.raw) < 1000, "Output too long"),
        (lambda o: "copyright" not in o.raw.lower(), "Contains copyright material"),
        (lambda o: o.raw.count("\n") < 10, "Too many line breaks")
    ]
    
    for check_func, error_msg in checks:
        if not check_func(output):
            return False, error_msg
    
    return True, output
```

### Output Transformation
```python
def transform_guardrail(output: TaskOutput) -> tuple[bool, any]:
    """Transform output to meet requirements"""
    # Clean up the output
    cleaned = output.raw.strip()
    cleaned = " ".join(cleaned.split())  # Normalize whitespace
    
    # Add required prefix/suffix
    if not cleaned.startswith("Summary:"):
        cleaned = f"Summary: {cleaned}"
    
    # Create new output with transformation
    new_output = TaskOutput(
        description=output.description,
        agent=output.agent,
        raw=cleaned,
        json_output=output.json_output,
        output_format=output.output_format
    )
    
    return True, new_output
```

## Complex LLM Validation
### Multi-Criteria Validation
```python
guardrail = LLMGuardrail(
    description="""
    Validate the customer service response meets these criteria:
    1. Professional and empathetic tone
    2. Addresses all customer concerns mentioned
    3. Provides clear next steps or resolution
    4. Includes appropriate greeting and closing
    5. No grammar or spelling errors
    6. Between 50-150 words
    """,
    llm=agent.llm
)
```

### Domain-Specific Validation
```python
medical_guardrail = LLMGuardrail(
    description="""
    Ensure the medical information:
    - Contains disclaimer about consulting healthcare professionals
    - Avoids definitive diagnoses
    - References only peer-reviewed sources
    - Uses appropriate medical terminology
    - Includes no treatment recommendations without professional consultation
    """,
    llm=agent.llm
)
```

## Integration Patterns
### With PraisonAIAgents
```python
from praisonaiagents import PraisonAIAgents

agents = PraisonAIAgents(
    agents=[agent1, agent2],
    tasks=[
        Task(
            description="Generate report",
            agent=agent1,
            guardrail="Ensure report includes executive summary, findings, and recommendations"
        ),
        Task(
            description="Review report",
            agent=agent2,
            guardrail=quality_guardrail
        )
    ]
)
```

### Conditional Guardrails
```python
def get_guardrail(task_type):
    """Return appropriate guardrail based on task type"""
    if task_type == "code":
        return "Ensure code is syntactically correct and includes comments"
    elif task_type == "email":
        return "Ensure email is professional and under 200 words"
    else:
        return None

task = Task(
    description="Write Python function",
    guardrail=get_guardrail("code")
)
```

### Async Guardrails
```python
async def async_guardrail(output: TaskOutput) -> tuple[bool, any]:
    """Async validation with external API"""
    # Check with external service
    is_valid = await check_content_api(output.raw)
    
    if not is_valid:
        return False, "Content failed external validation"
    
    return True, output
```

## Error Handling
```python
from praisonaiagents.agents.guardrails import GuardrailResult

try:
    # Task with guardrail
    result = task.execute()
except Exception as e:
    # Guardrail validation failed after all retries
    print(f"Task failed validation: {e}")
```

## Best Practices
1. **Clear Criteria** - Make validation criteria specific and measurable
2. **Helpful Feedback** - Provide clear error messages for failed validations
3. **Appropriate Retries** - Set `max_retries` based on task complexity
4. **Performance** - Consider validation overhead, especially for LLM-based guardrails
5. **Combine Approaches** - Use function guardrails for simple checks, LLM for complex validation
6. **Test Thoroughly** - Test guardrails with various outputs including edge cases
7. **Fail Gracefully** - Have fallback behavior when validation consistently fails