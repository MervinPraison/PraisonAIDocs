---
title: "Token Budgeting"
description: "Dynamic token budget management for large context handling"
icon: "calculator"
---

# Token Budgeting

Token budgeting enables intelligent management of context windows across different LLM models, ensuring optimal use of available tokens while reserving space for responses.

## Overview

The token budgeting system provides:
- **Dynamic budget calculation** based on model context windows
- **Reserved token allocation** for system prompts, history, and responses
- **Automatic enforcement** to prevent context overflow
- **Model-aware defaults** for popular LLM providers

## Quick Start

```python
from praisonaiagents.rag import TokenBudget, DefaultBudgetEnforcer

# Create budget for a specific model
budget = TokenBudget(model="gpt-4o-mini")

# Calculate available tokens
available = budget.dynamic_budget(
    system_tokens=500,
    history_tokens=1000,
)

print(f"Model context window: {budget.model_context_window}")
print(f"Available for context: {available}")
```

## Token Budget Configuration

### Basic Configuration

```python
from praisonaiagents.rag import TokenBudget

# Default budget (uses gpt-4o-mini defaults)
budget = TokenBudget()

# Custom budget with specific reservations
budget = TokenBudget(
    model="gpt-4o",
    reserved_response=4096,
    reserved_system=1000,
    reserved_history=2000,
)
```

### Model Context Windows

The system automatically detects context windows for popular models:

| Model | Context Window |
|-------|---------------|
| gpt-4o | 128,000 |
| gpt-4o-mini | 128,000 |
| gpt-4-turbo | 128,000 |
| gpt-3.5-turbo | 16,385 |
| claude-3-opus | 200,000 |
| claude-3-sonnet | 200,000 |
| gemini-pro | 32,000 |

## Budget Enforcement

### Using the Budget Enforcer

```python
from praisonaiagents.rag import TokenBudget, DefaultBudgetEnforcer

budget = TokenBudget(model="gpt-4o-mini")
enforcer = DefaultBudgetEnforcer()

# Enforce budget on retrieved chunks
chunks = ["chunk1 content...", "chunk2 content...", "chunk3 content..."]
enforced_chunks = enforcer.enforce(budget, chunks)

print(f"Original chunks: {len(chunks)}")
print(f"After enforcement: {len(enforced_chunks)}")
```

### Custom Enforcement Strategies

```python
from praisonaiagents.rag import BudgetEnforcerProtocol, TokenBudget
from typing import List

class PriorityBudgetEnforcer(BudgetEnforcerProtocol):
    """Enforce budget with priority-based selection."""
    
    def enforce(self, budget: TokenBudget, chunks: List[str]) -> List[str]:
        available = budget.dynamic_budget()
        result = []
        used = 0
        
        for chunk in chunks:
            chunk_tokens = len(chunk) // 4  # Simple estimation
            if used + chunk_tokens <= available:
                result.append(chunk)
                used += chunk_tokens
            else:
                break
        
        return result
```

## Integration with Agents

### Agent with Token Budget

```python
from praisonaiagents import Agent

agent = Agent(
    name="BudgetAwareAgent",
    instructions="Answer questions using the knowledge base.",
    knowledge={
        "sources": ["./docs"],
        "retrieval_k": 5,
    }
)

response = agent.chat("What are the key features?")
```

## CLI Usage

```bash
# Check token budget for a model
praisonai knowledge stats --json

# Index with token-aware chunking
praisonai knowledge index ./docs --verbose
```

## Best Practices

1. **Reserve adequate response tokens** - Ensure enough space for complete responses
2. **Monitor token usage** - Use verbose mode to track actual usage
3. **Adjust for model** - Different models have different optimal chunk sizes
4. **Consider history** - Multi-turn conversations consume tokens quickly

## API Reference

### TokenBudget

```python
@dataclass
class TokenBudget:
    model: str = "gpt-4o-mini"
    reserved_response: int = 4096
    reserved_system: int = 500
    reserved_history: int = 1000
    
    @property
    def model_context_window(self) -> int:
        """Get context window for the model."""
        
    @property
    def max_context_tokens(self) -> int:
        """Maximum tokens available for context."""
        
    def dynamic_budget(
        self,
        system_tokens: int = 0,
        history_tokens: int = 0,
    ) -> int:
        """Calculate available tokens dynamically."""
```

### DefaultBudgetEnforcer

```python
class DefaultBudgetEnforcer(BudgetEnforcerProtocol):
    def enforce(
        self,
        budget: TokenBudget,
        chunks: List[str],
    ) -> List[str]:
        """Enforce token budget on chunks."""
```
