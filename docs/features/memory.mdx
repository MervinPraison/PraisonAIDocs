---
title: "Agent Memory"
description: "Zero-dependency persistent memory for AI agents"
icon: "memory"
---

# Agent Memory (Zero Dependencies)

Enable persistent memory for agents without any extra packages. Memory is automatically injected into conversations, allowing agents to remember user preferences, facts, and context across sessions.

<Note>
**Looking for multi-agent memory?** See [Memory Concepts](/concepts/memory) for using memory with `PraisonAIAgents` class and multi-agent workflows.
</Note>

```mermaid
graph TB
    subgraph Memory Types
        STM[âš¡ Short-term Memory]
        LTM[ðŸ’¾ Long-term Memory]
        ENT[ðŸ‘¤ Entity Memory]
        EPI[ðŸ“… Episodic Memory]
    end
    
    subgraph Storage
        JSON[(ðŸ“„ JSON Files)]
    end
    
    subgraph Agent
        SYS[ðŸ“ System Prompt]
        LLM[ðŸ¤– LLM]
    end
    
    STM --> JSON
    LTM --> JSON
    ENT --> JSON
    EPI --> JSON
    
    JSON --> SYS
    SYS --> LLM
    
    classDef memory fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef storage fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef agent fill:#189AB4,stroke:#7C90A0,color:#fff
    
    class STM,LTM,ENT,EPI memory
    class JSON storage
    class SYS,LLM agent
```

## Quick Start

<CodeGroup>
```python Basic Usage
from praisonaiagents import Agent

# Enable memory with a single parameter
agent = Agent(
    name="Personal Assistant",
    instructions="You are a helpful assistant that remembers user preferences.",
    memory=True  # Enables file-based memory (no extra deps!)
)

# Memory is automatically injected into conversations
result = agent.start("My name is John and I prefer dark mode")
result = agent.start("What's my name?")  # Agent recalls: "John"
```

```python With User Isolation
from praisonaiagents import Agent

# Isolate memory per user
agent = Agent(
    name="Assistant",
    memory=True,
    user_id="user_123"  # Each user gets separate memory storage
)

# Store memories programmatically
agent.store_memory("User prefers concise answers", memory_type="short_term")
agent.store_memory("User is a software engineer", memory_type="long_term", importance=0.9)

# Get memory context
context = agent.get_memory_context()
print(context)
```

```python Memory Persistence
from praisonaiagents import Agent

# First session - store memories
agent1 = Agent(name="Assistant", memory=True, user_id="john_doe")
agent1.store_memory("User's favorite color is blue", memory_type="long_term", importance=0.8)
agent1.start("Remember that I work at Acme Corp")

# Later session - memories persist!
agent2 = Agent(name="Assistant", memory=True, user_id="john_doe")
result = agent2.start("Where do I work?")  # Agent recalls: "Acme Corp"
```
</CodeGroup>

## Memory Types

<CardGroup cols={2}>
  <Card title="Short-term Memory" icon="bolt">
    Rolling buffer of recent context. Auto-expires when limit reached. High-importance items can be auto-promoted to long-term.
  </Card>
  <Card title="Long-term Memory" icon="database">
    Persistent important facts sorted by importance score. Lowest importance items removed when limit reached.
  </Card>
  <Card title="Entity Memory" icon="user">
    Named entities (people, places, organizations) with attributes and relationships.
  </Card>
  <Card title="Episodic Memory" icon="calendar">
    Date-based interaction history. Configurable retention period with automatic cleanup.
  </Card>
</CardGroup>

## Storage Structure

Memory is stored in JSON files under `.praison/memory/{user_id}/`:

```
.praison/memory/user_123/
â”œâ”€â”€ config.json        # Memory configuration
â”œâ”€â”€ short_term.json    # Rolling buffer (recent context)
â”œâ”€â”€ long_term.json     # Persistent facts
â”œâ”€â”€ entities.json      # Named entities
â”œâ”€â”€ episodic/          # Date-based memories
â”‚   â”œâ”€â”€ 2024-12-15.json
â”‚   â””â”€â”€ 2024-12-14.json
â””â”€â”€ summaries.json     # LLM-generated summaries
```

## Configuration Options

```python
from praisonaiagents import Agent

# Full configuration
agent = Agent(
    name="Assistant",
    memory={
        "provider": "file",           # Storage provider
        "user_id": "user_123",        # User isolation
        "short_term_limit": 100,      # Max short-term items
        "long_term_limit": 1000,      # Max long-term items
        "importance_threshold": 0.7,  # Min importance for auto-promotion
        "auto_promote": True,         # Auto-promote high-importance items
        "episodic_retention_days": 30 # Days to keep episodic memories
    }
)
```

## Memory Methods

### Store Memory

```python
# Short-term (recent context)
agent.store_memory("User asked about Python", memory_type="short_term")

# Long-term (important facts)
agent.store_memory("User's name is Alice", memory_type="long_term", importance=0.95)

# Entity
agent._memory_instance.add_entity(
    name="Alice",
    entity_type="person",
    attributes={"role": "developer", "company": "Acme"}
)

# Episodic (date-based)
agent._memory_instance.add_episodic("Had a meeting about project X")
```

### Retrieve Memory

```python
# Get memory context for prompts
context = agent.get_memory_context(query="user preferences")

# Search memories
results = agent._memory_instance.search("Python", limit=10)

# Get specific memory types
short_term = agent._memory_instance.get_short_term(limit=10)
long_term = agent._memory_instance.get_long_term(limit=10)
entities = agent._memory_instance.get_all_entities(entity_type="person")
```

### Memory Management

```python
# Clear short-term memory
agent._memory_instance.clear_short_term()

# Clear all memory
agent._memory_instance.clear_all()

# Get statistics
stats = agent._memory_instance.get_stats()
print(stats)
# {'user_id': 'user_123', 'short_term_count': 5, 'long_term_count': 10, ...}

# Export/Import
data = agent._memory_instance.export()
agent._memory_instance.import_data(data)
```

## Storage Providers

| Provider | Dependencies | Description |
|----------|-------------|-------------|
| `memory=True` | None | File-based JSON storage (default) |
| `memory="file"` | None | Explicit file-based storage |
| `memory="sqlite"` | Built-in | SQLite with indexing |
| `memory="chromadb"` | chromadb | Vector/semantic search |
| `memory="mem0"` | mem0ai | Graph memory, cloud |

## How Memory Injection Works

When `memory=True`, the agent automatically:

1. **Loads** existing memories from JSON files on initialization
2. **Builds** a memory context string with important facts, entities, and recent context
3. **Injects** the context into the system prompt before each LLM call
4. **Persists** new memories to JSON files after storage

```python
# System prompt with memory injection looks like:
"""
You are a helpful assistant.

Your Role: Assistant
Your Goal: Help users with their tasks

## Memory (Information you remember about the user)
## Important Facts
- User's name is Alice
- User works as a software engineer
## Known Entities
- Alice (person): role=developer, company=Acme
## Recent Context
- User prefers detailed explanations
"""
```

## Best Practices

<AccordionGroup>
  <Accordion title="Use importance scores wisely">
    Set higher importance (0.8-1.0) for critical facts like user names, preferences, and key information. Lower importance (0.3-0.5) for transient context.
  </Accordion>
  <Accordion title="Isolate memory per user">
    Always set `user_id` when building multi-user applications to prevent memory leakage between users.
  </Accordion>
  <Accordion title="Clean up old memories">
    Call `cleanup_episodic()` periodically to remove old date-based memories and save storage space.
  </Accordion>
  <Accordion title="Use entities for structured data">
    Store people, places, and organizations as entities with attributes rather than plain text for better retrieval.
  </Accordion>
</AccordionGroup>

## Advanced: Direct FileMemory Usage

```python
from praisonaiagents.memory import FileMemory, create_memory

# Create memory instance directly
memory = FileMemory(
    user_id="user_123",
    base_path=".praison/memory",
    config={"short_term_limit": 50},
    verbose=1
)

# Use all memory features
memory.add_short_term("Recent interaction")
memory.add_long_term("Important fact", importance=0.9)
memory.add_entity("John", "person", {"role": "manager"})
memory.add_episodic("Meeting notes")

# Search and retrieve
results = memory.search("John")
context = memory.get_context(query="manager")

# Convenience function
memory = create_memory(user_id="user_456")
```

## Session Save/Resume (like Gemini CLI)

Save and resume conversation sessions for later continuation:

```python
from praisonaiagents.memory import FileMemory

memory = FileMemory(user_id="user_123")

# Add context during conversation
memory.add_short_term("User is working on ML project")
memory.add_long_term("User prefers Python", importance=0.9)

# Save session with conversation history
conversation = [
    {"role": "user", "content": "Help me with ML"},
    {"role": "assistant", "content": "I'd be happy to help..."}
]
memory.save_session("ml_project", conversation_history=conversation)

# Later: Resume the session
session_data = memory.resume_session("ml_project")
# Short-term memory is restored, conversation history available

# List all saved sessions
sessions = memory.list_sessions()
for s in sessions:
    print(f"{s['name']} - saved at {s['saved_at']}")

# Delete a session
memory.delete_session("old_session")
```

## Context Compression (like Gemini CLI)

Compress short-term memory to save context window space:

```python
from praisonaiagents.memory import FileMemory

memory = FileMemory(user_id="user_123", config={"short_term_limit": 100})

# Add many items during conversation
for i in range(50):
    memory.add_short_term(f"Discussion point {i}")

# Manual compression with LLM summarization
def llm_summarize(prompt):
    # Use your LLM to summarize
    return agent.chat(prompt)

summary = memory.compress(llm_func=llm_summarize, max_items=10)
# Compresses older items into a summary, keeps recent 10

# Auto-compress when memory gets full (70% threshold)
memory.auto_compress_if_needed(threshold_percent=0.7, llm_func=llm_summarize)
```

## Checkpointing (like Gemini CLI)

Create checkpoints before risky operations and restore if needed:

```python
from praisonaiagents.memory import FileMemory

memory = FileMemory(user_id="user_123")

# Create checkpoint before making changes
checkpoint_id = memory.create_checkpoint("before_refactor")

# Optionally include file snapshots
checkpoint_id = memory.create_checkpoint(
    "before_refactor",
    include_files=["main.py", "config.yaml"]
)

# Make changes...
memory.clear_all()
# Something went wrong!

# Restore from checkpoint
memory.restore_checkpoint(checkpoint_id)

# Restore with file snapshots
memory.restore_checkpoint(checkpoint_id, restore_files=True)

# List all checkpoints
checkpoints = memory.list_checkpoints()

# Delete old checkpoint
memory.delete_checkpoint("old_checkpoint")
```

## Memory Slash Commands

Handle memory commands programmatically (useful for CLI/chat interfaces):

```python
from praisonaiagents.memory import FileMemory

memory = FileMemory(user_id="user_123")

# Available commands
result = memory.handle_command("/memory show")      # Display stats
result = memory.handle_command("/memory add User likes coffee")  # Add memory
result = memory.handle_command("/memory search Python")  # Search
result = memory.handle_command("/memory clear short")    # Clear short-term
result = memory.handle_command("/memory save my_session")  # Save session
result = memory.handle_command("/memory resume my_session")  # Resume
result = memory.handle_command("/memory sessions")   # List sessions
result = memory.handle_command("/memory compress")   # Compress
result = memory.handle_command("/memory checkpoint") # Create checkpoint
result = memory.handle_command("/memory restore cp_123")  # Restore
result = memory.handle_command("/memory checkpoints")  # List checkpoints
result = memory.handle_command("/memory refresh")    # Reload from disk
result = memory.handle_command("/memory help")       # Show all commands
```

**Available Commands:**

| Command | Description |
|---------|-------------|
| `/memory show` | Display memory stats and recent items |
| `/memory add <content>` | Add to long-term memory |
| `/memory clear [short\|all]` | Clear memory |
| `/memory search <query>` | Search memories |
| `/memory save <name>` | Save session |
| `/memory resume <name>` | Resume session |
| `/memory sessions` | List saved sessions |
| `/memory compress` | Compress short-term memory |
| `/memory checkpoint [name]` | Create checkpoint |
| `/memory restore <id>` | Restore checkpoint |
| `/memory checkpoints` | List checkpoints |
| `/memory refresh` | Reload from disk |

## Auto-Generated Memories (like Windsurf Cascade)

Automatically extract and store memories from conversations without manual intervention:

```python
from praisonaiagents.memory import FileMemory, AutoMemory

# Create base memory
memory = FileMemory(user_id="user123")

# Wrap with auto-generation
auto = AutoMemory(memory, enabled=True)

# Process interactions - memories are automatically extracted
memories = auto.process_interaction(
    user_message="My name is John and I prefer Python for backend work",
    assistant_response="Nice to meet you, John! Python is great for backend."
)

# Extracted memories:
# - name: "John" (entity)
# - preference: "Python for backend work" (long-term)

print(f"Extracted {len(memories)} memories automatically")
```

### Pattern-Based Extraction

AutoMemory uses fast pattern matching (no LLM calls) to extract:

| Type | Examples | Importance |
|------|----------|------------|
| **Name** | "My name is John", "I'm Alice" | 0.95 |
| **Role** | "I'm a developer", "I work as an engineer" | 0.85 |
| **Preference** | "I prefer Python", "I like dark mode" | 0.70 |
| **Project** | "Working on ML project", "Building an app" | 0.75 |
| **Technology** | "Using Python", "prefer TypeScript" | 0.70 |
| **Location** | "I live in NYC", "Based in London" | 0.60 |

### LLM-Enhanced Extraction

For better accuracy, enable LLM-based extraction:

```python
from praisonaiagents.memory import AutoMemory, AutoMemoryExtractor

# Create extractor with LLM
extractor = AutoMemoryExtractor(
    min_importance=0.6,
    use_llm=True,
    llm_func=lambda prompt: agent.chat(prompt)
)

# Use with AutoMemory
auto = AutoMemory(
    memory=memory,
    extractor=extractor,
    enabled=True
)
```

### Quick Filter

Check if text likely contains memorable content before full extraction:

```python
from praisonaiagents.memory import AutoMemoryExtractor

extractor = AutoMemoryExtractor()

# Fast check (no extraction)
if extractor.should_remember("My name is John"):
    memories = extractor.extract("My name is John")
```

## See Also

<CardGroup cols={2}>
  <Card title="Advanced Memory" icon="brain" href="/features/advanced-memory">
    Multi-tiered memory with quality scoring, ChromaDB, and graph support
  </Card>
  <Card title="Graph Memory" icon="project-diagram" href="/features/graph-memory">
    Neo4j/Memgraph integration for relationship-based memory
  </Card>
  <Card title="Rules & Instructions" icon="scroll" href="/features/rules">
    Auto-discover and apply persistent rules like Cursor and Windsurf
  </Card>
  <Card title="Workflows" icon="diagram-project" href="/features/workflows">
    Create reusable multi-step workflows
  </Card>
</CardGroup>
