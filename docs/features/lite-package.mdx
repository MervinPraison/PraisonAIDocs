---
title: "Lite Package (BYO-LLM)"
description: "Lightweight agent framework without heavy dependencies"
icon: "feather"
---

# Lite Package (BYO-LLM)

The `praisonaiagents.lite` subpackage provides a minimal agent framework that lets you bring your own LLM client. It has no dependency on `litellm` and uses minimal memory.

## Installation

The lite package is included in praisonaiagents v0.5.0+:

```bash
pip install praisonaiagents>=0.5.0
```

## Quick Start

```python
from praisonaiagents.lite import LiteAgent, create_openai_llm_fn

# Create an LLM function using the OpenAI SDK directly
llm_fn = create_openai_llm_fn(model="gpt-4o-mini")

# Create a lite agent
agent = LiteAgent(
    name="MyAgent",
    llm_fn=llm_fn,
    instructions="You are a helpful assistant."
)

# Chat with the agent
response = agent.chat("Hello!")
print(response)
```

## Components

### LiteAgent

The main agent class with thread-safe chat history:

```python
from praisonaiagents.lite import LiteAgent

agent = LiteAgent(
    name="MyAgent",
    llm_fn=my_llm_function,
    instructions="System instructions",
    tools=[my_tool]  # Optional tools
)

# Chat
response = agent.chat("Hello")

# Access chat history (thread-safe)
print(agent.chat_history)

# Clear history
agent.clear_history()
```

### Custom LLM Functions

Bring your own LLM by providing a function that takes messages and returns a string:

```python
def my_custom_llm(messages):
    """
    Args:
        messages: List of dicts with 'role' and 'content'
    Returns:
        str: The assistant's response
    """
    # Your LLM implementation here
    return "Response from my LLM"

agent = LiteAgent(name="Agent", llm_fn=my_custom_llm)
```

### Built-in LLM Adapters

#### OpenAI Adapter

```python
from praisonaiagents.lite import create_openai_llm_fn

# Requires OPENAI_API_KEY environment variable
llm_fn = create_openai_llm_fn(
    model="gpt-4o-mini",
    temperature=0.7,
    max_tokens=1000
)
```

#### Anthropic Adapter

```python
from praisonaiagents.lite import create_anthropic_llm_fn

# Requires ANTHROPIC_API_KEY environment variable
llm_fn = create_anthropic_llm_fn(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1000
)
```

### Tools

Define tools using the `@tool` decorator:

```python
from praisonaiagents.lite import LiteAgent, tool

@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers together."""
    return a + b

@tool
def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"Weather in {city}: Sunny, 72Â°F"

agent = LiteAgent(
    name="ToolAgent",
    llm_fn=llm_fn,
    tools=[add_numbers, get_weather]
)

# Execute tools directly
result = agent.execute_tool("add_numbers", a=5, b=3)
print(result.output)  # 8
print(result.success)  # True
```

### LiteTask

For structured task execution:

```python
from praisonaiagents.lite import LiteAgent, LiteTask

agent = LiteAgent(name="Worker", llm_fn=llm_fn)

task = LiteTask(
    description="Summarize the following text",
    agent=agent,
    expected_output="A brief summary"
)

result = task.execute(context="Long text to summarize...")
print(result)
```

## Thread Safety

LiteAgent uses locks for thread-safe operations:

```python
import threading
from praisonaiagents.lite import LiteAgent

agent = LiteAgent(name="ThreadSafe", llm_fn=llm_fn)

def worker(prompt):
    response = agent.chat(prompt)
    print(f"Response: {response}")

# Safe to use from multiple threads
threads = [
    threading.Thread(target=worker, args=(f"Question {i}",))
    for i in range(5)
]

for t in threads:
    t.start()
for t in threads:
    t.join()
```

## Memory Efficiency

The lite package uses significantly less memory than the full package:

| Package | Memory Usage |
|---------|--------------|
| praisonaiagents (full) | ~93MB |
| praisonaiagents.lite | ~5MB |

## When to Use Lite

Use the lite package when:

- You want minimal dependencies
- You have your own LLM client
- Memory usage is critical
- You need fast startup time
- You're building a custom integration

Use the full package when:

- You need multi-provider support via litellm
- You want automatic model routing
- You need advanced features (memory, knowledge, etc.)

## Related

- [Lite Package CLI](/docs/cli/lite)
- [Lazy Imports](/docs/features/lazy-imports)
- [Performance Benchmarks](/docs/features/performance-benchmarks)
