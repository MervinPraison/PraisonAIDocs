---
title: "Knowledge Module"
description: "Documentation for the praisonaiagents.knowledge module - Advanced knowledge management and retrieval"
icon: "brain"
---

# Module praisonaiagents.knowledge

The knowledge module provides comprehensive knowledge management capabilities including document processing, vector storage, advanced chunking strategies, and semantic search functionality.

## Classes

### Knowledge
The main class for managing knowledge bases with vector storage and retrieval capabilities.

#### Parameters
- `config: Optional[Dict[str, Any]] = None` - Configuration for vector store and processing
- `verbose: int = 0` - Verbosity level (0-5+) for logging output

#### Properties
- `memory` - Returns a CustomMemory instance for knowledge storage
- `markdown` - Returns MarkItDown instance for document processing
- `chunker` - Returns a Chunking instance with configured strategy

#### Methods
- `store(content, user_id="user", agent_id=None, run_id=None, metadata=None)` - Store raw text or file content
- `add(file_path, user_id="user", agent_id=None, run_id=None, metadata=None)` - Process and add files to knowledge base
- `search(query, user_id=None, agent_id=None, run_id=None, rerank=True, **kwargs)` - Search knowledge with optional reranking
- `get(memory_id)` - Retrieve specific memory by ID
- `get_all(user_id=None, agent_id=None, run_id=None)` - Retrieve all memories with filtering
- `update(memory_id, data)` - Update existing memory
- `delete(memory_id)` - Delete specific memory
- `delete_all(user_id=None, agent_id=None, run_id=None)` - Batch delete with filtering
- `reset()` - Clear all memories
- `history(memory_id)` - Get change history for a memory

### CustomMemory
A specialized memory class that bypasses LLM usage for simple fact storage.

### Chunking
Unified interface for various text chunking strategies using the chonkie library.

#### Parameters
- `chunker_type: str = 'recursive'` - Type of chunking strategy
- `chunk_size: int = 512` - Maximum size of each chunk
- `chunk_overlap: int = 50` - Overlap between chunks
- `tokenizer: Optional[Any] = None` - Custom tokenizer (defaults to GPT-2)
- `embedding_model: Optional[Any] = None` - Embedding model for semantic chunking

#### Methods
- `chunk(text: str) â†’ List[Chunk]` - Split text into chunks using configured strategy

## Configuration

### Vector Store Configuration
```python
config = {
    "vector_store": {
        "provider": "chroma",  # Options: chroma, qdrant, pinecone
        "config": {
            "collection_name": "my_knowledge",
            "path": ".praison",  # For local storage
            # Additional provider-specific options
        }
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-small"
        }
    }
}
```

### Chunking Strategies
#### 1. Token Chunker (`'token'`)
Splits text by token count with overlapping windows.

```python
chunker = Chunking(
    chunker_type='token',
    chunk_size=512,
    chunk_overlap=50
)
```

#### 2. Sentence Chunker (`'sentence'`)
Splits text by sentences while respecting chunk size.

```python
chunker = Chunking(
    chunker_type='sentence',
    chunk_size=512,
    chunk_overlap=50
)
```

#### 3. Recursive Chunker (`'recursive'`) - Default
Hierarchical splitting with multiple separators.

```python
chunker = Chunking(
    chunker_type='recursive',
    chunk_size=512
)
```

#### 4. Semantic Chunker (`'semantic'`)
Groups semantically similar content together.

```python
chunker = Chunking(
    chunker_type='semantic',
    chunk_size=512,
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

#### 5. SDPM Chunker (`'sdpm'`)
Semantic Double-Pass Merge for optimal chunking.

```python
chunker = Chunking(
    chunker_type='sdpm',
    chunk_size=512,
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

#### 6. Late Chunker (`'late'`)
Optimized for retrieval performance with late interaction.

```python
chunker = Chunking(
    chunker_type='late',
    chunk_size=512,
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

## Usage Examples

### Basic Knowledge Management
```python
from praisonaiagents import Knowledge

# Initialize knowledge base
knowledge = Knowledge(verbose=2)

# Add documents
knowledge.add("documents/manual.pdf")
knowledge.add("data/notes.txt")

# Store raw text
knowledge.store("Important fact: The API key is XYZ123")

# Search knowledge
results = knowledge.search("API key", limit=5)
for result in results:
    print(f"Score: {result['score']}, Content: {result['memory']}")
```

### Agent Integration
```python
from praisonaiagents import Agent

agent = Agent(
    name="Research Assistant",
    role="Knowledge expert",
    goal="Answer questions using knowledge base",
    knowledge=["research.pdf", "notes.txt"],  # Files to process
    knowledge_config={
        "vector_store": {
            "provider": "chroma",
            "config": {
                "collection_name": "research_kb"
            }
        }
    }
)
```

### Advanced Configuration
```python
# Configure with custom embeddings and chunking
config = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name": "advanced_kb",
            "path": "./knowledge_store"
        }
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-large"
        }
    }
}

knowledge = Knowledge(config=config, verbose=3)

# Use semantic chunking for better retrieval
knowledge.chunker = Chunking(
    chunker_type='semantic',
    chunk_size=1024,
    embedding_model="BAAI/bge-small-en-v1.5"
)
```

### Scoped Knowledge Retrieval
```python
# User-specific knowledge
user_results = knowledge.search(
    "project requirements",
    user_id="user123"
)

# Agent-specific knowledge
agent_results = knowledge.search(
    "conversation history",
    agent_id="support_agent"
)

# Session-specific knowledge
session_results = knowledge.search(
    "current task",
    run_id="session_456"
)
```

## Supported File Types
* **Documents**: PDF, DOC, DOCX, PPT, PPTX, XLS, XLSX
* **Text**: TXT, MD, CSV, JSON, XML, HTML
* **Images**: JPG, PNG, GIF, BMP, SVG
* **Audio**: MP3, WAV, M4A (transcription support)
* **Archives**: ZIP (planned)

## Performance Optimization
1. **Batch Processing** - Add multiple files in one call for efficiency
2. **Chunk Size** - Larger chunks for narrative content, smaller for technical
3. **Reranking** - Disable for faster search when precision isn't critical
4. **Embedding Cache** - Reuse embeddings for duplicate content

## Best Practices
1. **Choose Appropriate Chunking** - Semantic for varied content, recursive for structured
2. **Set Meaningful Metadata** - Use metadata for filtering and organization
3. **Regular Cleanup** - Delete outdated knowledge to maintain relevance
4. **Monitor Storage** - Check vector store size for large knowledge bases
5. **Test Retrieval Quality** - Verify search results match expectations