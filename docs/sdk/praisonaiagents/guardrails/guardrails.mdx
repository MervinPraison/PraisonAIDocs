---
title: "Guardrails"
description: "Input/output validation and safety mechanisms for task outputs"
icon: "shield"
---

# Guardrails Module

The guardrails module provides validation and safety mechanisms for task outputs, including both function-based and LLM-based guardrails.

## Installation

```bash
pip install praisonaiagents
```

## Classes

### GuardrailResult

Result of a guardrail validation.

```python
from praisonaiagents import GuardrailResult
```

#### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `success` | `bool` | Whether the guardrail check passed |
| `result` | `str \| TaskOutput \| None` | The result if modified, or None if unchanged |
| `error` | `str` | Error message if validation failed (default: "") |

#### Class Methods

##### `from_tuple(result: Tuple[bool, Any]) -> GuardrailResult`

Create a GuardrailResult from a tuple returned by a guardrail function.

```python
# From a passing guardrail
result = GuardrailResult.from_tuple((True, task_output))

# From a failing guardrail
result = GuardrailResult.from_tuple((False, "Validation failed: content too short"))
```

---

### LLMGuardrail

An LLM-powered guardrail that validates task outputs using natural language descriptions.

```python
from praisonaiagents import LLMGuardrail
```

#### Constructor

```python
LLMGuardrail(description: str, llm: Any = None)
```

| Parameter | Type | Description |
|-----------|------|-------------|
| `description` | `str` | Natural language description of what to validate |
| `llm` | `Any` | The LLM instance to use for validation (string, LLM instance, or dict) |

#### Methods

##### `__call__(task_output: TaskOutput) -> Tuple[bool, Union[str, TaskOutput]]`

Validate the task output using the LLM.

**Returns:** Tuple of (success, result) where result is the output or error message.

## Usage Examples

### Function-based Guardrail

```python
from praisonaiagents import Agent, Task, PraisonAIAgents, GuardrailResult

def content_length_guardrail(task_output):
    """Validate that content is at least 100 characters."""
    if len(task_output.raw) < 100:
        return (False, "Content too short, must be at least 100 characters")
    return (True, task_output)

agent = Agent(
    name="Writer",
    role="Content Writer",
    goal="Write detailed content"
)

task = Task(
    description="Write about AI",
    expected_output="A detailed article",
    agent=agent,
    guardrail=content_length_guardrail
)

agents = PraisonAIAgents(agents=[agent], tasks=[task])
result = agents.start()
```

### LLM-based Guardrail

```python
from praisonaiagents import Agent, Task, PraisonAIAgents, LLMGuardrail

# Create an LLM-powered guardrail
quality_guardrail = LLMGuardrail(
    description="The content must be professional, factually accurate, and free of grammatical errors",
    llm="gpt-4o-mini"
)

agent = Agent(
    name="Writer",
    role="Content Writer",
    goal="Write high-quality content"
)

task = Task(
    description="Write a professional article about machine learning",
    expected_output="A well-written article",
    agent=agent,
    guardrail=quality_guardrail
)

agents = PraisonAIAgents(agents=[agent], tasks=[task])
result = agents.start()
```

### Multiple Guardrails

```python
from praisonaiagents import Agent, Task, PraisonAIAgents, LLMGuardrail

def length_check(task_output):
    if len(task_output.raw) < 200:
        return (False, "Content must be at least 200 characters")
    return (True, task_output)

def no_profanity(task_output):
    profanity_list = ["bad", "inappropriate"]  # simplified example
    for word in profanity_list:
        if word.lower() in task_output.raw.lower():
            return (False, f"Content contains inappropriate language: {word}")
    return (True, task_output)

# Chain guardrails
def combined_guardrail(task_output):
    # Check length first
    success, result = length_check(task_output)
    if not success:
        return (success, result)
    
    # Then check profanity
    return no_profanity(task_output)

task = Task(
    description="Write content",
    expected_output="Clean, detailed content",
    agent=agent,
    guardrail=combined_guardrail
)
```

## Best Practices

1. **Keep guardrails focused** - Each guardrail should check one specific aspect
2. **Provide clear error messages** - Help users understand why validation failed
3. **Use LLM guardrails for complex validation** - When rules are hard to express programmatically
4. **Chain guardrails for multiple checks** - Combine simple guardrails for comprehensive validation
5. **Handle errors gracefully** - Guardrails should not crash the application

## Related

- [Task](/docs/sdk/praisonaiagents/task/task) - Task configuration with guardrails
- [Agent](/docs/sdk/praisonaiagents/agent/agent) - Agent configuration
