---
title: "Cache"
description: "Response caching for LLM calls"
icon: "bolt"
---

# Cache

Cache provides response caching to reduce API costs and improve response times.

## Available Providers

| Provider | Description |
|----------|-------------|
| `MemoryCache` | In-memory cache |
| `FileCache` | File-based persistent cache |

## Quick Start

```typescript
import { createMemoryCache, createFileCache } from 'praisonai';

// Memory cache
const memCache = createMemoryCache({
  maxSize: 1000,
  ttl: 3600000  // 1 hour
});

// File cache
const fileCache = createFileCache({
  directory: './cache',
  ttl: 86400000  // 24 hours
});
```

## Configuration

```typescript
interface CacheConfig {
  maxSize?: number;
  ttl?: number;  // Time to live in ms
}
```

## Usage with Agents

```typescript
import { Agent, createMemoryCache } from 'praisonai';

const cache = createMemoryCache({ ttl: 3600000 });

const agent = new Agent({
  name: 'CachedAgent',
  instructions: 'You are helpful.',
  cache
});

// First call - hits API
const response1 = await agent.chat('Hello');

// Second call - returns cached response
const response2 = await agent.chat('Hello');
```

## CLI Usage

```bash
praisonai-ts cache info
praisonai-ts cache providers --json
```
