---
title: "LLM Provider â€¢ Rust AI Agent SDK"
sidebarTitle: "LLM Provider"
description: "LlmProvider: Trait for LLM providers"
icon: "brackets-curly"
---

# LlmProvider

> Defined in the [**LLM**](../modules/llm) module.

<Badge color="orange">Rust AI Agent SDK</Badge>

Trait for LLM providers

## Methods

### `chat`

```rust
async fn chat(
        &self,
        messages: &[Message],
        tools: Option<&[ToolDefinition]>,
    ) -> Result<LlmResponse>
```

Send a chat completion request

**Parameters:**

| Name | Type |
|------|------|
| `messages` | `&[Message]` |
| `tools` | `Option&lt;&[ToolDefinition]&gt;` |

### `chat_stream`

```rust
async fn chat_stream(
        &self,
        messages: &[Message],
        tools: Option<&[ToolDefinition]>,
    ) -> Result<Box<dyn futures::Stream<Item = Result<String>> + Send + Unpin>>
```

Stream a chat completion (returns chunks)

**Parameters:**

| Name | Type |
|------|------|
| `messages` | `&[Message]` |
| `tools` | `Option&lt;&[ToolDefinition]&gt;` |

### `model`

```rust
fn model(&self) -> &str
```

Get the model name


## Source

<Card title="View on GitHub" icon="github" href="https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-rust/praisonai/src/llm/mod.rs">
  `praisonai/src/llm/mod.rs` at line 0
</Card>



---

## Related Documentation

<CardGroup cols={2}>
  <Card title="Models Overview" icon="microchip" href="/docs/models" />
  <Card title="LLM Configuration" icon="gear" href="/docs/configuration/llm-config" />
  <Card title="Model Router" icon="route" href="/docs/features/model-router" />
  <Card title="Model Failover" icon="shield" href="/docs/features/model-failover" />
</CardGroup>
