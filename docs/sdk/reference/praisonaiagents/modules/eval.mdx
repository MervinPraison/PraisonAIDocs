---
title: "eval â€¢ AI Agent SDK"
sidebarTitle: "eval"
description: "PraisonAI Agents Evaluation Framework."
icon: "gauge"
---

# eval

<Badge color="blue">AI Agent</Badge>

PraisonAI Agents Evaluation Framework.

Provides comprehensive evaluation capabilities for AI agents with zero performance
impact when not in use through lazy loading.

Evaluator Types:
    - AccuracyEvaluator: Compare output against expected output using LLM-as-judge
    - PerformanceEvaluator: Measure runtime and memory usage
    - ReliabilityEvaluator: Verify expected tool calls are made
    - CriteriaEvaluator: Evaluate against custom criteria

Example:
    &gt;&gt;&gt; from praisonaiagents.eval import AccuracyEvaluator
    &gt;&gt;&gt; evaluator = AccuracyEvaluator(
    ...     agent=my_agent,
    ...     input_text="What is 2+2?",
    ...     expected_output="4"
    ... )
    &gt;&gt;&gt; result = evaluator.run(print_summary=True)

## Import

```python
from praisonaiagents import eval
```

### Constants

| Name | Value |
|------|-------|
| `_LAZY_IMPORTS` | `{'BaseEvaluator': ('base', 'BaseEvaluator'), 'AccuracyEvaluator': ('accuracy', 'AccuracyEvaluator'), 'PerformanceEvaluator': ('performance', 'PerformanceEvaluator'), 'ReliabilityEvaluator': ('reliabil...` |



---

## Related Documentation

<CardGroup cols={2}>
  <Card title="Evaluation Concept" icon="gavel" href="/docs/concepts/evaluation" />
  <Card title="LLM as Judge" icon="scale-balanced" href="/docs/features/llm-as-judge" />
  <Card title="Evaluation Loop" icon="rotate" href="/docs/eval/evaluation-loop" />
</CardGroup>
