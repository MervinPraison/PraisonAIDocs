---
title: "agent"
description: "API reference for agent"
icon: "bot"
---

# agent

<Badge color="blue">Core SDK</Badge>

## Import

```python
from praisonaiagents import agent
```

## Classes

### Agent

<Accordion title="Constructor Parameters">

| Parameter | Type | Required | Default |
|-----------|------|----------|---------|
| `name` | `Optional` | No | `None` |
| `role` | `Optional` | No | `None` |
| `goal` | `Optional` | No | `None` |
| `backstory` | `Optional` | No | `None` |
| `instructions` | `Optional` | No | `None` |
| `llm` | `Optional` | No | `None` |
| `model` | `Optional` | No | `None` |
| `function_calling_llm` | `Optional` | No | `None` |
| `llm_config` | `Optional` | No | `None` |
| `base_url` | `Optional` | No | `None` |
| `api_key` | `Optional` | No | `None` |
| `tools` | `Optional` | No | `None` |
| `allow_delegation` | `bool` | No | `False` |
| `allow_code_execution` | `Optional` | No | `False` |
| `code_execution_mode` | `Literal` | No | `'safe'` |
| `handoffs` | `Optional` | No | `None` |
| `auto_save` | `Optional` | No | `None` |
| `rate_limiter` | `Optional` | No | `None` |
| `memory` | `Optional` | No | `None` |
| `knowledge` | `Optional` | No | `None` |
| `planning` | `Optional` | No | `False` |
| `reflection` | `Optional` | No | `None` |
| `guardrails` | `Optional` | No | `None` |
| `web` | `Optional` | No | `None` |
| `context` | `Optional` | No | `None` |
| `autonomy` | `Optional` | No | `None` |
| `verification_hooks` | `Optional` | No | `None` |
| `output` | `Optional` | No | `None` |
| `execution` | `Optional` | No | `None` |
| `templates` | `Optional` | No | `None` |
| `caching` | `Optional` | No | `None` |
| `hooks` | `Optional` | No | `None` |
| `skills` | `Optional` | No | `None` |

</Accordion>

<Accordion title="Methods">

- **stream_emitter**(``) → `Any`
  Lazy-loaded StreamEventEmitter for real-time events (zero overhead when not used
- **stream_emitter**(`value: Any`) → `Any`
  Allow setting stream_emitter directly.
- **auto_memory**(``) → `Any`
  AutoMemory instance for automatic memory extraction.
- **auto_memory**(`value: Any`) → `Any`
- **policy**(``) → `Any`
  PolicyEngine instance for execution control.
- **policy**(`value: Any`) → `Any`
- **background**(``) → `Any`
  BackgroundRunner instance for async task execution.
- **background**(`value: Any`) → `Any`
- **checkpoints**(``) → `Any`
  CheckpointService instance for file-level undo/restore.
- **checkpoints**(`value: Any`) → `Any`
- **output_style**(``) → `Any`
  OutputStyle instance for response formatting.
- **output_style**(`value: Any`) → `Any`
- **thinking_budget**(``) → `Any`
  ThinkingBudget instance for extended thinking control.
- **thinking_budget**(`value: Any`) → `Any`
- **context_manager**(``) → `Any`
  ContextManager instance for unified context management.
- **context_manager**(`value: Any`) → `Any`
  Set context manager directly.
- **console**(``) → `Any`
  Lazily initialize Rich Console only when needed AND verbose is True.
- **skill_manager**(``) → `Any`
  Lazily initialize SkillManager only when skills are accessed.
- **get_skills_prompt**(``) → `str`
  Get the XML prompt for available skills.
- **agent_id**(``) → `Any`
  Lazily generate agent ID when first accessed.
- **display_name**(``) → `str`
  Safe display name that never returns None.
- **analyze_prompt**(`prompt: str`) → `set`
  Analyze prompt for autonomy signals.
- **get_recommended_stage**(`prompt: str`) → `str`
  Get recommended execution stage for prompt.
- **run_autonomous**(`prompt: str, max_iterations: Optional`) → `Any`
  Run an autonomous task execution loop.
- **handoff_to**(`target_agent: Agent, prompt: str, context: Optional`) → `HandoffResult`
  Programmatically hand off a task to another agent.
- **async handoff_to_async**(`target_agent: Agent, prompt: str, context: Optional`) → `HandoffResult`
  Asynchronously hand off a task to another agent.
- **get_available_tools**(``) → `List`
  Get tools available to this agent, filtered by plan_mode if enabled.
- **rules_manager**(``) → `Any`
  Lazy-initialized RulesManager for persistent rules/instructions.
- **get_rules_context**(`file_path: Optional`) → `str`
  Get rules context for the current conversation.
- **get_memory_context**(`query: Optional`) → `str`
  Get memory context for the current conversation.
- **store_memory**(`content: str, memory_type: str`) → `Any`
  Store content in memory.
- **llm_model**(``) → `Any`
  Unified property to get the LLM model regardless of configuration type.
- **retrieval_config**(``) → `Any`
  Get the unified retrieval configuration.
- **rag**(``) → `Any`
  Lazy-loaded RAG instance for advanced retrieval with citations.
- **retrieve**(`query: str`) → `ContextPack`
  Retrieve context from knowledge without LLM generation.
- **query**(`question: str`) → `RAGResult`
  Query knowledge and get a structured answer with citations.
- **rag_query**(`question: str`) → `RAGResult`
  Query knowledge using RAG pipeline with citations.
- **chat_with_context**(`message: str, context: ContextPack`) → `str`
  Chat with pre-retrieved context.
- **generate_task**(``) → `Task`
  Generate a Task object from the agent's instructions
- **execute_tool**(`function_name: Any, arguments: Any`) → `Any`
  Execute a tool dynamically based on the function name and arguments.
- **clear_history**(``) → `Any`
- **prune_history**(`keep_last: int`) → `int`
  Prune chat history to keep only the last N messages.
- **delete_history**(`index: int`) → `bool`
  Delete a specific message from chat history by index.
- **delete_history_matching**(`pattern: str`) → `int`
  Delete all messages matching a pattern.
- **get_history_size**(``) → `int`
  Get the current number of messages in chat history.
- **ephemeral**(``) → `Any`
  Context manager for ephemeral conversations.
- **session_id**(``) → `Optional`
  Get the current session ID.
- **chat**(`prompt: Any, temperature: Any, tools: Any, output_json: Any, output_pydantic: Any, reasoning_steps: Any, stream: Any, task_name: Any, task_description: Any, task_id: Any, config: Any, force_retrieval: Any, skip_retrieval: Any, attachments: Any`) → `Any`
  Chat with the agent.
- **clean_json_output**(`output: str`) → `str`
  Clean and extract JSON from response text.
- **async achat**(`prompt: str, temperature: Any, tools: Any, output_json: Any, output_pydantic: Any, reasoning_steps: Any, task_name: Any, task_description: Any, task_id: Any, attachments: Any`) → `Any`
  Async version of chat method with self-reflection support.
- **async arun**(`prompt: str`) → `Any`
  Async version of run() - silent, non-streaming, returns structured result.
- **async astart**(`prompt: str`) → `Any`
  Async version of start() - interactive, streaming-aware.
- **run**(`prompt: str`) → `Any`
  Execute agent silently and return structured result.
- **switch_model**(`new_model: str`) → `None`
  Switch the agent's LLM model while preserving conversation history.
- **start**(`prompt: str`) → `Any`
  Start the agent interactively with verbose output.
- **iter_stream**(`prompt: str`) → `Any`
  Stream agent response as an iterator of chunks.
- **execute**(`task: Any, context: Any`) → `Any`
  Execute a task synchronously - backward compatibility method
- **async aexecute**(`task: Any, context: Any`) → `Any`
  Execute a task asynchronously - backward compatibility method
- **async execute_tool_async**(`function_name: str, arguments: Dict`) → `Any`
  Async version of execute_tool
- **launch**(`path: str, port: int, host: str, debug: bool, protocol: str`) → `Any`
  Launch the agent as an HTTP API endpoint or an MCP server.

</Accordion>

## Constants

| Name | Value |
|------|-------|
| `DEFAULT_TOOL_OUTPUT_LIMIT` | `16000` |
