---
title: "agent"
description: "API reference for agent"
icon: "bot"
---

# agent

<Badge color="blue">Core SDK</Badge>

## Overview

```mermaid
graph TD
    Agent["Agent"]:::agent
    classDef agent fill:#8B0000,stroke:#8B0000,color:#fff
```

## Import

```python
from praisonaiagents import agent
```

## Classes

<AccordionGroup>
### Agent

<Expandable title="Constructor Parameters">

<ParamField query="name" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="role" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="goal" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="backstory" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="instructions" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="llm" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="model" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="function_calling_llm" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="llm_config" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="base_url" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="api_key" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="tools" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="allow_delegation" type="bool">
   (default: `False`)
</ParamField>
<ParamField query="allow_code_execution" type="Optional">
   (default: `False`)
</ParamField>
<ParamField query="code_execution_mode" type="Literal">
   (default: `'safe'`)
</ParamField>
<ParamField query="handoffs" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="auto_save" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="rate_limiter" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="memory" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="knowledge" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="planning" type="Optional">
   (default: `False`)
</ParamField>
<ParamField query="reflection" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="guardrails" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="web" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="context" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="autonomy" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="verification_hooks" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="output" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="execution" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="templates" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="caching" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="hooks" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="skills" type="Optional">
   (default: `None`)
</ParamField>

</Expandable>

<AccordionGroup>
<Accordion title="stream_emitter() -> Any">
  Lazy-loaded StreamEventEmitter for real-time events (zero overhead when not used).
</Accordion>
<Accordion title="stream_emitter(value: Any) -> Any">
  Allow setting stream_emitter directly.
</Accordion>
<Accordion title="auto_memory() -> Any">
  AutoMemory instance for automatic memory extraction.
</Accordion>
<Accordion title="auto_memory(value: Any) -> Any">
</Accordion>
<Accordion title="policy() -> Any">
  PolicyEngine instance for execution control.
</Accordion>
<Accordion title="policy(value: Any) -> Any">
</Accordion>
<Accordion title="background() -> Any">
  BackgroundRunner instance for async task execution.
</Accordion>
<Accordion title="background(value: Any) -> Any">
</Accordion>
<Accordion title="checkpoints() -> Any">
  CheckpointService instance for file-level undo/restore.
</Accordion>
<Accordion title="checkpoints(value: Any) -> Any">
</Accordion>
<Accordion title="output_style() -> Any">
  OutputStyle instance for response formatting.
</Accordion>
<Accordion title="output_style(value: Any) -> Any">
</Accordion>
<Accordion title="thinking_budget() -> Any">
  ThinkingBudget instance for extended thinking control.
</Accordion>
<Accordion title="thinking_budget(value: Any) -> Any">
</Accordion>
<Accordion title="context_manager() -> Any">
  ContextManager instance for unified context management.

Lazy initialized on first access when context=True or context=ManagerConfig.
Returns None when context=False (zero overhead).

Example:
    agent = Agent(instructions="...", context=True)
    # Access manager for advanced operations
    if agent.context_manager:
        stats = agent.context_manager.get_stats()
</Accordion>
<Accordion title="context_manager(value: Any) -> Any">
  Set context manager directly.
</Accordion>
<Accordion title="console() -> Any">
  Lazily initialize Rich Console only when needed AND verbose is True.
</Accordion>
<Accordion title="skill_manager() -> Any">
  Lazily initialize SkillManager only when skills are accessed.
</Accordion>
<Accordion title="get_skills_prompt() -> str">
  Get the XML prompt for available skills.

Returns:
    XML string with &lt;available_skills&gt; block, or empty string if no skills
</Accordion>
<Accordion title="agent_id() -> Any">
  Lazily generate agent ID when first accessed.
</Accordion>
<Accordion title="display_name() -> str">
  Safe display name that never returns None.

Returns the agent's name if set, otherwise returns 'Agent N' where N is a unique index.
Use this for UI display, logging, and string operations where None would cause errors.
</Accordion>
<Accordion title="analyze_prompt(prompt: str) -> set">
  Analyze prompt for autonomy signals.

Args:
    prompt: The user prompt
    
Returns:
    Set of detected signal names
</Accordion>
<Accordion title="get_recommended_stage(prompt: str) -> str">
  Get recommended execution stage for prompt.

Args:
    prompt: The user prompt
    
Returns:
    Stage name as string (direct, heuristic, planned, autonomous)
</Accordion>
<Accordion title="run_autonomous(prompt: str, max_iterations: Optional) -> Any">
  Run an autonomous task execution loop.

This method executes a task autonomously, using the agent's tools
and capabilities to complete the task. It handles:
- Progressive escalation based on task complexity
- Doom loop detection and recovery
- Iteration limits and timeouts
- Completion detection

Args:
    prompt: The task to execute
    max_iterations: Override max iterations (default from config)
    timeout_seconds: Timeout in seconds (default: no timeout)
    
Returns:
    AutonomyResult with success status, output, and metadata
    
Raises:
    ValueError: If autonomy is not enabled
    
Example:
    agent = Agent(instructions="...", autonomy=True)
    result = agent.run_autonomous("Refactor the auth module")
    if result.success:
        print(result.output)
</Accordion>
<Accordion title="handoff_to(target_agent: Agent, prompt: str, context: Optional) -> HandoffResult">
  Programmatically hand off a task to another agent.

This is the unified programmatic handoff API that replaces delegate().
It uses the same Handoff mechanism as LLM-driven handoffs but can be
called directly from code.

Args:
    target_agent: The agent to hand off to
    prompt: The task/prompt to pass to target agent
    context: Optional additional context dictionary
    config: Optional HandoffConfig for advanced settings
    
Returns:
    HandoffResult with response or error
    
Example:
    ```python
    result = agent_a.handoff_to(agent_b, "Complete this analysis")
    if result.success:
        print(result.response)
    ```
</Accordion>
<Accordion title="async handoff_to_async(target_agent: Agent, prompt: str, context: Optional) -> HandoffResult">
  Asynchronously hand off a task to another agent.

This is the async version of handoff_to() with concurrency control
and timeout support.

Args:
    target_agent: The agent to hand off to
    prompt: The task/prompt to pass to target agent
    context: Optional additional context dictionary
    config: Optional HandoffConfig for advanced settings
    
Returns:
    HandoffResult with response or error
    
Example:
    ```python
    result = await agent_a.handoff_to_async(agent_b, "Complete this analysis")
    if result.success:
        print(result.response)
    ```
</Accordion>
<Accordion title="get_available_tools() -> List">
  Get tools available to this agent, filtered by plan_mode if enabled.

In plan_mode, only read-only tools are available to prevent
modifications during the planning phase.

Returns:
    List of available tools
</Accordion>
<Accordion title="rules_manager() -> Any">
  Lazy-initialized RulesManager for persistent rules/instructions.

This property initializes the RulesManager only when first accessed,
avoiding expensive filesystem operations during agent instantiation.

Returns:
    RulesManager instance or None if not available
</Accordion>
<Accordion title="get_rules_context(file_path: Optional) -> str">
  Get rules context for the current conversation.

Args:
    file_path: Optional file path for glob-based rule matching
    include_manual: Optional list of manual rule names to include (via @mention)
    
Returns:
    Formatted rules context string
</Accordion>
<Accordion title="get_memory_context(query: Optional) -> str">
  Get memory context for the current conversation.

Args:
    query: Optional query to focus the context
    
Returns:
    Formatted memory context string
</Accordion>
<Accordion title="store_memory(content: str, memory_type: str) -> Any">
  Store content in memory.

Args:
    content: Content to store
    memory_type: Type of memory (short_term, long_term, entity, episodic)
    **kwargs: Additional arguments for the memory method
</Accordion>
<Accordion title="llm_model() -> Any">
  Unified property to get the LLM model regardless of configuration type.

Returns:
    The LLM model/instance being used by this agent.
    - For standard models: returns the model string (e.g., "gpt-4o-mini")
    - For custom LLM instances: returns the LLM instance object
    - For provider models: returns the LLM instance object
</Accordion>
<Accordion title="retrieval_config() -> Any">
  Get the unified retrieval configuration.
</Accordion>
<Accordion title="rag() -> Any">
  Lazy-loaded RAG instance for advanced retrieval with citations.

Returns RAG instance configured with agent's knowledge and retrieval_config.
Returns None if no knowledge is configured.

Usage:
    agent = Agent(knowledge=["doc.pdf"], retrieval_config=&#123;"citations": True&#125;)
    result = agent.rag.query("What is the main finding?")
    print(result.answer)
    for citation in result.citations:
        print(f"[&#123;citation.id&#125;] &#123;citation.source&#125;")
</Accordion>
<Accordion title="retrieve(query: str) -> ContextPack">
  Retrieve context from knowledge without LLM generation.

Returns a ContextPack that can be passed to chat_with_context().
This enables conditional retrieval - only retrieve when needed.

Args:
    query: Search query
    **kwargs: Additional arguments (top_k, rerank, etc.)
    
Returns:
    ContextPack with context string and citations (no LLM call)
    
Raises:
    ValueError: If no knowledge is configured
    ImportError: If RAG module is not available
    
Usage:
    agent = Agent(knowledge=["docs/"], retrieval_config=&#123;"citations": True&#125;)
    context = agent.retrieve("What are the key findings?")
    print(f"Found &#123;len(context.citations)&#125; sources")
    response = agent.chat_with_context("Summarize", context)
</Accordion>
<Accordion title="query(question: str) -> RAGResult">
  Query knowledge and get a structured answer with citations.

This is the recommended method for getting answers with source citations.
Returns a structured result with answer, citations, context used, and metadata.

Args:
    question: The question to answer
    **kwargs: Additional arguments (top_k, rerank, etc.)
    
Returns:
    RAGResult with answer, citations, context_used, and metadata
    
Raises:
    ValueError: If no knowledge is configured
    ImportError: If RAG module is not available
    
Usage:
    agent = Agent(knowledge=["doc.pdf"], retrieval_config=&#123;"citations": True&#125;)
    result = agent.query("What is the main finding?")
    print(result.answer)
    for citation in result.citations:
        print(f"  [&#123;citation.id&#125;] &#123;citation.source&#125;")
</Accordion>
<Accordion title="rag_query(question: str) -> RAGResult">
  Query knowledge using RAG pipeline with citations.

This is the recommended way to get answers with citations from an agent's knowledge.

Args:
    question: The question to answer
    **kwargs: Additional arguments passed to RAG.query()
    
Returns:
    RAGResult with answer, citations, context_used, and metadata
    
Raises:
    ValueError: If no knowledge sources are configured
    ImportError: If RAG module is not available
    
Usage:
    agent = Agent(knowledge=["doc.pdf"], rag_config=&#123;"include_citations": True&#125;)
    result = agent.rag_query("What is the main finding?")
    print(result.answer)
    for citation in result.citations:
        print(f"[&#123;citation.id&#125;] &#123;citation.source&#125;: &#123;citation.text[:100]&#125;")
</Accordion>
<Accordion title="chat_with_context(message: str, context: ContextPack) -> str">
  Chat with pre-retrieved context.

This method allows AutoRagAgent or manual workflows to inject 
pre-retrieved context into the agent's chat, enabling conditional 
retrieval without duplicating RAG logic.

Args:
    message: User message/question
    context: ContextPack from RAG.retrieve()
    citations_mode: How to include citations (append/hidden/none)
    **kwargs: Additional arguments passed to chat()
    
Returns:
    Agent response with optional citations
    
Usage:
    from praisonaiagents import AutoRagAgent
    
    auto_rag = AutoRagAgent(agent=my_agent)
    result = auto_rag.chat("What are the key findings?")
    
    # Or manually:
    context_pack = rag.retrieve("What are the key findings?")
    response = agent.chat_with_context("What are the key findings?", context_pack)
</Accordion>
<Accordion title="generate_task() -> Task">
  Generate a Task object from the agent's instructions
</Accordion>
<Accordion title="execute_tool(function_name: Any, arguments: Any) -> Any">
  Execute a tool dynamically based on the function name and arguments.
Injects agent state for tools with Injected[T] parameters.
</Accordion>
<Accordion title="clear_history() -> Any">
</Accordion>
<Accordion title="prune_history(keep_last: int) -> int">
  Prune chat history to keep only the last N messages.

Useful for cleaning up large history after image analysis sessions
to prevent context window saturation.

Args:
    keep_last: Number of recent messages to keep
    
Returns:
    Number of messages deleted
</Accordion>
<Accordion title="delete_history(index: int) -> bool">
  Delete a specific message from chat history by index.

Supports negative indexing (-1 for last message, etc.).

Args:
    index: Message index (0-based, supports negative indexing)
    
Returns:
    True if deleted, False if index out of range
</Accordion>
<Accordion title="delete_history_matching(pattern: str) -> int">
  Delete all messages matching a pattern.

Useful for removing all image-related messages after processing.

Args:
    pattern: Substring to match in message content
    
Returns:
    Number of messages deleted
</Accordion>
<Accordion title="get_history_size() -> int">
  Get the current number of messages in chat history.
</Accordion>
<Accordion title="ephemeral() -> Any">
  Context manager for ephemeral conversations.

Messages within this block are NOT permanently stored in chat_history.
History is restored to pre-block state after exiting.

Example:
    with agent.ephemeral():
        response = agent.chat("[IMAGE] Analyze this")
        # After block, history is restored - image NOT persisted
</Accordion>
<Accordion title="session_id() -> Optional">
  Get the current session ID.
</Accordion>
<Accordion title="chat(prompt: Any, temperature: Any, tools: Any, output_json: Any, output_pydantic: Any, reasoning_steps: Any, stream: Any, task_name: Any, task_description: Any, task_id: Any, config: Any, force_retrieval: Any, skip_retrieval: Any, attachments: Any) -> Any">
  Chat with the agent.

Args:
    prompt: Text query that WILL be stored in chat_history
    attachments: Optional list of image/file paths that are ephemeral
                (used for THIS turn only, NEVER stored in history).
                Supports: file paths, URLs, or data URIs.
    ...other args...
</Accordion>
<Accordion title="clean_json_output(output: str) -> str">
  Clean and extract JSON from response text.
</Accordion>
<Accordion title="async achat(prompt: str, temperature: Any, tools: Any, output_json: Any, output_pydantic: Any, reasoning_steps: Any, task_name: Any, task_description: Any, task_id: Any, attachments: Any) -> Any">
  Async version of chat method with self-reflection support.

Args:
    prompt: Text query that WILL be stored in chat_history
    attachments: Optional list of image/file paths that are ephemeral
                (used for THIS turn only, NEVER stored in history).
</Accordion>
<Accordion title="async arun(prompt: str) -> Any">
  Async version of run() - silent, non-streaming, returns structured result.

Production-friendly async execution. Does not stream or display output.

Args:
    prompt: The input prompt to process
    **kwargs: Additional arguments passed to achat()
    
Returns:
    The agent's response as a string
</Accordion>
<Accordion title="async astart(prompt: str) -> Any">
  Async version of start() - interactive, streaming-aware.

Beginner-friendly async execution. Streams by default when in TTY.

Args:
    prompt: The input prompt to process
    **kwargs: Additional arguments passed to achat()
    
Returns:
    The agent's response as a string
</Accordion>
<Accordion title="run(prompt: str) -> Any">
  Execute agent silently and return structured result.

Production-friendly execution. Always uses silent mode with no streaming
or verbose display, regardless of TTY status. Use this for programmatic,
scripted, or automated usage where you want just the result.

Args:
    prompt: The input prompt to process
    **kwargs: Additional arguments:
        - stream (bool): Force streaming if True. Default: False
        - output (str): Output preset override (rarely needed)
        
Returns:
    The agent's response as a string
    
Example:
    ```python
    agent = Agent(instructions="You are helpful")
    result = agent.run("What is 2+2?")  # Silent, returns "4"
    print(result)
    ```
    
Note:
    Unlike .start() which enables verbose output in TTY for interactive
    use, .run() is always silent. This makes it suitable for:
    - Production pipelines
    - Automated scripts
    - Background processing
    - API endpoints
</Accordion>
<Accordion title="switch_model(new_model: str) -> None">
  Switch the agent's LLM model while preserving conversation history.

Args:
    new_model: The new model name to switch to (e.g., "gpt-4o", "claude-3-sonnet")
</Accordion>
<Accordion title="start(prompt: str) -> Any">
  Start the agent interactively with verbose output.

Beginner-friendly execution. Defaults to verbose output with streaming
when running in a TTY. Use this for interactive/terminal usage where 
you want to see output in real-time with rich formatting.

Args:
    prompt: The input prompt to process. If not provided, uses the 
            agent's instructions as the task (useful when instructions
            already describe what the agent should do).
    **kwargs: Additional arguments:
        - stream (bool | None): Override streaming. None = auto-detect TTY
        - output (str): Output preset override (e.g., "silent", "verbose")
        
Returns:
    - If streaming: Generator yielding response chunks
    - If not streaming: The complete response as a string
    
Example:
    ```python
    # Minimal usage - instructions IS the task
    agent = Agent(instructions="Research AI trends and summarize")
    result = agent.start()  # Uses instructions as task
    
    # With explicit prompt (overrides/adds to instructions)
    agent = Agent(instructions="You are a helpful assistant")
    result = agent.start("What is 2+2?")  # Uses prompt as task
    ```
    
Note:
    Unlike .run() which is always silent (production use), .start()
    enables verbose output by default when in a TTY for beginner-friendly
    interactive use. Use .run() for programmatic/scripted usage.
</Accordion>
<Accordion title="iter_stream(prompt: str) -> Any">
  Stream agent response as an iterator of chunks.

App-friendly streaming. Yields response chunks without terminal display.
Use this for building custom UIs or processing streams programmatically.

Args:
    prompt: The input prompt to process
    **kwargs: Additional arguments:
        - display (bool): Show terminal output. Default: False
        - output (str): Output preset override
        
Yields:
    str: Response chunks as they are generated
    
Example:
    ```python
    agent = Agent(instructions="You are helpful")
    
    # Process stream programmatically
    full_response = ""
    for chunk in agent.iter_stream("Tell me a story"):
        full_response += chunk
        # Custom processing here
    
    # Or collect all at once
    response = "".join(agent.iter_stream("Hello"))
    ```
</Accordion>
<Accordion title="execute(task: Any, context: Any) -> Any">
  Execute a task synchronously - backward compatibility method
</Accordion>
<Accordion title="async aexecute(task: Any, context: Any) -> Any">
  Execute a task asynchronously - backward compatibility method
</Accordion>
<Accordion title="async execute_tool_async(function_name: str, arguments: Dict) -> Any">
  Async version of execute_tool
</Accordion>
<Accordion title="launch(path: str, port: int, host: str, debug: bool, protocol: str) -> Any">
  Launch the agent as an HTTP API endpoint or an MCP server.

Args:
    path: API endpoint path (default: '/') for HTTP, or base path for MCP.
    port: Server port (default: 8000)
    host: Server host (default: '0.0.0.0')
    debug: Enable debug mode for uvicorn (default: False)
    protocol: "http" to launch as FastAPI, "mcp" to launch as MCP server.
    
Returns:
    None
</Accordion>
</AccordionGroup>

</AccordionGroup>
