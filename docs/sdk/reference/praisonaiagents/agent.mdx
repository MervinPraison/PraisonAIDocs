---
title: "Agent Module"
description: "API reference for agent"
icon: "robot"
---

# agent



## Import

```python
from praisonaiagents import agent
```

## Classes

### Agent



#### Constructor Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `name` | `Optional[str]` |  |
| `role` | `Optional[str]` |  |
| `goal` | `Optional[str]` |  |
| `backstory` | `Optional[str]` |  |
| `instructions` | `Optional[str]` |  |
| `llm` | `Optional[Union[str, Any]]` |  |
| `model` | `Optional[Union[str, Any]]` |  |
| `function_calling_llm` | `Optional[Any]` |  |
| `llm_config` | `Optional[Dict[str, Any]]` |  |
| `base_url` | `Optional[str]` |  |
| `api_key` | `Optional[str]` |  |
| `tools` | `Optional[List[Any]]` |  |
| `allow_delegation` | `bool` |  |
| `allow_code_execution` | `Optional[bool]` |  |
| `code_execution_mode` | `Literal['safe', 'unsafe']` |  |
| `handoffs` | `Optional[List[Union['Agent', 'Handoff']]]` |  |
| `auto_save` | `Optional[str]` |  |
| `rate_limiter` | `Optional[Any]` |  |
| `memory` | `Optional[Any]` |  |
| `knowledge` | `Optional[Union[bool, List[str], Any]]` |  |
| `planning` | `Optional[Union[bool, Any]]` |  |
| `reflection` | `Optional[Union[bool, Any]]` |  |
| `guardrails` | `Optional[Union[bool, Callable, Any]]` |  |
| `web` | `Optional[Union[bool, Any]]` |  |
| `context` | `Optional[Union[bool, Any]]` |  |
| `autonomy` | `Optional[Union[bool, Dict[str, Any], Any]]` |  |
| `verification_hooks` | `Optional[List[Any]]` |  |
| `output` | `Optional[Union[str, Any]]` |  |
| `execution` | `Optional[Union[str, Any]]` |  |
| `templates` | `Optional[Any]` |  |
| `caching` | `Optional[Union[bool, Any]]` |  |
| `hooks` | `Optional[Union[List[Any], Any]]` |  |
| `skills` | `Optional[Union[List[str], Any]]` |  |

#### Methods

- **stream_emitter**(``) → `Any`
  Lazy-loaded StreamEventEmitter for real-time events (zero overhead when not used).
- **stream_emitter**(`value: Any`) → `Any`
  Allow setting stream_emitter directly.
- **from_template**(`uri: str, config: Optional[Dict[str, Any]], offline: bool`) → `Agent`
  Create an Agent from a template.

Args:
    uri: Template URI (local path, package ref, or github re
- **auto_memory**(``) → `Any`
  AutoMemory instance for automatic memory extraction.
- **auto_memory**(`value: Any`) → `Any`
- **policy**(``) → `Any`
  PolicyEngine instance for execution control.
- **policy**(`value: Any`) → `Any`
- **background**(``) → `Any`
  BackgroundRunner instance for async task execution.
- **background**(`value: Any`) → `Any`
- **checkpoints**(``) → `Any`
  CheckpointService instance for file-level undo/restore.
- **checkpoints**(`value: Any`) → `Any`
- **output_style**(``) → `Any`
  OutputStyle instance for response formatting.
- **output_style**(`value: Any`) → `Any`
- **thinking_budget**(``) → `Any`
  ThinkingBudget instance for extended thinking control.
- **thinking_budget**(`value: Any`) → `Any`
- **context_manager**(``) → `Any`
  ContextManager instance for unified context management.

Lazy initialized on first access when conte
- **context_manager**(`value: Any`) → `Any`
  Set context manager directly.
- **console**(``) → `Any`
  Lazily initialize Rich Console only when needed AND verbose is True.
- **skill_manager**(``) → `Any`
  Lazily initialize SkillManager only when skills are accessed.
- **get_skills_prompt**(``) → `str`
  Get the XML prompt for available skills.

Returns:
    XML string with `<available_skills>` block, or 
- **agent_id**(``) → `Any`
  Lazily generate agent ID when first accessed.
- **display_name**(``) → `str`
  Safe display name that never returns None.

Returns the agent's name if set, otherwise returns 'Agen
- **analyze_prompt**(`prompt: str`) → `set`
  Analyze prompt for autonomy signals.

Args:
    prompt: The user prompt
    
Returns:
    Set of det
- **get_recommended_stage**(`prompt: str`) → `str`
  Get recommended execution stage for prompt.

Args:
    prompt: The user prompt
    
Returns:
    Sta
- **run_autonomous**(`prompt: str, max_iterations: Optional[int], timeout_seconds: Optional[float]`) → `Any`
  Run an autonomous task execution loop.

This method executes a task autonomously, using the agent's 
- **handoff_to**(`target_agent: Agent, prompt: str, context: Optional[Dict[str, Any]], config: Optional['HandoffConfig']`) → `HandoffResult`
  Programmatically hand off a task to another agent.

This is the unified programmatic handoff API tha
- **get_available_tools**(``) → `List[Any]`
  Get tools available to this agent, filtered by plan_mode if enabled.

In plan_mode, only read-only t
- **rules_manager**(``) → `Any`
  Lazy-initialized RulesManager for persistent rules/instructions.

This property initializes the Rule
- **get_rules_context**(`file_path: Optional[str], include_manual: Optional[List[str]]`) → `str`
  Get rules context for the current conversation.

Args:
    file_path: Optional file path for glob-ba
- **get_memory_context**(`query: Optional[str]`) → `str`
  Get memory context for the current conversation.

Args:
    query: Optional query to focus the conte
- **store_memory**(`content: str, memory_type: str`) → `Any`
  Store content in memory.

Args:
    content: Content to store
    memory_type: Type of memory (short
- **llm_model**(``) → `Any`
  Unified property to get the LLM model regardless of configuration type.

Returns:
    The LLM model/
- **retrieval_config**(``) → `Any`
  Get the unified retrieval configuration.
- **rag**(``) → `Any`
  Lazy-loaded RAG instance for advanced retrieval with citations.

Returns RAG instance configured wit
- **retrieve**(`query: str`) → `ContextPack`
  Retrieve context from knowledge without LLM generation.

Returns a ContextPack that can be passed to
- **query**(`question: str`) → `RAGResult`
  Query knowledge and get a structured answer with citations.

This is the recommended method for gett
- **rag_query**(`question: str`) → `RAGResult`
  Query knowledge using RAG pipeline with citations.

This is the recommended way to get answers with 
- **chat_with_context**(`message: str, context: ContextPack`) → `str`
  Chat with pre-retrieved context.

This method allows AutoRagAgent or manual workflows to inject 
pre
- **generate_task**(``) → `Task`
  Generate a Task object from the agent's instructions
- **execute_tool**(`function_name: Any, arguments: Any`) → `Any`
  Execute a tool dynamically based on the function name and arguments.
Injects agent state for tools w
- **clear_history**(``) → `Any`
- **prune_history**(`keep_last: int`) → `int`
  Prune chat history to keep only the last N messages.

Useful for cleaning up large history after ima
- **delete_history**(`index: int`) → `bool`
  Delete a specific message from chat history by index.

Supports negative indexing (-1 for last messa
- **delete_history_matching**(`pattern: str`) → `int`
  Delete all messages matching a pattern.

Useful for removing all image-related messages after proces
- **get_history_size**(``) → `int`
  Get the current number of messages in chat history.
- **ephemeral**(``) → `Any`
  Context manager for ephemeral conversations.

Messages within this block are NOT permanently stored 
- **session_id**(``) → `Optional[str]`
  Get the current session ID.
- **chat**(`prompt: Any, temperature: Any, tools: Any, output_json: Any, output_pydantic: Any, reasoning_steps: Any, stream: Any, task_name: Any, task_description: Any, task_id: Any, config: Any, force_retrieval: Any, skip_retrieval: Any, attachments: Any`) → `Any`
  Chat with the agent.

Args:
    prompt: Text query that WILL be stored in chat_history
    attachmen
- **clean_json_output**(`output: str`) → `str`
  Clean and extract JSON from response text.
- **run**(`prompt: str`) → `Any`
  Execute agent silently and return structured result.

Production-friendly execution. Always uses sil
- **switch_model**(`new_model: str`) → `None`
  Switch the agent's LLM model while preserving conversation history.

Args:
    new_model: The new mo
- **start**(`prompt: str`) → `Any`
  Start the agent interactively with verbose output.

Beginner-friendly execution. Defaults to verbose
- **iter_stream**(`prompt: str`) → `Any`
  Stream agent response as an iterator of chunks.

App-friendly streaming. Yields response chunks with
- **execute**(`task: Any, context: Any`) → `Any`
  Execute a task synchronously - backward compatibility method
- **launch**(`path: str, port: int, host: str, debug: bool, protocol: str`) → `Any`
  Launch the agent as an HTTP API endpoint or an MCP server.

Args:
    path: API endpoint path (defau
