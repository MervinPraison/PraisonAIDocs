---
title: "eval"
description: "PraisonAI Agents Evaluation Framework."
icon: "flask-conical"
---

# eval

<Badge color="blue">Core SDK</Badge>

PraisonAI Agents Evaluation Framework.

Provides comprehensive evaluation capabilities for AI agents with zero performance
impact when not in use through lazy loading.

Evaluator Types:
    - AccuracyEvaluator: Compare output against expected output using LLM-as-judge
    - PerformanceEvaluator: Measure runtime and memory usage
    - ReliabilityEvaluator: Verify expected tool calls are made
    - CriteriaEvaluator: Evaluate against custom criteria

Example:
    &gt;&gt;&gt; from praisonaiagents.eval import AccuracyEvaluator
    &gt;&gt;&gt; evaluator = AccuracyEvaluator(
    ...     agent=my_agent,
    ...     input_text="What is 2+2?",
    ...     expected_output="4"
    ... )
    &gt;&gt;&gt; result = evaluator.run(print_summary=True)

## Import

```python
from praisonaiagents import eval
```

## Constants

| Name | Value |
|------|-------|
| `_LAZY_IMPORTS` | `\{BaseEvaluator: ('base', BaseEvaluator), 'Accu` |
