---
title: "deep_research_agent"
description: "Deep Research Agent Module"
icon: "microscope"
---

# deep_research_agent

<Badge color="blue">Core SDK</Badge>

## Overview

```mermaid
graph TD
    Agent["Agent"]:::agent
    classDef agent fill:#8B0000,stroke:#8B0000,color:#fff
```

Deep Research Agent Module

This module provides the DeepResearchAgent class for automating complex research 
workflows using Deep Research APIs from multiple providers:

- **OpenAI**: o3-deep-research, o4-mini-deep-research (via Responses API)
- **Gemini**: deep-research-pro (via Interactions API)

The agent automatically detects the provider based on the model name and uses
the appropriate API.

Example:
    from praisonaiagents import DeepResearchAgent
    
    # OpenAI Deep Research
    agent = DeepResearchAgent(
        name="Research Assistant",
        model="o3-deep-research",
        instructions="You are a professional researcher..."
    )
    
    # Gemini Deep Research
    agent = DeepResearchAgent(
        name="Research Assistant", 
        model="deep-research-pro",
        instructions="You are a professional researcher..."
    )
    
    result = agent.research("What are the economic impacts of AI on healthcare?")
    print(result.report)
    for citation in result.citations:
        print(f"- &#123;citation.title&#125;: &#123;citation.url&#125;")

## Import

```python
from praisonaiagents import deep_research_agent
```

## Classes

<AccordionGroup>
### Citation

Represents a citation in the research report.

<Expandable title="Properties">

<ResponseField name="title" type="str">
</ResponseField>
<ResponseField name="url" type="str">
</ResponseField>
<ResponseField name="start_index" type="int">
</ResponseField>
<ResponseField name="end_index" type="int">
</ResponseField>

</Expandable>

### ReasoningStep

Represents a reasoning step in the research process.

<Expandable title="Properties">

<ResponseField name="text" type="str">
</ResponseField>
<ResponseField name="type" type="str">
</ResponseField>

</Expandable>

### WebSearchCall

Represents a web search call made during research.

<Expandable title="Properties">

<ResponseField name="query" type="str">
</ResponseField>
<ResponseField name="status" type="str">
</ResponseField>

</Expandable>

### CodeExecutionStep

Represents a code execution step during research.

<Expandable title="Properties">

<ResponseField name="input_code" type="str">
</ResponseField>
<ResponseField name="output" type="Optional">
</ResponseField>

</Expandable>

### MCPCall

Represents an MCP tool call during research.

<Expandable title="Properties">

<ResponseField name="name" type="str">
</ResponseField>
<ResponseField name="server_label" type="str">
</ResponseField>
<ResponseField name="arguments" type="Dict">
</ResponseField>

</Expandable>

### FileSearchCall

Represents a file search call (Gemini-specific).

<Expandable title="Properties">

<ResponseField name="store_names" type="List">
</ResponseField>

</Expandable>

### DeepResearchResponse

Complete response from a Deep Research query.

Attributes:
    report: The final research report text
    citations: List of citations with source metadata
    reasoning_steps: List of reasoning steps taken
    web_searches: List of web search queries executed
    code_executions: List of code execution steps
    mcp_calls: List of MCP tool calls
    file_searches: List of file search calls (Gemini)
    provider: The provider used (openai, gemini, litellm)
    interaction_id: Interaction ID (Gemini) or Response ID (OpenAI)
    raw_response: The raw API response object

<Expandable title="Properties">

<ResponseField name="report" type="str">
</ResponseField>
<ResponseField name="citations" type="List">
</ResponseField>
<ResponseField name="reasoning_steps" type="List">
</ResponseField>
<ResponseField name="web_searches" type="List">
</ResponseField>
<ResponseField name="code_executions" type="List">
</ResponseField>
<ResponseField name="mcp_calls" type="List">
</ResponseField>
<ResponseField name="file_searches" type="List">
</ResponseField>
<ResponseField name="provider" type="str">
</ResponseField>
<ResponseField name="interaction_id" type="Optional">
</ResponseField>
<ResponseField name="raw_response" type="Optional">
</ResponseField>

</Expandable>

<AccordionGroup>
<Accordion title="get_citation_text(citation: Citation) -> str">
  Extract the text that a citation refers to.
</Accordion>
<Accordion title="get_all_sources() -> List">
  Get a list of all unique sources cited.
</Accordion>
</AccordionGroup>

### Provider

*Extends: Enum*

Supported Deep Research providers.

### DeepResearchAgent

Agent for performing deep research using multiple provider APIs.

Supports:
- **OpenAI Deep Research**: o3-deep-research, o4-mini-deep-research
- **Gemini Deep Research**: deep-research-pro-preview
- **LiteLLM**: Unified interface for OpenAI models

The provider is auto-detected based on the model name, or can be explicitly set.

Example:
    # OpenAI (default)
    agent = DeepResearchAgent(
        model="o3-deep-research",
        instructions="You are a professional researcher."
    )
    
    # Gemini
    agent = DeepResearchAgent(
        model="deep-research-pro",
        instructions="You are a professional researcher."
    )
    
    # Using LiteLLM
    agent = DeepResearchAgent(
        model="o3-deep-research",
        use_litellm=True
    )
    
    result = agent.research("Research the economic impact of AI on healthcare.")
    print(result.report)

<Expandable title="Constructor Parameters">

<ParamField query="name" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="instructions" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="model" type="str">
   (default: `'o3-deep-research'`)
</ParamField>
<ParamField query="provider" type="Optional">
   (default: `'auto'`)
</ParamField>
<ParamField query="api_key" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="base_url" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="verbose" type="bool">
   (default: `True`)
</ParamField>
<ParamField query="summary_mode" type="Literal">
   (default: `'auto'`)
</ParamField>
<ParamField query="enable_web_search" type="bool">
   (default: `True`)
</ParamField>
<ParamField query="enable_code_interpreter" type="bool">
   (default: `False`)
</ParamField>
<ParamField query="enable_file_search" type="bool">
   (default: `False`)
</ParamField>
<ParamField query="mcp_servers" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="file_search_stores" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="use_litellm" type="bool">
   (default: `False`)
</ParamField>
<ParamField query="poll_interval" type="int">
   (default: `10`)
</ParamField>
<ParamField query="max_wait_time" type="int">
   (default: `3600`)
</ParamField>

</Expandable>

<AccordionGroup>
<Accordion title="openai_client() -> Any">
  Get the synchronous OpenAI client (lazy initialization).
</Accordion>
<Accordion title="async_openai_client() -> Any">
  Get the asynchronous OpenAI client (lazy initialization).
</Accordion>
<Accordion title="gemini_client() -> Any">
  Get the Gemini client (lazy initialization).
</Accordion>
<Accordion title="research(query: str, instructions: Optional) -> DeepResearchResponse">
  Perform a deep research query.

Args:
    query: The research question or topic to investigate
    instructions: Override the agent's default instructions
    model: Override the default model
    summary_mode: Summary mode ("auto", "detailed", "concise") - OpenAI only
    web_search: Enable web search - OpenAI only
    code_interpreter: Enable code interpreter - OpenAI only
    mcp_servers: MCP server configurations - OpenAI only
    file_ids: File IDs for code interpreter - OpenAI only
    file_search: Enable file search - Gemini only
    file_search_stores: File search store names - Gemini only
    stream: Enable streaming for real-time progress (default: True)
    
Returns:
    DeepResearchResponse containing the report, citations, and metadata
    
Example:
    # Standard research
    result = agent.research(
        "What are the latest developments in quantum computing?",
        summary_mode="detailed"
    )
    
    # Streaming research (Gemini) - shows real-time progress
    result = agent.research(
        "Research AI trends",
        stream=True  # Real-time thinking summaries
    )
    print(result.report)
</Accordion>
<Accordion title="async aresearch(query: str, instructions: Optional) -> DeepResearchResponse">
  Async version of research().

For Gemini, this still uses polling but with async sleep.
For OpenAI, uses the async client.
</Accordion>
<Accordion title="follow_up(query: str, previous_interaction_id: str, model: Optional) -> DeepResearchResponse">
  Ask a follow-up question based on a previous research interaction.

Only supported for Gemini provider.

Args:
    query: The follow-up question
    previous_interaction_id: ID from a previous research response
    model: Model to use (defaults to gemini-3-pro for follow-ups)
    
Returns:
    DeepResearchResponse with the follow-up answer
</Accordion>
<Accordion title="clarify(query: str, model: Optional) -> str">
  Generate clarifying questions for a research query.

Args:
    query: The initial research query
    model: Model to use for generating questions
    
Returns:
    String containing clarifying questions
</Accordion>
<Accordion title="rewrite_query(query: str, model: Optional) -> str">
  Rewrite a research query to be more specific and detailed.

Args:
    query: The initial research query
    model: Model to use for rewriting
    
Returns:
    Rewritten, more detailed query
</Accordion>
</AccordionGroup>

</AccordionGroup>
