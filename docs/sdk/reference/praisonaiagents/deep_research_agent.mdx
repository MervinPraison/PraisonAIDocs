---
title: "Deep_Research_Agent Module"
description: "Deep Research Agent Module

This module provides the DeepResearchAgent class for automating complex research 
workflows using Deep Research APIs from "
icon: "code"
---

# deep_research_agent

Deep Research Agent Module

This module provides the DeepResearchAgent class for automating complex research 
workflows using Deep Research APIs from multiple providers:

- **OpenAI**: o3-deep-research, o4-mini-deep-research (via Responses API)
- **Gemini**: deep-research-pro (via Interactions API)

The agent automatically detects the provider based on the model name and uses
the appropriate API.

Example:
    from praisonaiagents import DeepResearchAgent
    
    # OpenAI Deep Research
    agent = DeepResearchAgent(
        name="Research Assistant",
        model="o3-deep-research",
        instructions="You are a professional researcher..."
    )
    
    # Gemini Deep Research
    agent = DeepResearchAgent(
        name="Research Assistant", 
        model="deep-research-pro",
        instructions="You are a professional researcher..."
    )
    
    result = agent.research("What are the economic impacts of AI on healthcare?")
    print(result.report)
    for citation in result.citations:
        print(f"- {citation.title}: {citation.url}")

## Import

```python
from praisonaiagents import deep_research_agent
```

## Classes

### Citation

Represents a citation in the research report.

### ReasoningStep

Represents a reasoning step in the research process.

### WebSearchCall

Represents a web search call made during research.

### CodeExecutionStep

Represents a code execution step during research.

### MCPCall

Represents an MCP tool call during research.

### FileSearchCall

Represents a file search call (Gemini-specific).

### DeepResearchResponse

Complete response from a Deep Research query.

Attributes:
    report: The final research report text
    citations: List of citations with source metadata
    reasoning_steps: List of reasoning steps taken
    web_searches: List of web search queries executed
    code_executions: List of code execution steps
    mcp_calls: List of MCP tool calls
    file_searches: List of file search calls (Gemini)
    provider: The provider used (openai, gemini, litellm)
    interaction_id: Interaction ID (Gemini) or Response ID (OpenAI)
    raw_response: The raw API response object

#### Methods

- **get_citation_text**(`citation: Citation`) → `str`
  Extract the text that a citation refers to.
- **get_all_sources**(``) → `List[Dict[str, str]]`
  Get a list of all unique sources cited.

### Provider

Supported Deep Research providers.

### DeepResearchAgent

Agent for performing deep research using multiple provider APIs.

Supports:
- **OpenAI Deep Research**: o3-deep-research, o4-mini-deep-research
- **Gemini Deep Research**: deep-research-pro-preview
- **LiteLLM**: Unified interface for OpenAI models

The provider is auto-detected based on the model name, or can be explicitly set.

Example:
    # OpenAI (default)
    agent = DeepResearchAgent(
        model="o3-deep-research",
        instructions="You are a professional researcher."
    )
    
    # Gemini
    agent = DeepResearchAgent(
        model="deep-research-pro",
        instructions="You are a professional researcher."
    )
    
    # Using LiteLLM
    agent = DeepResearchAgent(
        model="o3-deep-research",
        use_litellm=True
    )
    
    result = agent.research("Research the economic impact of AI on healthcare.")
    print(result.report)

#### Constructor Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `name` | `Optional[str]` |  |
| `instructions` | `Optional[str]` |  |
| `model` | `str` |  |
| `provider` | `Optional[Literal['openai', 'gemini', 'litellm', 'auto']]` |  |
| `api_key` | `Optional[str]` |  |
| `base_url` | `Optional[str]` |  |
| `verbose` | `bool` |  |
| `summary_mode` | `Literal['auto', 'detailed', 'concise']` |  |
| `enable_web_search` | `bool` |  |
| `enable_code_interpreter` | `bool` |  |
| `enable_file_search` | `bool` |  |
| `mcp_servers` | `Optional[List[Dict[str, Any]]]` |  |
| `file_search_stores` | `Optional[List[str]]` |  |
| `use_litellm` | `bool` |  |
| `poll_interval` | `int` |  |
| `max_wait_time` | `int` |  |

#### Methods

- **openai_client**(``) → `Any`
  Get the synchronous OpenAI client (lazy initialization).
- **async_openai_client**(``) → `Any`
  Get the asynchronous OpenAI client (lazy initialization).
- **gemini_client**(``) → `Any`
  Get the Gemini client (lazy initialization).
- **research**(`query: str, instructions: Optional[str], model: Optional[str], summary_mode: Optional[Literal['auto', 'detailed', 'concise']], web_search: Optional[bool], code_interpreter: Optional[bool], mcp_servers: Optional[List[Dict[str, Any]]], file_ids: Optional[List[str]], file_search: Optional[bool], file_search_stores: Optional[List[str]], stream: bool`) → `DeepResearchResponse`
  Perform a deep research query.

Args:
    query: The research question or topic to investigate
    i
- **follow_up**(`query: str, previous_interaction_id: str, model: Optional[str]`) → `DeepResearchResponse`
  Ask a follow-up question based on a previous research interaction.

Only supported for Gemini provid
- **clarify**(`query: str, model: Optional[str]`) → `str`
  Generate clarifying questions for a research query.

Args:
    query: The initial research query
   
- **rewrite_query**(`query: str, model: Optional[str]`) → `str`
  Rewrite a research query to be more specific and detailed.

Args:
    query: The initial research qu
