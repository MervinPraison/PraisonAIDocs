---
title: "agents"
description: "API reference for agents"
icon: "users"
---

# agents

<Badge color="blue">Core SDK</Badge>

## Overview

```mermaid
graph TD
    Agent["Agent"]:::agent
    classDef agent fill:#8B0000,stroke:#8B0000,color:#fff
```

## Import

```python
from praisonaiagents import agents
```

## Classes

### TaskStatus

*Extends: Enum*

Enumeration for task status values to ensure consistency

### Agents

<Expandable title="Constructor Parameters">

<ParamField query="agents" type="Any">
   (Required)
</ParamField>
<ParamField query="tasks" type="Any">
   (default: `None`)
</ParamField>
<ParamField query="process" type="Any">
   (default: `'sequential'`)
</ParamField>
<ParamField query="manager_llm" type="Any">
   (default: `None`)
</ParamField>
<ParamField query="name" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="variables" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="memory" type="Optional">
   (default: `False`)
</ParamField>
<ParamField query="planning" type="Optional">
   (default: `False`)
</ParamField>
<ParamField query="context" type="Optional">
   (default: `False`)
</ParamField>
<ParamField query="output" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="execution" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="hooks" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="autonomy" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="knowledge" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="guardrails" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="web" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="reflection" type="Optional">
   (default: `None`)
</ParamField>
<ParamField query="caching" type="Optional">
   (default: `None`)
</ParamField>

</Expandable>

#### Methods

##### add_task(task: Any) -> Any
---
##### clean_json_output(output: str) -> str
---
##### context_manager() -> Any

ContextManager instance for unified context management across all agents.

Lazy initialized on first access when context=True or context=ManagerConfig.
Returns None when context=False (zero overhead).

For multi-agent scenarios, uses MultiAgentContextManager for per-agent isolation.

---
##### default_completion_checker(task: Any, agent_output: Any) -> Any
---
##### async aexecute_task(task_id: Any) -> Any

Async version of execute_task method

---
##### async arun_task(task_id: Any) -> Any

Async version of run_task method

---
##### async arun_all_tasks() -> Any

Async version of run_all_tasks method

---
##### async astart(content: Any, return_dict: Any) -> Any

Async version of start method.

Args:
    content: Optional content to add to all tasks' context
    return_dict: If True, returns the full results dictionary instead of only the final response
    **kwargs: Additional arguments

---
##### save_output_to_file(task: Any, task_output: Any) -> Any
---
##### execute_task(task_id: Any) -> Any

Synchronous version of execute_task method

---
##### run_task(task_id: Any) -> Any

Synchronous version of run_task method

---
##### run_all_tasks() -> Any

Synchronous version of run_all_tasks method

---
##### get_task_status(task_id: Any) -> Any
---
##### get_all_tasks_status() -> Any
---
##### get_task_result(task_id: Any) -> Any
---
##### get_task_details(task_id: Any) -> Any
---
##### get_agent_details(agent_name: Any) -> Any
---
##### start(content: Any, return_dict: Any, output: Any) -> Any

Start agent execution with verbose output (beginner-friendly).

Shows Rich panels with workflow progress when in TTY. Use .run() for
silent execution in production/scripts.

Args:
    content: Optional content to add to all tasks' context
    return_dict: If True, returns the full results dictionary
    output: Output preset - "silent", "verbose", "normal", etc.
            Default in TTY: "verbose" (shows progress)
            Default non-TTY: "silent"
    **kwargs: Additional arguments
    
Example:
    ```python
    # Interactive - shows Rich panels
    agents = Agents(agents=[agent1, agent2])
    result = agents.start()  # Verbose output by default
    
    # Force silent mode
    result = agents.start(output="silent")
    ```

---
##### run(content: Any, return_dict: Any) -> Any

Run agents silently (production use).

Unlike .start() which shows verbose output, .run() executes silently
for programmatic/production use.

Args:
    content: Optional content to add to all tasks' context
    return_dict: If True, returns the full results dictionary
    **kwargs: Additional arguments

---
##### set_state(key: str, value: Any) -> None

Set a state value

---
##### get_state(key: str, default: Any) -> Any

Get a state value

---
##### update_state(updates: Dict) -> None

Update multiple state values

---
##### clear_state() -> None

Clear all state values

---
##### has_state(key: str) -> bool

Check if a state key exists

---
##### get_all_state() -> Dict

Get a copy of the entire state dictionary

---
##### delete_state(key: str) -> bool

Delete a state key if it exists. Returns True if deleted, False if key didn't exist.

---
##### increment_state(key: str, amount: float, default: float) -> float

Increment a numeric state value. Creates the key with default if it doesn't exist.

---
##### append_to_state(key: str, value: Any, max_length: Optional) -> List

Append a value to a list state. Creates the list if it doesn't exist.

Args:
    key: State key
    value: Value to append
    max_length: Optional maximum length for the list
    
Returns:
    The updated list
    
Raises:
    TypeError: If the existing value is not a list and convert_to_list=False

---
##### save_session_state(session_id: str, include_memory: bool) -> None

Save current state to memory for session persistence

---
##### restore_session_state(session_id: str) -> bool

Restore state from memory for session persistence. Returns True if restored.

---
##### get_token_usage_summary() -> Dict

Get a summary of token usage across all agents and tasks.

---
##### get_detailed_token_report() -> Dict

Get a detailed token usage report.

---
##### display_token_usage() -> Any

Display token usage in a formatted table.

---
##### launch(path: str, port: int, host: str, debug: bool, protocol: str) -> Any

Launch all agents as a single API endpoint (HTTP) or an MCP server. 
In HTTP mode, the endpoint accepts a query and processes it through all agents in sequence.
In MCP mode, an MCP server is started, exposing a tool to run the agent workflow.

Args:
    path: API endpoint path (default: '/agents') for HTTP, or base path for MCP.
    port: Server port (default: 8000)
    host: Server host (default: '0.0.0.0')
    debug: Enable debug mode for uvicorn (default: False)
    protocol: "http" to launch as FastAPI, "mcp" to launch as MCP server.
    
Returns:
    None

---
##### current_plan() -> Any

Get the current plan.

---
##### todo_list() -> Any

Get the current todo list.

---
##### get_plan_markdown() -> str

Get the current plan as markdown.

Returns:
    Markdown string or empty string if no plan

---
##### get_todo_markdown() -> str

Get the current todo list as markdown.

Returns:
    Markdown string or empty string if no todo list

---
##### update_plan_step_status(step_id: str, status: str) -> bool

Update the status of a plan step.

Args:
    step_id: ID of the step to update
    status: New status
    
Returns:
    True if updated, False if not found

---


## Functions

### encode_file_to_base64()

Base64-encode a file.

```python
def encode_file_to_base64(file_path: str) -> str
```

<Expandable title="Parameters">

<ParamField query="file_path" type="str">
</ParamField>

</Expandable>

### process_video()

Split video into frames (base64-encoded).

```python
def process_video(video_path: str, seconds_per_frame: Any) -> Any
```

<Expandable title="Parameters">

<ParamField query="video_path" type="str">
</ParamField>
<ParamField query="seconds_per_frame" type="Any">
</ParamField>

</Expandable>

### get_multimodal_message()

Build multimodal message content for LLM with text and images.

DRY helper - replaces duplicate _get_multimodal_message in aexecute_task/execute_task.

Args:
    text_prompt: The text content of the message
    images: List of image paths (local or URL)
    
Returns:
    List of content items for multimodal LLM message

```python
def get_multimodal_message(text_prompt: str, images: list) -> list
```

<Expandable title="Parameters">

<ParamField query="text_prompt" type="str">
</ParamField>
<ParamField query="images" type="list">
</ParamField>

</Expandable>

### process_task_context()

Process a single context item for task execution.
This helper function avoids code duplication between async and sync execution methods.
Args:
    context_item: The context item to process (can be string, list, task object, or dict)
    verbose: Verbosity level for logging
    user_id: User ID for database queries
    
Returns:
    str: Formatted context string for this item

```python
def process_task_context(context_item: Any, verbose: Any, user_id: Any) -> Any
```

<Expandable title="Parameters">

<ParamField query="context_item" type="Any">
</ParamField>
<ParamField query="verbose" type="Any">
</ParamField>
<ParamField query="user_id" type="Any">
</ParamField>

</Expandable>

