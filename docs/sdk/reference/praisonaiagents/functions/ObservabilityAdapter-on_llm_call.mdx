---
title: "On LLM Call â€¢ AI Agent SDK"
sidebarTitle: "On LLM Call"
description: "on_llm_call: Called when an LLM call is made."
icon: "function"
---

# on_llm_call

<div className="flex items-center gap-2">
  <Badge color="purple">Method</Badge>
</div>

> This is a method of the [**ObservabilityAdapter**](../classes/ObservabilityAdapter) class in the [**obs**](../modules/obs) module.

Called when an LLM call is made.

## Signature

```python
def on_llm_call(span_id: str, model: str, messages: List[Dict[str, Any]], response: Optional[str], tokens: Optional[Dict[str, int]], latency_ms: Optional[float]) -> None
```

## Parameters

<ParamField query="span_id" type="str" required={true}>
  No description available.
</ParamField>

<ParamField query="model" type="str" required={true}>
  No description available.
</ParamField>

<ParamField query="messages" type="List" required={true}>
  No description available.
</ParamField>

<ParamField query="response" type="Optional" required={false}>
  No description available.
</ParamField>

<ParamField query="tokens" type="Optional" required={false}>
  No description available.
</ParamField>

<ParamField query="latency_ms" type="Optional" required={false}>
  No description available.
</ParamField>


## Source

<Card title="View on GitHub" icon="github" href="https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/obs/__init__.py#L100">
  `praisonaiagents/obs/__init__.py` at line 100
</Card>



---

## Related Documentation

<CardGroup cols={2}>
  <Card title="Models Overview" icon="microchip" href="/docs/models" />
  <Card title="LLM Configuration" icon="gear" href="/docs/configuration/llm-config" />
  <Card title="Model Router" icon="route" href="/docs/features/model-router" />
  <Card title="Model Failover" icon="shield" href="/docs/features/model-failover" />
</CardGroup>
