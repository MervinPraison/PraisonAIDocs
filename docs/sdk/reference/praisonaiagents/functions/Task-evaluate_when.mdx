---
title: "Evaluate When â€¢ AI Agent SDK"
sidebarTitle: "Evaluate When"
description: "evaluate_when: Evaluate the 'when' condition against the given context."
icon: "function"
---

# evaluate_when

<div className="flex items-center gap-2">
  <Badge color="purple">Method</Badge>
</div>

> This is a method of the [**Task**](../classes/Task) class in the [**task**](../modules/task) module.

Evaluate the 'when' condition against the given context.

Uses the shared evaluate_condition function from the conditions module
for DRY compliance with AgentFlow condition evaluation.

## Signature

```python
def evaluate_when(context: Dict[str, Any]) -> bool
```

## Parameters

<ParamField query="context" type="Dict" required={true}>
  Dictionary containing variables for evaluation. May include workflow variables, previous outputs, etc.
</ParamField>

### Returns

<ResponseField name="Returns" type="bool">
  True if condition is met or no condition is set.
    False if condition is not met.
</ResponseField>


## Uses

- `evaluate_condition`


## Used By

- [`Task.get_next_task`](../functions/Task-get_next_task)


## Source

<Card title="View on GitHub" icon="github" href="https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/task/task.py#L817">
  `praisonaiagents/task/task.py` at line 817
</Card>



---

## Related Documentation

<CardGroup cols={2}>
  <Card title="Evaluation Concept" icon="gavel" href="/docs/concepts/evaluation" />
  <Card title="LLM as Judge" icon="scale-balanced" href="/docs/features/llm-as-judge" />
  <Card title="Evaluation Loop" icon="rotate" href="/docs/eval/evaluation-loop" />
</CardGroup>
