---
title: MCP SSE Real-time Streaming Examples
description: Learn how to implement real-time streaming capabilities using MCP Server-Sent Events
---

# MCP SSE Real-time Streaming Examples

MCP (Model Context Protocol) with Server-Sent Events enables real-time streaming communication between agents and servers, perfect for building responsive AI applications with live updates, progress tracking, and event-driven architectures.

## Overview

Server-Sent Events (SSE) in PraisonAI provide:
- One-way real-time communication from server to client
- Automatic reconnection and session persistence
- Progress tracking for long-running operations
- Event-based architectures for multi-agent systems
- Streaming responses for improved user experience

## Basic SSE Setup

### Creating an SSE-Enabled MCP Server

```python
from mcp.server.fastmcp import FastMCP
from mcp.server.sse import SseServerTransport
from starlette.applications import Starlette
from starlette.routing import Route, Mount
from starlette.responses import StreamingResponse
import asyncio
import json

# Create MCP application
mcp = FastMCP("streaming-tools")

@mcp.tool()
async def process_large_dataset(dataset_name: str) -> str:
    """Process a large dataset with progress updates"""
    total_items = 1000
    
    for i in range(total_items):
        # Send progress update via SSE
        progress = (i + 1) / total_items * 100
        yield f"data: {json.dumps({'progress': progress, 'status': f'Processing item {i+1}/{total_items}'})}\n\n"
        
        # Simulate processing
        await asyncio.sleep(0.01)
    
    return f"Completed processing {dataset_name}"

@mcp.tool()
async def monitor_system_metrics() -> str:
    """Stream real-time system metrics"""
    while True:
        metrics = {
            "cpu": 45.2,
            "memory": 67.8,
            "disk": 82.1,
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(metrics)}\n\n"
        await asyncio.sleep(1)

# Create SSE transport
sse = SseServerTransport("/messages/")

# Starlette app with SSE endpoints
app = Starlette(
    routes=[
        Route("/sse", endpoint=lambda r: StreamingResponse(
            sse.handle_sse(r),
            media_type="text/event-stream"
        )),
        Mount("/messages/", app=sse.handle_post_message),
    ]
)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

### Connecting an Agent to SSE Server

```python
from praisonaiagents import Agent, MCP

# Create agent with SSE-enabled MCP tools
streaming_agent = Agent(
    name="Streaming Agent",
    instructions="You process data and provide real-time updates",
    tools=MCP("http://localhost:8080/sse")  # SSE endpoint
)

# Use the streaming tool
response = streaming_agent.start("Process the customer_data dataset")
# Agent will receive and display progress updates in real-time
```

## Advanced Streaming Patterns

### 1. Progress Tracking for Long-Running Operations

```python
from mcp.server.fastmcp import FastMCP
import asyncio
from typing import AsyncGenerator

mcp = FastMCP("progress-tracker")

class ProgressTracker:
    def __init__(self):
        self.active_jobs = {}
    
    async def emit_progress(self, job_id: str, progress: float, message: str):
        """Emit SSE progress event"""
        event = {
            "type": "progress",
            "job_id": job_id,
            "progress": progress,
            "message": message,
            "timestamp": datetime.now().isoformat()
        }
        return f"data: {json.dumps(event)}\n\n"

progress_tracker = ProgressTracker()

@mcp.tool()
async def analyze_documents(
    document_paths: list[str],
    analysis_type: str = "sentiment"
) -> AsyncGenerator[str, None]:
    """Analyze multiple documents with progress updates"""
    job_id = str(uuid.uuid4())
    total_docs = len(document_paths)
    
    progress_tracker.active_jobs[job_id] = {
        "status": "running",
        "total": total_docs,
        "completed": 0
    }
    
    results = []
    
    for idx, doc_path in enumerate(document_paths):
        # Emit progress
        progress = (idx / total_docs) * 100
        yield await progress_tracker.emit_progress(
            job_id, 
            progress,
            f"Analyzing document {idx + 1}/{total_docs}: {doc_path}"
        )
        
        # Simulate document analysis
        await asyncio.sleep(1)
        
        # Analyze document (simplified)
        result = {
            "document": doc_path,
            "sentiment": "positive",  # Simplified
            "confidence": 0.95
        }
        results.append(result)
        
        # Update job status
        progress_tracker.active_jobs[job_id]["completed"] = idx + 1
    
    # Final progress
    yield await progress_tracker.emit_progress(
        job_id, 
        100,
        f"Completed analysis of {total_docs} documents"
    )
    
    # Return final results
    return json.dumps({
        "job_id": job_id,
        "results": results,
        "summary": f"Analyzed {total_docs} documents successfully"
    })

# Client code with progress handling
from praisonaiagents import Agent, MCP

agent = Agent(
    name="Document Analyzer",
    instructions="You analyze documents and provide real-time progress updates",
    tools=MCP("http://localhost:8080/sse"),
    # Enable SSE event handling
    event_handler=lambda event: print(f"Progress: {event.get('progress', 0):.1f}% - {event.get('message', '')}")
)

response = agent.start("Analyze these documents: [doc1.pdf, doc2.pdf, doc3.pdf] for sentiment")
```

### 2. Real-time Data Streaming

```python
from mcp.server.fastmcp import FastMCP
import asyncio
import random

mcp = FastMCP("data-streamer")

@mcp.tool()
async def stream_market_data(symbols: list[str], duration_seconds: int = 60):
    """Stream real-time market data"""
    start_time = asyncio.get_event_loop().time()
    
    while asyncio.get_event_loop().time() - start_time < duration_seconds:
        for symbol in symbols:
            # Generate mock market data
            data = {
                "type": "market_update",
                "symbol": symbol,
                "price": round(100 + random.uniform(-5, 5), 2),
                "volume": random.randint(1000, 10000),
                "timestamp": datetime.now().isoformat(),
                "change": round(random.uniform(-2, 2), 2)
            }
            
            yield f"data: {json.dumps(data)}\n\n"
        
        await asyncio.sleep(0.5)  # Update every 500ms
    
    # Send completion event
    yield f"data: {json.dumps({'type': 'stream_complete', 'duration': duration_seconds})}\n\n"
    
    return f"Streamed market data for {len(symbols)} symbols over {duration_seconds} seconds"

@mcp.tool()
async def stream_log_analysis(log_file: str, pattern: str = None):
    """Stream log file analysis in real-time"""
    import aiofiles
    
    async with aiofiles.open(log_file, 'r') as f:
        line_count = 0
        matches = 0
        
        async for line in f:
            line_count += 1
            
            # Check pattern if provided
            if pattern and pattern in line:
                matches += 1
                event = {
                    "type": "log_match",
                    "line_number": line_count,
                    "content": line.strip(),
                    "pattern": pattern
                }
                yield f"data: {json.dumps(event)}\n\n"
            
            # Periodic status update
            if line_count % 1000 == 0:
                status = {
                    "type": "status_update",
                    "lines_processed": line_count,
                    "matches_found": matches
                }
                yield f"data: {json.dumps(status)}\n\n"
        
        # Final summary
        summary = {
            "type": "analysis_complete",
            "total_lines": line_count,
            "total_matches": matches,
            "match_rate": (matches / line_count * 100) if line_count > 0 else 0
        }
        yield f"data: {json.dumps(summary)}\n\n"
    
    return f"Completed analysis: {matches} matches in {line_count} lines"
```

### 3. Multi-Agent Coordination via SSE

```python
from mcp.server.fastmcp import FastMCP
from typing import Dict, List
import asyncio

mcp = FastMCP("coordinator")

class AgentCoordinator:
    def __init__(self):
        self.agents: Dict[str, dict] = {}
        self.tasks: Dict[str, dict] = {}
        self.event_subscribers: List[asyncio.Queue] = []
    
    async def broadcast_event(self, event: dict):
        """Broadcast event to all subscribers"""
        event_data = f"data: {json.dumps(event)}\n\n"
        for queue in self.event_subscribers:
            await queue.put(event_data)
    
    def subscribe(self) -> asyncio.Queue:
        """Subscribe to coordinator events"""
        queue = asyncio.Queue()
        self.event_subscribers.append(queue)
        return queue

coordinator = AgentCoordinator()

@mcp.tool()
async def register_agent(agent_id: str, capabilities: list[str]):
    """Register an agent with the coordinator"""
    coordinator.agents[agent_id] = {
        "id": agent_id,
        "capabilities": capabilities,
        "status": "idle",
        "registered_at": datetime.now().isoformat()
    }
    
    # Broadcast registration event
    await coordinator.broadcast_event({
        "type": "agent_registered",
        "agent_id": agent_id,
        "capabilities": capabilities
    })
    
    return f"Agent {agent_id} registered successfully"

@mcp.tool()
async def assign_task(task_id: str, task_type: str, requirements: dict):
    """Assign a task to available agents"""
    # Find capable agents
    capable_agents = [
        agent_id for agent_id, info in coordinator.agents.items()
        if task_type in info["capabilities"] and info["status"] == "idle"
    ]
    
    if not capable_agents:
        await coordinator.broadcast_event({
            "type": "task_queued",
            "task_id": task_id,
            "reason": "No available agents"
        })
        return "Task queued - no available agents"
    
    # Assign to first available agent
    assigned_agent = capable_agents[0]
    coordinator.agents[assigned_agent]["status"] = "busy"
    
    coordinator.tasks[task_id] = {
        "id": task_id,
        "type": task_type,
        "assigned_to": assigned_agent,
        "status": "in_progress",
        "started_at": datetime.now().isoformat()
    }
    
    # Broadcast assignment
    await coordinator.broadcast_event({
        "type": "task_assigned",
        "task_id": task_id,
        "agent_id": assigned_agent,
        "task_type": task_type
    })
    
    return f"Task {task_id} assigned to agent {assigned_agent}"

@mcp.tool()
async def stream_coordination_events():
    """Stream coordination events to clients"""
    queue = coordinator.subscribe()
    
    try:
        while True:
            event = await queue.get()
            yield event
    except asyncio.CancelledError:
        coordinator.event_subscribers.remove(queue)
        raise

# Multi-agent setup using coordination
from praisonaiagents import Agent, MCP

# Coordinator agent
coordinator_agent = Agent(
    name="Coordinator",
    instructions="You coordinate tasks between multiple agents",
    tools=MCP("http://localhost:8080/sse")
)

# Worker agents that register with coordinator
worker1 = Agent(
    name="Data Processor",
    instructions="You process data tasks",
    tools=MCP("http://localhost:8080/sse")
)

worker2 = Agent(
    name="Report Generator", 
    instructions="You generate reports",
    tools=MCP("http://localhost:8080/sse")
)

# Register workers
worker1.start("Register me with capabilities: [data_processing, analysis]")
worker2.start("Register me with capabilities: [report_generation, visualization]")

# Coordinator assigns tasks
coordinator_agent.start("Assign a data_processing task to available agents")
```

## Streaming UI Integration Examples

### 1. Streamlit with SSE Updates

```python
import streamlit as st
import requests
import sseclient
from praisonaiagents import Agent, MCP

st.title("Real-time AI Processing")

# Initialize session state
if "agent" not in st.session_state:
    st.session_state.agent = Agent(
        name="Streaming Assistant",
        tools=MCP("http://localhost:8080/sse")
    )

# Progress container
progress_container = st.empty()
status_text = st.empty()

# File upload
uploaded_files = st.file_uploader(
    "Upload files for processing",
    accept_multiple_files=True
)

if st.button("Process Files") and uploaded_files:
    # Create SSE connection for progress
    response = requests.get(
        "http://localhost:8080/sse/stream_coordination_events",
        stream=True
    )
    
    client = sseclient.SSEClient(response)
    
    # Process files with real-time updates
    file_paths = [f.name for f in uploaded_files]
    
    # Start processing
    st.session_state.agent.start_async(
        f"Process these files: {file_paths}"
    )
    
    # Show real-time progress
    for event in client.events():
        data = json.loads(event.data)
        
        if data["type"] == "progress":
            progress_container.progress(
                data["progress"] / 100,
                text=f"{data['progress']:.1f}% - {data['message']}"
            )
        elif data["type"] == "status_update":
            status_text.write(f"Status: {data['message']}")
        elif data["type"] == "complete":
            st.success("Processing complete!")
            st.balloons()
            break
```

### 2. Gradio with Streaming Responses

```python
import gradio as gr
from praisonaiagents import Agent, MCP
import asyncio

# Create streaming agent
agent = Agent(
    name="Streaming Chat Assistant",
    instructions="You provide helpful responses with real-time updates",
    tools=MCP("http://localhost:8080/sse")
)

async def stream_response(message, history):
    """Stream responses with typing effect"""
    # Start agent processing
    full_response = ""
    
    # Simulate streaming response
    response_generator = agent.stream(message)
    
    async for chunk in response_generator:
        full_response += chunk
        # Yield partial response for Gradio
        yield history + [[message, full_response]]
    
    # Final response
    yield history + [[message, full_response]]

# Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# Streaming AI Assistant")
    
    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="Message")
    clear = gr.Button("Clear")
    
    async def user_message(message, history):
        return "", history + [[message, None]]
    
    async def bot_response(history):
        user_message = history[-1][0]
        async for updated_history in stream_response(user_message, history[:-1]):
            yield updated_history
    
    msg.submit(user_message, [msg, chatbot], [msg, chatbot]).then(
        bot_response, chatbot, chatbot
    )
    clear.click(lambda: None, None, chatbot, queue=False)

demo.queue()
demo.launch()
```

## Event-Driven Architectures

### 1. Event Bus Pattern

```python
from mcp.server.fastmcp import FastMCP
from typing import Dict, List, Callable
import asyncio

mcp = FastMCP("event-bus")

class EventBus:
    def __init__(self):
        self.subscribers: Dict[str, List[Callable]] = {}
        self.event_history = []
        
    async def publish(self, event_type: str, data: dict):
        """Publish event to subscribers"""
        event = {
            "id": str(uuid.uuid4()),
            "type": event_type,
            "data": data,
            "timestamp": datetime.now().isoformat()
        }
        
        self.event_history.append(event)
        
        # Notify subscribers
        if event_type in self.subscribers:
            for callback in self.subscribers[event_type]:
                await callback(event)
        
        # Also notify wildcard subscribers
        if "*" in self.subscribers:
            for callback in self.subscribers["*"]:
                await callback(event)
        
        return event
    
    def subscribe(self, event_type: str, callback: Callable):
        """Subscribe to event type"""
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        self.subscribers[event_type].append(callback)

event_bus = EventBus()

@mcp.tool()
async def publish_event(event_type: str, data: dict):
    """Publish an event to the event bus"""
    event = await event_bus.publish(event_type, data)
    return f"Published event {event['id']} of type {event_type}"

@mcp.tool()
async def stream_events(event_types: list[str] = None):
    """Stream events matching specified types"""
    queue = asyncio.Queue()
    
    async def event_handler(event):
        await queue.put(f"data: {json.dumps(event)}\n\n")
    
    # Subscribe to requested event types
    if event_types:
        for event_type in event_types:
            event_bus.subscribe(event_type, event_handler)
    else:
        # Subscribe to all events
        event_bus.subscribe("*", event_handler)
    
    # Stream events
    try:
        while True:
            event_data = await queue.get()
            yield event_data
    except asyncio.CancelledError:
        # Cleanup subscriptions
        raise

# Usage example
from praisonaiagents import Agent, MCP

# Event publisher agent
publisher = Agent(
    name="Event Publisher",
    instructions="You publish events based on system activities",
    tools=MCP("http://localhost:8080/sse")
)

# Event consumer agent
consumer = Agent(
    name="Event Consumer",
    instructions="You react to system events",
    tools=MCP("http://localhost:8080/sse"),
    event_handler=lambda event: print(f"Received: {event}")
)

# Publish events
publisher.start("Publish a user_login event for user john_doe")
publisher.start("Publish a data_processed event with result: success")

# Consumer automatically receives and processes events
```

### 2. Workflow Progress Tracking

```python
from mcp.server.fastmcp import FastMCP
import asyncio

mcp = FastMCP("workflow-tracker")

class WorkflowEngine:
    def __init__(self):
        self.workflows = {}
        self.step_handlers = {}
    
    async def execute_workflow(self, workflow_id: str, steps: list[dict]):
        """Execute workflow with SSE progress updates"""
        self.workflows[workflow_id] = {
            "id": workflow_id,
            "steps": steps,
            "current_step": 0,
            "status": "running",
            "results": []
        }
        
        for idx, step in enumerate(steps):
            # Update current step
            self.workflows[workflow_id]["current_step"] = idx
            
            # Emit step started event
            yield f"data: {json.dumps({
                'type': 'step_started',
                'workflow_id': workflow_id,
                'step_index': idx,
                'step_name': step['name'],
                'total_steps': len(steps)
            })}\n\n"
            
            # Execute step
            try:
                result = await self.execute_step(step)
                self.workflows[workflow_id]["results"].append(result)
                
                # Emit step completed event
                yield f"data: {json.dumps({
                    'type': 'step_completed',
                    'workflow_id': workflow_id,
                    'step_index': idx,
                    'step_name': step['name'],
                    'result': result
                })}\n\n"
                
            except Exception as e:
                # Emit error event
                yield f"data: {json.dumps({
                    'type': 'step_error',
                    'workflow_id': workflow_id,
                    'step_index': idx,
                    'step_name': step['name'],
                    'error': str(e)
                })}\n\n"
                
                self.workflows[workflow_id]["status"] = "failed"
                break
        
        # Workflow complete
        self.workflows[workflow_id]["status"] = "completed"
        yield f"data: {json.dumps({
            'type': 'workflow_completed',
            'workflow_id': workflow_id,
            'status': self.workflows[workflow_id]['status'],
            'results': self.workflows[workflow_id]['results']
        })}\n\n"
    
    async def execute_step(self, step: dict):
        """Execute individual workflow step"""
        # Simulate step execution
        await asyncio.sleep(1)
        return f"Completed {step['name']}"

workflow_engine = WorkflowEngine()

@mcp.tool()
async def run_workflow(workflow_name: str, steps: list[dict]):
    """Run a workflow with real-time progress"""
    workflow_id = str(uuid.uuid4())
    
    async for event in workflow_engine.execute_workflow(workflow_id, steps):
        yield event
    
    return f"Workflow {workflow_name} completed"
```

## Performance Optimization

### 1. Connection Management

```python
from mcp.server.fastmcp import FastMCP
import asyncio
from contextlib import asynccontextmanager

class SSEConnectionPool:
    def __init__(self, max_connections=100):
        self.max_connections = max_connections
        self.active_connections = {}
        self.connection_queue = asyncio.Queue(maxsize=max_connections)
    
    @asynccontextmanager
    async def get_connection(self, client_id: str):
        """Get or create SSE connection"""
        try:
            # Wait for available slot
            await self.connection_queue.put(client_id)
            
            if client_id not in self.active_connections:
                self.active_connections[client_id] = {
                    "queue": asyncio.Queue(),
                    "created": datetime.now()
                }
            
            yield self.active_connections[client_id]
            
        finally:
            # Release connection slot
            await self.connection_queue.get()

# Use connection pool in MCP server
connection_pool = SSEConnectionPool()

@mcp.tool()
async def stream_with_connection_pool(client_id: str):
    """Stream data with connection pooling"""
    async with connection_pool.get_connection(client_id) as conn:
        queue = conn["queue"]
        
        # Stream data from queue
        while True:
            data = await queue.get()
            yield f"data: {data}\n\n"
```

### 2. Event Batching

```python
class EventBatcher:
    def __init__(self, batch_size=10, batch_timeout=0.1):
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.pending_events = []
        self.last_flush = asyncio.get_event_loop().time()
    
    async def add_event(self, event: dict):
        """Add event to batch"""
        self.pending_events.append(event)
        
        # Check if batch should be flushed
        current_time = asyncio.get_event_loop().time()
        
        if (len(self.pending_events) >= self.batch_size or 
            current_time - self.last_flush >= self.batch_timeout):
            return await self.flush()
        
        return None
    
    async def flush(self):
        """Flush pending events"""
        if not self.pending_events:
            return None
        
        batch = {
            "type": "event_batch",
            "events": self.pending_events,
            "count": len(self.pending_events),
            "timestamp": datetime.now().isoformat()
        }
        
        self.pending_events = []
        self.last_flush = asyncio.get_event_loop().time()
        
        return f"data: {json.dumps(batch)}\n\n"
```

## Best Practices

### 1. Implement Heartbeats

```python
@mcp.tool()
async def stream_with_heartbeat(data_stream):
    """Stream data with periodic heartbeats"""
    last_event = asyncio.get_event_loop().time()
    heartbeat_interval = 30  # seconds
    
    async for data in data_stream:
        yield f"data: {json.dumps(data)}\n\n"
        last_event = asyncio.get_event_loop().time()
        
        # Check if heartbeat needed
        while asyncio.get_event_loop().time() - last_event > heartbeat_interval:
            yield ":heartbeat\n\n"
            last_event = asyncio.get_event_loop().time()
            await asyncio.sleep(heartbeat_interval)
```

### 2. Error Recovery

```python
class ResilientSSEClient:
    def __init__(self, url, max_retries=3):
        self.url = url
        self.max_retries = max_retries
        self.retry_count = 0
    
    async def connect(self):
        """Connect with automatic retry"""
        while self.retry_count < self.max_retries:
            try:
                response = await self.make_request()
                async for event in self.process_events(response):
                    yield event
                    
                # Reset retry count on successful connection
                self.retry_count = 0
                
            except Exception as e:
                self.retry_count += 1
                wait_time = min(2 ** self.retry_count, 60)
                print(f"Connection failed, retrying in {wait_time}s...")
                await asyncio.sleep(wait_time)
```

### 3. Event Filtering

```python
@mcp.tool()
async def stream_filtered_events(
    event_types: list[str] = None,
    min_priority: int = 0,
    include_metadata: bool = True
):
    """Stream events with filtering"""
    async for event in event_source:
        # Apply filters
        if event_types and event.get("type") not in event_types:
            continue
            
        if event.get("priority", 0) < min_priority:
            continue
        
        # Add metadata if requested
        if include_metadata:
            event["_metadata"] = {
                "filtered": True,
                "timestamp": datetime.now().isoformat(),
                "server_id": "sse-01"
            }
        
        yield f"data: {json.dumps(event)}\n\n"
```

## Conclusion

MCP SSE provides powerful real-time streaming capabilities for PraisonAI agents. By implementing Server-Sent Events, you can build responsive applications with live updates, progress tracking, and event-driven architectures. The combination of SSE with agent systems enables sophisticated patterns like multi-agent coordination, workflow tracking, and real-time data processing.

## Next Steps

- Explore [Multi-Provider Examples](./multi-provider-switching.mdx) for streaming across providers
- Learn about [Session Persistence](./session-persistence.mdx) for stateful streaming
- Check out [Quality-Based RAG](./quality-based-rag.mdx) for streaming search results