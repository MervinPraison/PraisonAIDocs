---
title: Quality-Based RAG Advanced Retrieval Patterns
description: Learn how to implement sophisticated RAG systems with quality scoring, reranking, and advanced retrieval strategies
---

# Quality-Based RAG Advanced Retrieval Patterns

Quality-Based RAG in PraisonAI goes beyond simple vector similarity search by implementing sophisticated retrieval patterns that consider relevance, quality scores, reranking, and multi-tier memory systems for superior results.

## Overview

Advanced RAG patterns provide:
- Multi-dimensional quality scoring for retrieved content
- Reranking for improved relevance
- Hybrid search combining vector and keyword methods
- Quality-based storage decisions
- Advanced chunking strategies
- Performance optimization techniques

## Basic Quality-Based RAG Setup

### Simple RAG with Quality Scoring

```python
from praisonaiagents import Agent, Knowledge
import numpy as np

# Create knowledge base with quality scoring
knowledge = Knowledge(
    vector_db={
        "provider": "chroma",
        "config": {
            "collection_name": "quality_docs",
            "path": "./chroma_db"
        }
    },
    embedder={
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-large"
        }
    }
)

# Add documents with quality metadata
documents = [
    {
        "content": "Machine learning is a subset of artificial intelligence...",
        "metadata": {
            "source": "textbook",
            "quality_score": 0.95,
            "author": "Dr. Smith",
            "verified": True,
            "date": "2024-01-15"
        }
    },
    {
        "content": "ML is basically when computers learn stuff...",
        "metadata": {
            "source": "blog",
            "quality_score": 0.6,
            "author": "Anonymous",
            "verified": False,
            "date": "2023-06-10"
        }
    }
]

for doc in documents:
    knowledge.add(
        text=doc["content"],
        metadata=doc["metadata"]
    )

# Create agent with quality-aware retrieval
agent = Agent(
    name="Quality RAG Assistant",
    instructions="You provide accurate information using high-quality sources",
    knowledge=knowledge
)

# Retrieve with quality filtering
@agent.tool
def search_with_quality(query: str, min_quality: float = 0.8) -> str:
    """Search knowledge base with quality threshold"""
    results = knowledge.search(
        query=query,
        top_k=10,
        filter={"quality_score": {"$gte": min_quality}}
    )
    
    # Format results with quality indicators
    formatted_results = []
    for result in results:
        quality = result.metadata.get("quality_score", 0)
        source = result.metadata.get("source", "unknown")
        formatted_results.append(
            f"[Quality: {quality:.2f}, Source: {source}] {result.text}"
        )
    
    return "\n\n".join(formatted_results)
```

### Advanced Memory with Quality Management

```python
from praisonaiagents import Agent, Memory
from typing import Dict, List
import time

class QualityMemory(Memory):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.quality_weights = {
            "relevance": 0.4,
            "completeness": 0.3,
            "clarity": 0.2,
            "recency": 0.1
        }
    
    def calculate_quality_score(self, memory: Dict) -> float:
        """Calculate multi-dimensional quality score"""
        scores = {
            "relevance": self._calculate_relevance(memory),
            "completeness": self._calculate_completeness(memory),
            "clarity": self._calculate_clarity(memory),
            "recency": self._calculate_recency(memory)
        }
        
        # Weighted average
        total_score = sum(
            scores[metric] * weight 
            for metric, weight in self.quality_weights.items()
        )
        
        return total_score
    
    def _calculate_relevance(self, memory: Dict) -> float:
        """Calculate relevance based on semantic similarity"""
        # Simplified - in production, use embedding similarity
        return memory.get("similarity_score", 0.5)
    
    def _calculate_completeness(self, memory: Dict) -> float:
        """Calculate completeness based on content length and structure"""
        content = memory.get("content", "")
        
        # Check for key information markers
        has_context = "because" in content or "since" in content
        has_details = len(content.split()) > 20
        has_examples = "example" in content or "such as" in content
        
        score = 0.3
        if has_context: score += 0.3
        if has_details: score += 0.2
        if has_examples: score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_clarity(self, memory: Dict) -> float:
        """Calculate clarity based on readability metrics"""
        content = memory.get("content", "")
        
        # Simple clarity metrics
        avg_sentence_length = len(content.split()) / max(content.count('.'), 1)
        
        # Optimal sentence length is 15-20 words
        if 15 <= avg_sentence_length <= 20:
            return 1.0
        elif avg_sentence_length < 10 or avg_sentence_length > 30:
            return 0.5
        else:
            return 0.8
    
    def _calculate_recency(self, memory: Dict) -> float:
        """Calculate recency score based on timestamp"""
        timestamp = memory.get("timestamp", 0)
        age_days = (time.time() - timestamp) / 86400
        
        # Exponential decay
        return np.exp(-age_days / 30)  # Half-life of 30 days
    
    def add_with_quality(self, content: str, metadata: Dict = None) -> Dict:
        """Add memory with quality assessment"""
        memory = {
            "content": content,
            "metadata": metadata or {},
            "timestamp": time.time()
        }
        
        # Calculate quality
        quality_score = self.calculate_quality_score(memory)
        memory["quality_score"] = quality_score
        
        # Decide storage tier based on quality
        if quality_score >= 0.8:
            memory["tier"] = "long_term"
        elif quality_score >= 0.5:
            memory["tier"] = "short_term"
        else:
            memory["tier"] = "ephemeral"
        
        # Store in appropriate tier
        self.add(content, metadata={**metadata, **memory})
        
        return memory

# Create agent with quality memory
quality_memory = QualityMemory(
    provider="mem0",
    config={
        "vector_store": {
            "provider": "chroma",
            "config": {
                "collection_name": "quality_memories"
            }
        }
    }
)

agent = Agent(
    name="Quality-Aware Assistant",
    instructions="You maintain high-quality memory and provide accurate information",
    memory=quality_memory
)

# Add memories with automatic quality assessment
agent.memory.add_with_quality(
    "The capital of France is Paris. Paris is known for the Eiffel Tower, Louvre Museum, and its rich cultural heritage.",
    {"topic": "geography", "verified": True}
)

agent.memory.add_with_quality(
    "France capital = Paris",
    {"topic": "geography", "verified": False}
)

# Quality-based retrieval
high_quality_memories = agent.memory.search(
    "What is the capital of France?",
    filter={"quality_score": {"$gte": 0.7}}
)
```

## Advanced Retrieval Patterns

### 1. Reranking with Cross-Encoders

```python
from praisonaiagents import Agent, Knowledge
from sentence_transformers import CrossEncoder
import torch

class RerankingKnowledge(Knowledge):
    def __init__(self, reranking_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2", **kwargs):
        super().__init__(**kwargs)
        self.reranker = CrossEncoder(reranking_model)
    
    def search_with_reranking(
        self, 
        query: str, 
        initial_top_k: int = 50,
        final_top_k: int = 10,
        rerank: bool = True
    ) -> List[Dict]:
        """Search with optional reranking"""
        # Initial retrieval (cast a wide net)
        initial_results = self.search(query, top_k=initial_top_k)
        
        if not rerank or len(initial_results) <= final_top_k:
            return initial_results[:final_top_k]
        
        # Prepare pairs for reranking
        pairs = [(query, result["text"]) for result in initial_results]
        
        # Get reranking scores
        scores = self.reranker.predict(pairs)
        
        # Sort by reranking scores
        reranked_indices = np.argsort(scores)[::-1][:final_top_k]
        
        # Return reranked results
        reranked_results = []
        for idx in reranked_indices:
            result = initial_results[idx]
            result["rerank_score"] = float(scores[idx])
            reranked_results.append(result)
        
        return reranked_results

# Create knowledge base with reranking
reranking_kb = RerankingKnowledge(
    vector_db={
        "provider": "chroma",
        "config": {
            "collection_name": "reranked_docs"
        }
    }
)

# Agent with reranking
agent = Agent(
    name="Reranking Assistant",
    instructions="You provide the most relevant information using advanced reranking",
    knowledge=reranking_kb
)

@agent.tool
def search_documents(query: str, use_reranking: bool = True) -> str:
    """Search with optional reranking"""
    results = agent.knowledge.search_with_reranking(
        query=query,
        initial_top_k=50,
        final_top_k=5,
        rerank=use_reranking
    )
    
    response = f"Found {len(results)} relevant documents:\n\n"
    for i, result in enumerate(results, 1):
        score_info = f"(Rerank score: {result.get('rerank_score', 0):.3f})" if use_reranking else ""
        response += f"{i}. {result['text'][:200]}... {score_info}\n\n"
    
    return response

# Compare with and without reranking
print("With reranking:")
print(agent.search_documents("machine learning algorithms", use_reranking=True))

print("\nWithout reranking:")
print(agent.search_documents("machine learning algorithms", use_reranking=False))
```

### 2. Hybrid Search Pattern

```python
from praisonaiagents import Agent, Knowledge
from typing import List, Dict
import numpy as np

class HybridSearchKnowledge(Knowledge):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.alpha = 0.7  # Weight for vector search (0.7 vector, 0.3 keyword)
    
    def hybrid_search(
        self, 
        query: str,
        top_k: int = 10,
        keyword_boost_terms: List[str] = None
    ) -> List[Dict]:
        """Combine vector and keyword search"""
        # Vector search
        vector_results = self.search(query, top_k=top_k * 2)
        vector_scores = {r["id"]: r["score"] for r in vector_results}
        
        # Keyword search
        keyword_results = self._keyword_search(query, top_k=top_k * 2, boost_terms=keyword_boost_terms)
        keyword_scores = {r["id"]: r["score"] for r in keyword_results}
        
        # Combine scores
        all_ids = set(vector_scores.keys()) | set(keyword_scores.keys())
        combined_results = []
        
        for doc_id in all_ids:
            vector_score = vector_scores.get(doc_id, 0)
            keyword_score = keyword_scores.get(doc_id, 0)
            
            # Normalize scores
            norm_vector = vector_score / max(vector_scores.values()) if vector_scores else 0
            norm_keyword = keyword_score / max(keyword_scores.values()) if keyword_scores else 0
            
            # Combined score
            combined_score = self.alpha * norm_vector + (1 - self.alpha) * norm_keyword
            
            # Get document
            doc = self._get_document(doc_id)
            if doc:
                doc["hybrid_score"] = combined_score
                doc["vector_score"] = norm_vector
                doc["keyword_score"] = norm_keyword
                combined_results.append(doc)
        
        # Sort by combined score
        combined_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
        
        return combined_results[:top_k]
    
    def _keyword_search(self, query: str, top_k: int, boost_terms: List[str] = None) -> List[Dict]:
        """Perform keyword search with optional term boosting"""
        # Simplified BM25-like scoring
        query_terms = query.lower().split()
        boost_terms = boost_terms or []
        
        results = []
        all_docs = self._get_all_documents()  # Method to retrieve all docs
        
        for doc in all_docs:
            content = doc["text"].lower()
            score = 0
            
            # Base term matching
            for term in query_terms:
                term_freq = content.count(term)
                if term_freq > 0:
                    # BM25-like scoring
                    score += np.log(1 + term_freq) / (1 + np.log(len(content.split())))
            
            # Boost specific terms
            for boost_term in boost_terms:
                if boost_term.lower() in content:
                    score *= 1.5
            
            if score > 0:
                doc["score"] = score
                results.append(doc)
        
        # Sort by score
        results.sort(key=lambda x: x["score"], reverse=True)
        
        return results[:top_k]

# Create hybrid search agent
hybrid_kb = HybridSearchKnowledge()

agent = Agent(
    name="Hybrid Search Assistant",
    instructions="You use advanced hybrid search to find the most relevant information",
    knowledge=hybrid_kb
)

@agent.tool
def hybrid_search(query: str, important_terms: List[str] = None) -> str:
    """Search using hybrid vector + keyword approach"""
    results = agent.knowledge.hybrid_search(
        query=query,
        top_k=5,
        keyword_boost_terms=important_terms
    )
    
    response = "Hybrid search results:\n\n"
    for i, result in enumerate(results, 1):
        response += f"{i}. {result['text'][:150]}...\n"
        response += f"   Scores - Vector: {result['vector_score']:.3f}, "
        response += f"Keyword: {result['keyword_score']:.3f}, "
        response += f"Combined: {result['hybrid_score']:.3f}\n\n"
    
    return response

# Example usage
response = agent.hybrid_search(
    "machine learning classification algorithms",
    important_terms=["SVM", "neural network", "decision tree"]
)
```

### 3. Multi-Stage Retrieval Pipeline

```python
from praisonaiagents import Agent, Knowledge
from typing import List, Dict, Tuple
import asyncio

class MultiStageRAG:
    def __init__(self):
        self.stages = []
    
    def add_stage(self, name: str, retriever, processor=None):
        """Add a retrieval stage"""
        self.stages.append({
            "name": name,
            "retriever": retriever,
            "processor": processor or (lambda x: x)
        })
    
    async def retrieve(self, query: str) -> List[Dict]:
        """Execute multi-stage retrieval pipeline"""
        results = []
        context = {"query": query, "previous_results": []}
        
        for stage in self.stages:
            print(f"Executing stage: {stage['name']}")
            
            # Retrieve
            stage_results = await stage["retriever"](query, context)
            
            # Process
            processed_results = stage["processor"](stage_results)
            
            # Update context
            context["previous_results"].extend(processed_results)
            results.extend(processed_results)
        
        return self._deduplicate_results(results)
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """Remove duplicate results"""
        seen = set()
        unique_results = []
        
        for result in results:
            # Use content hash for deduplication
            content_hash = hash(result.get("text", ""))
            if content_hash not in seen:
                seen.add(content_hash)
                unique_results.append(result)
        
        return unique_results

# Create multi-stage pipeline
pipeline = MultiStageRAG()

# Stage 1: Fast approximate search
async def fast_search(query: str, context: dict) -> List[Dict]:
    # Quick vector search with lower precision
    kb = Knowledge(
        vector_db={"provider": "chroma", "config": {"collection_name": "fast_index"}}
    )
    return kb.search(query, top_k=100)

# Stage 2: Semantic filtering
async def semantic_filter(query: str, context: dict) -> List[Dict]:
    # Filter previous results based on semantic similarity threshold
    prev_results = context["previous_results"]
    filtered = [r for r in prev_results if r.get("score", 0) > 0.7]
    return filtered[:50]

# Stage 3: Reranking
async def rerank_stage(query: str, context: dict) -> List[Dict]:
    # Use cross-encoder to rerank
    prev_results = context["previous_results"]
    # Reranking logic here
    return prev_results[:20]

# Stage 4: Quality filtering
async def quality_filter(query: str, context: dict) -> List[Dict]:
    # Filter by quality metrics
    prev_results = context["previous_results"]
    high_quality = [
        r for r in prev_results 
        if r.get("metadata", {}).get("quality_score", 0) > 0.8
    ]
    return high_quality[:10]

# Add stages to pipeline
pipeline.add_stage("fast_search", fast_search)
pipeline.add_stage("semantic_filter", semantic_filter)
pipeline.add_stage("rerank", rerank_stage)
pipeline.add_stage("quality_filter", quality_filter)

# Create agent with pipeline
agent = Agent(
    name="Pipeline RAG Assistant",
    instructions="You use a sophisticated multi-stage retrieval pipeline"
)

@agent.tool
async def advanced_search(query: str) -> str:
    """Search using multi-stage pipeline"""
    results = await pipeline.retrieve(query)
    
    response = f"Found {len(results)} high-quality results:\n\n"
    for i, result in enumerate(results, 1):
        response += f"{i}. {result['text'][:200]}...\n"
        response += f"   Quality: {result.get('metadata', {}).get('quality_score', 'N/A')}\n\n"
    
    return response
```

## Advanced Chunking Strategies

### 1. Semantic Chunking

```python
from praisonaiagents import Knowledge
from typing import List
import numpy as np
from sentence_transformers import SentenceTransformer

class SemanticChunker:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", threshold: float = 0.5):
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
    
    def chunk_document(self, text: str, min_chunk_size: int = 100) -> List[str]:
        """Split document into semantic chunks"""
        # Split into sentences
        sentences = text.split('. ')
        
        if not sentences:
            return []
        
        # Get embeddings for all sentences
        embeddings = self.model.encode(sentences)
        
        # Find semantic boundaries
        chunks = []
        current_chunk = [sentences[0]]
        current_embedding = embeddings[0]
        
        for i in range(1, len(sentences)):
            # Calculate similarity with current chunk
            similarity = np.dot(current_embedding, embeddings[i]) / (
                np.linalg.norm(current_embedding) * np.linalg.norm(embeddings[i])
            )
            
            # Check if we should start a new chunk
            if similarity < self.threshold or len('. '.join(current_chunk)) > min_chunk_size * 2:
                # Save current chunk
                chunks.append('. '.join(current_chunk) + '.')
                
                # Start new chunk
                current_chunk = [sentences[i]]
                current_embedding = embeddings[i]
            else:
                # Add to current chunk
                current_chunk.append(sentences[i])
                # Update chunk embedding (average)
                current_embedding = np.mean(
                    [current_embedding, embeddings[i]], 
                    axis=0
                )
        
        # Add final chunk
        if current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
        
        return chunks

# Use semantic chunking with knowledge base
semantic_chunker = SemanticChunker(threshold=0.6)

knowledge = Knowledge(
    chunking_strategy="custom",
    custom_chunker=semantic_chunker.chunk_document
)

# Add document with semantic chunking
document = """
Machine learning is a subset of artificial intelligence that focuses on the development of algorithms.
These algorithms allow computers to learn from and make predictions based on data.
The key advantage of machine learning is its ability to improve performance without explicit programming.

Deep learning is a specialized branch of machine learning that uses neural networks.
Neural networks are inspired by the human brain's structure and function.
They consist of layers of interconnected nodes that process information.

Natural language processing is another important application of machine learning.
It enables computers to understand, interpret, and generate human language.
NLP powers many modern applications including chatbots and translation services.
"""

chunks = semantic_chunker.chunk_document(document)
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {chunk[:100]}...")
    knowledge.add(chunk, metadata={"chunk_index": i, "chunking_method": "semantic"})
```

### 2. Sliding Window with Overlap

```python
class SlidingWindowChunker:
    def __init__(self, window_size: int = 512, overlap: int = 128):
        self.window_size = window_size
        self.overlap = overlap
    
    def chunk_with_context(self, text: str) -> List[Dict[str, str]]:
        """Create chunks with surrounding context"""
        words = text.split()
        chunks = []
        
        i = 0
        while i < len(words):
            # Main chunk
            chunk_end = min(i + self.window_size, len(words))
            chunk_words = words[i:chunk_end]
            
            # Previous context (from previous chunk)
            prev_context_start = max(0, i - self.overlap)
            prev_context = words[prev_context_start:i]
            
            # Next context (preview of next chunk)
            next_context_end = min(len(words), chunk_end + self.overlap)
            next_context = words[chunk_end:next_context_end]
            
            chunks.append({
                "text": " ".join(chunk_words),
                "prev_context": " ".join(prev_context),
                "next_context": " ".join(next_context),
                "position": i,
                "total_words": len(words)
            })
            
            # Move window
            i += self.window_size - self.overlap
        
        return chunks

# Create knowledge base with context-aware chunks
sliding_chunker = SlidingWindowChunker(window_size=200, overlap=50)

knowledge = Knowledge()

document = """Your long document text here..."""
chunks = sliding_chunker.chunk_with_context(document)

for chunk_data in chunks:
    # Store chunk with context metadata
    knowledge.add(
        text=chunk_data["text"],
        metadata={
            "prev_context": chunk_data["prev_context"],
            "next_context": chunk_data["next_context"],
            "position": chunk_data["position"],
            "chunking_method": "sliding_window"
        }
    )

# Retrieval with context
def retrieve_with_context(query: str, top_k: int = 3) -> List[str]:
    """Retrieve chunks and include surrounding context"""
    results = knowledge.search(query, top_k=top_k)
    
    enhanced_results = []
    for result in results:
        # Reconstruct with context
        full_text = ""
        if result.metadata.get("prev_context"):
            full_text += f"[Previous context: {result.metadata['prev_context']}]\n\n"
        
        full_text += result.text
        
        if result.metadata.get("next_context"):
            full_text += f"\n\n[Following context: {result.metadata['next_context']}]"
        
        enhanced_results.append(full_text)
    
    return enhanced_results
```

### 3. Hierarchical Chunking

```python
class HierarchicalChunker:
    def __init__(self):
        self.levels = {
            "document": {"max_size": 10000, "overlap": 1000},
            "section": {"max_size": 2000, "overlap": 200},
            "paragraph": {"max_size": 500, "overlap": 50},
            "sentence": {"max_size": 100, "overlap": 0}
        }
    
    def create_hierarchy(self, text: str) -> Dict:
        """Create hierarchical representation of document"""
        hierarchy = {
            "document": {
                "text": text[:self.levels["document"]["max_size"]],
                "sections": []
            }
        }
        
        # Split into sections (by double newline)
        sections = text.split('\n\n')
        
        for section_text in sections:
            section = {
                "text": section_text[:self.levels["section"]["max_size"]],
                "paragraphs": []
            }
            
            # Split into paragraphs (by single newline)
            paragraphs = section_text.split('\n')
            
            for para_text in paragraphs:
                paragraph = {
                    "text": para_text[:self.levels["paragraph"]["max_size"]],
                    "sentences": []
                }
                
                # Split into sentences
                sentences = para_text.split('. ')
                
                for sent_text in sentences:
                    if sent_text.strip():
                        paragraph["sentences"].append({
                            "text": sent_text[:self.levels["sentence"]["max_size"]]
                        })
                
                if paragraph["sentences"]:
                    section["paragraphs"].append(paragraph)
            
            if section["paragraphs"]:
                hierarchy["document"]["sections"].append(section)
        
        return hierarchy
    
    def flatten_hierarchy(self, hierarchy: Dict, include_level: bool = True) -> List[Dict]:
        """Flatten hierarchy for storage"""
        chunks = []
        
        def traverse(node, level, parent_context=""):
            if "text" in node:
                chunk = {
                    "text": node["text"],
                    "level": level,
                    "parent_context": parent_context
                }
                chunks.append(chunk)
                
                # Update parent context for children
                parent_context = node["text"][:100]  # First 100 chars as context
            
            # Traverse children
            for child_key in ["sections", "paragraphs", "sentences"]:
                if child_key in node:
                    for child in node[child_key]:
                        traverse(child, child_key[:-1], parent_context)  # Remove 's' from key
        
        traverse(hierarchy["document"], "document")
        return chunks

# Use hierarchical chunking
hierarchical_chunker = HierarchicalChunker()

document = """
# Introduction to Machine Learning

Machine learning has revolutionized how we process data.
It enables computers to learn patterns without explicit programming.

## Supervised Learning

Supervised learning uses labeled data to train models.
Common algorithms include decision trees and neural networks.

### Classification

Classification predicts discrete categories.
Examples include spam detection and image recognition.

### Regression  

Regression predicts continuous values.
It's used for price prediction and trend analysis.
"""

# Create hierarchy
hierarchy = hierarchical_chunker.create_hierarchy(document)
chunks = hierarchical_chunker.flatten_hierarchy(hierarchy)

# Store with level information
knowledge = Knowledge()
for chunk in chunks:
    knowledge.add(
        text=chunk["text"],
        metadata={
            "level": chunk["level"],
            "parent_context": chunk["parent_context"],
            "chunking_method": "hierarchical"
        }
    )

# Multi-level retrieval
def hierarchical_search(query: str) -> Dict[str, List]:
    """Search at different hierarchy levels"""
    results_by_level = {}
    
    for level in ["sentence", "paragraph", "section", "document"]:
        results = knowledge.search(
            query,
            filter={"level": level},
            top_k=3
        )
        results_by_level[level] = results
    
    return results_by_level
```

## Performance Optimization

### 1. Caching and Memoization

```python
from functools import lru_cache
import hashlib
import pickle
import redis

class CachedRAG:
    def __init__(self, knowledge_base: Knowledge, cache_backend: str = "memory"):
        self.knowledge = knowledge_base
        self.cache_backend = cache_backend
        
        if cache_backend == "redis":
            self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        else:
            self.memory_cache = {}
    
    def _generate_cache_key(self, query: str, params: dict) -> str:
        """Generate cache key from query and parameters"""
        cache_data = f"{query}:{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    def _get_from_cache(self, key: str):
        """Retrieve from cache"""
        if self.cache_backend == "redis":
            cached = self.redis_client.get(key)
            return pickle.loads(cached) if cached else None
        else:
            return self.memory_cache.get(key)
    
    def _set_cache(self, key: str, value: any, ttl: int = 3600):
        """Store in cache"""
        if self.cache_backend == "redis":
            self.redis_client.setex(key, ttl, pickle.dumps(value))
        else:
            self.memory_cache[key] = value
    
    @lru_cache(maxsize=128)
    def search_with_cache(self, query: str, top_k: int = 10) -> List[Dict]:
        """Cached search with LRU for frequent queries"""
        cache_key = self._generate_cache_key(query, {"top_k": top_k})
        
        # Check cache
        cached_results = self._get_from_cache(cache_key)
        if cached_results is not None:
            return cached_results
        
        # Perform search
        results = self.knowledge.search(query, top_k=top_k)
        
        # Cache results
        self._set_cache(cache_key, results)
        
        return results
    
    def warm_cache(self, common_queries: List[str]):
        """Pre-populate cache with common queries"""
        for query in common_queries:
            self.search_with_cache(query)
        print(f"Warmed cache with {len(common_queries)} queries")

# Use cached RAG
cached_rag = CachedRAG(knowledge, cache_backend="memory")

# Warm cache with common queries
common_queries = [
    "What is machine learning?",
    "How does neural network work?",
    "Explain deep learning",
    "What are embedding vectors?"
]
cached_rag.warm_cache(common_queries)

# Fast retrieval for cached queries
import time

# First call - hits the knowledge base
start = time.time()
results1 = cached_rag.search_with_cache("What is machine learning?")
print(f"First call: {time.time() - start:.3f}s")

# Second call - hits the cache
start = time.time()
results2 = cached_rag.search_with_cache("What is machine learning?")
print(f"Cached call: {time.time() - start:.3f}s")
```

### 2. Batch Processing

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import numpy as np

class BatchRAG:
    def __init__(self, knowledge_base: Knowledge, batch_size: int = 32):
        self.knowledge = knowledge_base
        self.batch_size = batch_size
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def batch_search(self, queries: List[str]) -> List[List[Dict]]:
        """Process multiple queries in batches"""
        results = []
        
        # Process in batches
        for i in range(0, len(queries), self.batch_size):
            batch = queries[i:i + self.batch_size]
            
            # Process batch in parallel
            batch_results = await self._process_batch(batch)
            results.extend(batch_results)
        
        return results
    
    async def _process_batch(self, batch: List[str]) -> List[List[Dict]]:
        """Process a single batch of queries"""
        # Get embeddings for all queries at once
        embeddings = self._batch_embed(batch)
        
        # Perform batch similarity search
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(
                self.executor,
                self._single_search,
                query,
                embedding
            )
            for query, embedding in zip(batch, embeddings)
        ]
        
        return await asyncio.gather(*tasks)
    
    def _batch_embed(self, texts: List[str]) -> np.ndarray:
        """Get embeddings for multiple texts at once"""
        # This would use your embedding model's batch processing
        # For example with sentence-transformers:
        # return self.model.encode(texts, batch_size=self.batch_size)
        pass
    
    def _single_search(self, query: str, embedding: np.ndarray) -> List[Dict]:
        """Perform single search with pre-computed embedding"""
        # Use pre-computed embedding for faster search
        return self.knowledge.search_with_embedding(embedding)

# Usage
batch_rag = BatchRAG(knowledge, batch_size=16)

# Process many queries efficiently
queries = [
    "What is AI?",
    "Explain machine learning",
    "How do neural networks work?",
    # ... many more queries
]

# Batch processing
results = asyncio.run(batch_rag.batch_search(queries))
```

### 3. Index Optimization

```python
class OptimizedKnowledge(Knowledge):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.indices = {}
    
    def build_optimized_indices(self):
        """Build multiple specialized indices"""
        
        # Build quality-based index
        self.indices["quality"] = self._build_quality_index()
        
        # Build date-based index
        self.indices["date"] = self._build_date_index()
        
        # Build source-based index
        self.indices["source"] = self._build_source_index()
        
        # Build semantic clusters
        self.indices["clusters"] = self._build_semantic_clusters()
    
    def _build_quality_index(self) -> Dict:
        """Index documents by quality tiers"""
        quality_tiers = {
            "excellent": [],
            "good": [],
            "average": [],
            "poor": []
        }
        
        for doc in self.get_all_documents():
            quality = doc.metadata.get("quality_score", 0.5)
            
            if quality >= 0.9:
                quality_tiers["excellent"].append(doc["id"])
            elif quality >= 0.7:
                quality_tiers["good"].append(doc["id"])
            elif quality >= 0.5:
                quality_tiers["average"].append(doc["id"])
            else:
                quality_tiers["poor"].append(doc["id"])
        
        return quality_tiers
    
    def _build_semantic_clusters(self, n_clusters: int = 10) -> Dict:
        """Build semantic clusters for faster similarity search"""
        from sklearn.cluster import KMeans
        
        # Get all embeddings
        embeddings = []
        doc_ids = []
        
        for doc in self.get_all_documents():
            if "embedding" in doc:
                embeddings.append(doc["embedding"])
                doc_ids.append(doc["id"])
        
        if not embeddings:
            return {}
        
        # Cluster embeddings
        embeddings_array = np.array(embeddings)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings_array)
        
        # Build cluster index
        clusters = {}
        for doc_id, cluster_id in zip(doc_ids, cluster_labels):
            if cluster_id not in clusters:
                clusters[cluster_id] = []
            clusters[cluster_id].append(doc_id)
        
        return {
            "clusters": clusters,
            "centroids": kmeans.cluster_centers_,
            "model": kmeans
        }
    
    def search_with_optimization(
        self, 
        query: str,
        use_quality_filter: bool = True,
        use_clustering: bool = True,
        min_quality: float = 0.7
    ) -> List[Dict]:
        """Search using optimized indices"""
        
        # Filter by quality first
        if use_quality_filter:
            quality_doc_ids = []
            for tier in ["excellent", "good"]:
                quality_doc_ids.extend(self.indices["quality"][tier])
            
            # Limit search to high-quality documents
            search_filter = {"id": {"$in": quality_doc_ids}}
        else:
            search_filter = None
        
        # Use clustering for initial filtering
        if use_clustering and "clusters" in self.indices:
            # Find nearest cluster
            query_embedding = self.embed(query)
            cluster_data = self.indices["clusters"]
            
            distances = np.linalg.norm(
                cluster_data["centroids"] - query_embedding,
                axis=1
            )
            nearest_clusters = np.argsort(distances)[:3]  # Top 3 clusters
            
            # Get documents from nearest clusters
            cluster_doc_ids = []
            for cluster_id in nearest_clusters:
                cluster_doc_ids.extend(cluster_data["clusters"][cluster_id])
            
            if search_filter:
                # Intersect with quality filter
                search_filter["id"]["$in"] = list(
                    set(search_filter["id"]["$in"]) & set(cluster_doc_ids)
                )
            else:
                search_filter = {"id": {"$in": cluster_doc_ids}}
        
        # Perform optimized search
        return self.search(query, filter=search_filter)
```

## Best Practices

### 1. Quality Metrics Design

```python
class QualityMetricsFramework:
    """Framework for designing quality metrics"""
    
    def __init__(self):
        self.metrics = {}
    
    def add_metric(
        self, 
        name: str, 
        scorer_func: callable,
        weight: float = 1.0,
        normalize: bool = True
    ):
        """Add a quality metric"""
        self.metrics[name] = {
            "scorer": scorer_func,
            "weight": weight,
            "normalize": normalize
        }
    
    def calculate_composite_score(self, document: Dict) -> float:
        """Calculate weighted composite quality score"""
        scores = {}
        
        for metric_name, metric_config in self.metrics.items():
            # Calculate raw score
            raw_score = metric_config["scorer"](document)
            
            # Normalize if needed
            if metric_config["normalize"]:
                raw_score = max(0, min(1, raw_score))
            
            scores[metric_name] = raw_score * metric_config["weight"]
        
        # Calculate weighted average
        total_weight = sum(m["weight"] for m in self.metrics.values())
        composite_score = sum(scores.values()) / total_weight if total_weight > 0 else 0
        
        return composite_score, scores

# Example quality metrics
quality_framework = QualityMetricsFramework()

# Source reliability metric
def source_reliability_scorer(doc: Dict) -> float:
    source = doc.get("metadata", {}).get("source", "")
    reliability_scores = {
        "academic": 1.0,
        "textbook": 0.95,
        "documentation": 0.9,
        "blog": 0.6,
        "forum": 0.5,
        "unknown": 0.3
    }
    return reliability_scores.get(source, 0.3)

# Freshness metric
def freshness_scorer(doc: Dict) -> float:
    import datetime
    date_str = doc.get("metadata", {}).get("date", "")
    if not date_str:
        return 0.5
    
    try:
        doc_date = datetime.datetime.fromisoformat(date_str)
        age_days = (datetime.datetime.now() - doc_date).days
        # Exponential decay with 180-day half-life
        return np.exp(-age_days / 180)
    except:
        return 0.5

# Completeness metric
def completeness_scorer(doc: Dict) -> float:
    text = doc.get("text", "")
    
    # Check various completeness indicators
    has_examples = "example" in text.lower() or "for instance" in text.lower()
    has_explanation = "because" in text.lower() or "therefore" in text.lower()
    has_definition = "is defined as" in text.lower() or "means" in text.lower()
    sufficient_length = len(text.split()) > 50
    
    score = 0.25 * (has_examples + has_explanation + has_definition + sufficient_length)
    return score

# Add metrics to framework
quality_framework.add_metric("source_reliability", source_reliability_scorer, weight=2.0)
quality_framework.add_metric("freshness", freshness_scorer, weight=1.5)
quality_framework.add_metric("completeness", completeness_scorer, weight=1.0)
```

### 2. Evaluation and Testing

```python
class RAGEvaluator:
    """Evaluate RAG system performance"""
    
    def __init__(self, ground_truth: List[Dict]):
        """
        ground_truth: List of {"query": str, "expected_docs": List[str], "expected_answer": str}
        """
        self.ground_truth = ground_truth
    
    def evaluate_retrieval(self, rag_system, k_values: List[int] = [1, 5, 10]) -> Dict:
        """Evaluate retrieval performance"""
        metrics = {f"recall@{k}": [] for k in k_values}
        metrics["mrr"] = []  # Mean Reciprocal Rank
        
        for test_case in self.ground_truth:
            query = test_case["query"]
            expected_docs = set(test_case["expected_docs"])
            
            # Get retrieval results
            results = rag_system.search(query, top_k=max(k_values))
            retrieved_ids = [r["id"] for r in results]
            
            # Calculate recall@k
            for k in k_values:
                retrieved_k = set(retrieved_ids[:k])
                recall = len(retrieved_k & expected_docs) / len(expected_docs)
                metrics[f"recall@{k}"].append(recall)
            
            # Calculate MRR
            for i, doc_id in enumerate(retrieved_ids):
                if doc_id in expected_docs:
                    metrics["mrr"].append(1 / (i + 1))
                    break
            else:
                metrics["mrr"].append(0)
        
        # Average metrics
        return {
            metric: np.mean(values) 
            for metric, values in metrics.items()
        }
    
    def evaluate_answer_quality(self, rag_system, llm_judge=None) -> Dict:
        """Evaluate generated answer quality"""
        metrics = {
            "relevance": [],
            "correctness": [],
            "completeness": []
        }
        
        for test_case in self.ground_truth:
            query = test_case["query"]
            expected_answer = test_case.get("expected_answer", "")
            
            # Generate answer
            generated_answer = rag_system.generate_answer(query)
            
            if llm_judge:
                # Use LLM as judge
                eval_prompt = f"""
                Query: {query}
                Expected Answer: {expected_answer}
                Generated Answer: {generated_answer}
                
                Rate the generated answer on:
                1. Relevance (0-1): How relevant is it to the query?
                2. Correctness (0-1): How factually correct is it?
                3. Completeness (0-1): How complete is the answer?
                
                Return JSON: {{"relevance": X, "correctness": Y, "completeness": Z}}
                """
                
                eval_result = llm_judge(eval_prompt)
                # Parse results and add to metrics
            else:
                # Simple similarity-based evaluation
                from difflib import SequenceMatcher
                similarity = SequenceMatcher(None, expected_answer, generated_answer).ratio()
                
                metrics["relevance"].append(similarity)
                metrics["correctness"].append(similarity)
                metrics["completeness"].append(similarity)
        
        return {
            metric: np.mean(values)
            for metric, values in metrics.items()
        }

# Example evaluation
test_cases = [
    {
        "query": "What is machine learning?",
        "expected_docs": ["doc_001", "doc_002", "doc_015"],
        "expected_answer": "Machine learning is a subset of AI that enables systems to learn from data."
    },
    # More test cases...
]

evaluator = RAGEvaluator(test_cases)
retrieval_metrics = evaluator.evaluate_retrieval(rag_system)
print("Retrieval Performance:", retrieval_metrics)
```

### 3. Continuous Improvement

```python
class AdaptiveRAG:
    """RAG system that learns from feedback"""
    
    def __init__(self, knowledge_base: Knowledge):
        self.knowledge = knowledge_base
        self.feedback_history = []
        self.performance_metrics = defaultdict(list)
    
    def search_and_track(self, query: str, user_id: str = None) -> List[Dict]:
        """Search and track for learning"""
        search_id = str(uuid.uuid4())
        
        # Perform search
        results = self.knowledge.search(query)
        
        # Track search
        self.feedback_history.append({
            "search_id": search_id,
            "query": query,
            "user_id": user_id,
            "results": [r["id"] for r in results],
            "timestamp": time.time()
        })
        
        return results, search_id
    
    def record_feedback(
        self, 
        search_id: str,
        relevant_docs: List[str],
        irrelevant_docs: List[str] = None
    ):
        """Record user feedback on search results"""
        feedback = {
            "search_id": search_id,
            "relevant_docs": relevant_docs,
            "irrelevant_docs": irrelevant_docs or [],
            "timestamp": time.time()
        }
        
        # Update document scores based on feedback
        for doc_id in relevant_docs:
            self._boost_document(doc_id, boost_factor=1.1)
        
        for doc_id in irrelevant_docs or []:
            self._boost_document(doc_id, boost_factor=0.9)
        
        # Learn from patterns
        self._analyze_feedback_patterns()
    
    def _boost_document(self, doc_id: str, boost_factor: float):
        """Adjust document relevance score"""
        # Update document metadata with boost
        doc = self.knowledge.get_document(doc_id)
        if doc:
            current_boost = doc.metadata.get("relevance_boost", 1.0)
            new_boost = current_boost * boost_factor
            # Clamp between 0.1 and 10
            new_boost = max(0.1, min(10, new_boost))
            
            doc.metadata["relevance_boost"] = new_boost
            self.knowledge.update_document(doc_id, doc)
    
    def _analyze_feedback_patterns(self):
        """Analyze feedback to improve retrieval"""
        # Analyze patterns in feedback
        if len(self.feedback_history) < 100:
            return
        
        # Example: Find commonly irrelevant sources
        irrelevant_sources = defaultdict(int)
        
        for feedback in self.feedback_history[-100:]:
            for doc_id in feedback.get("irrelevant_docs", []):
                doc = self.knowledge.get_document(doc_id)
                if doc:
                    source = doc.metadata.get("source", "unknown")
                    irrelevant_sources[source] += 1
        
        # Adjust source reliability scores
        for source, count in irrelevant_sources.items():
            if count > 10:  # Threshold
                print(f"Lowering reliability score for source: {source}")
                # Update all documents from this source
                self._update_source_reliability(source, factor=0.8)
    
    def get_improvement_insights(self) -> Dict:
        """Get insights on system improvement"""
        if not self.feedback_history:
            return {"message": "No feedback data available"}
        
        recent_feedback = self.feedback_history[-50:]
        
        # Calculate metrics
        avg_relevant = np.mean([
            len(f.get("relevant_docs", [])) 
            for f in recent_feedback
        ])
        
        avg_irrelevant = np.mean([
            len(f.get("irrelevant_docs", [])) 
            for f in recent_feedback
        ])
        
        precision = avg_relevant / (avg_relevant + avg_irrelevant) if (avg_relevant + avg_irrelevant) > 0 else 0
        
        return {
            "total_searches": len(self.feedback_history),
            "recent_precision": precision,
            "avg_relevant_per_search": avg_relevant,
            "avg_irrelevant_per_search": avg_irrelevant,
            "improvement_trend": self._calculate_improvement_trend()
        }
    
    def _calculate_improvement_trend(self) -> str:
        """Calculate if system is improving"""
        if len(self.feedback_history) < 20:
            return "insufficient_data"
        
        # Compare first 10 vs last 10
        first_10 = self.feedback_history[:10]
        last_10 = self.feedback_history[-10:]
        
        first_precision = self._calculate_precision(first_10)
        last_precision = self._calculate_precision(last_10)
        
        if last_precision > first_precision * 1.1:
            return "improving"
        elif last_precision < first_precision * 0.9:
            return "degrading"
        else:
            return "stable"
```

## Conclusion

Quality-Based RAG in PraisonAI provides sophisticated retrieval capabilities that go far beyond simple vector similarity search. By implementing quality scoring, reranking, hybrid search, and advanced chunking strategies, you can build RAG systems that deliver highly relevant and accurate results. The combination of these techniques with continuous improvement through feedback creates adaptive systems that get better over time.

## Next Steps

- Explore [Router Agent Examples](./router-agent-cost-optimization.mdx) for routing queries to specialized RAG systems
- Learn about [Graph Memory](./graph-memory-neo4j.mdx) for relationship-aware retrieval
- Check out [Session Persistence](./session-persistence.mdx) for stateful RAG applications