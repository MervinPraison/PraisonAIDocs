---
title: Multi-Provider Advanced Switching Examples
description: Learn how to implement sophisticated provider switching strategies across multiple AI providers
---

# Multi-Provider Advanced Switching Examples

PraisonAI's multi-provider support enables sophisticated switching strategies between different AI providers, allowing you to optimize for cost, performance, reliability, and capabilities while maintaining a consistent interface.

## Overview

Advanced provider switching enables:
- Automatic failover for high availability
- Dynamic model selection based on task requirements
- Load balancing across providers
- Cost optimization with quality constraints
- Provider-specific capability routing
- A/B testing and gradual rollouts

## Basic Multi-Provider Setup

### Simple Multi-Model Agent

```python
from praisonaiagents import Agent

# Agent with multiple models
multi_model_agent = Agent(
    name="Multi-Model Assistant",
    instructions="You are a helpful assistant that can use different AI models",
    model=[
        "openai/gpt-4o-mini",      # Primary model
        "anthropic/claude-3-haiku", # Fallback model
        "groq/llama-3.3-70b",      # Secondary fallback
    ]
)

# The agent will automatically failover if primary model fails
response = multi_model_agent.start("Explain quantum computing")
```

### Provider Configuration

```python
import os
from praisonaiagents import Agent

# Configure multiple providers
os.environ["OPENAI_API_KEY"] = "your-openai-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"
os.environ["GROQ_API_KEY"] = "your-groq-key"
os.environ["GOOGLE_API_KEY"] = "your-google-key"

# Agent with provider preferences
agent = Agent(
    name="Provider-Aware Assistant",
    model={
        "primary": "openai/gpt-4o",
        "fallbacks": [
            "anthropic/claude-3-5-sonnet",
            "google/gemini-1.5-pro"
        ],
        "budget": "groq/llama-3.3-70b"
    }
)
```

## Advanced Routing Strategies

### 1. Task-Based Routing

```python
from praisonaiagents import RouterAgent, Agent

# Create specialized agents for different tasks
code_expert = Agent(
    name="Code Expert",
    instructions="You excel at code generation and debugging",
    model="deepseek-chat"  # DeepSeek excels at code
)

creative_writer = Agent(
    name="Creative Writer",
    instructions="You excel at creative and narrative tasks",
    model="anthropic/claude-3-5-sonnet"  # Claude excels at creative writing
)

analyst = Agent(
    name="Data Analyst",
    instructions="You excel at data analysis and mathematical tasks",
    model="openai/o1-preview"  # O1 excels at reasoning
)

general_assistant = Agent(
    name="General Assistant",
    instructions="You handle general queries efficiently",
    model="gemini/gemini-1.5-flash"  # Fast and cost-effective
)

# Router that selects appropriate agent based on task
router = RouterAgent(
    name="Task Router",
    instructions="Route tasks to the most appropriate specialist",
    agents=[code_expert, creative_writer, analyst, general_assistant],
    routing_strategy="auto"
)

# Examples of automatic routing
router.start("Write a Python function to sort a list")  # Routes to Code Expert
router.start("Write a short story about time travel")   # Routes to Creative Writer
router.start("Analyze this sales data and find trends") # Routes to Data Analyst
router.start("What's the weather like?")               # Routes to General Assistant
```

### 2. Custom Routing Logic

```python
from praisonaiagents import ModelRouter, ModelProfile, Agent
import re

class CustomRouter(ModelRouter):
    def __init__(self):
        # Define custom model profiles
        profiles = [
            ModelProfile(
                provider="openai",
                name="gpt-4o",
                strengths=["general", "reasoning", "analysis"],
                context_window=128000,
                supports_tools=True,
                supports_streaming=True,
                cost_per_1k_tokens=0.0075
            ),
            ModelProfile(
                provider="anthropic",
                name="claude-3-5-sonnet",
                strengths=["writing", "creativity", "long-form"],
                context_window=200000,
                supports_tools=True,
                supports_streaming=True,
                cost_per_1k_tokens=0.009
            ),
            ModelProfile(
                provider="deepseek",
                name="deepseek-chat",
                strengths=["code", "math", "technical"],
                context_window=64000,
                supports_tools=True,
                supports_streaming=True,
                cost_per_1k_tokens=0.0014
            ),
            ModelProfile(
                provider="groq",
                name="llama-3.3-70b-versatile",
                strengths=["fast", "general", "conversational"],
                context_window=128000,
                supports_tools=True,
                supports_streaming=True,
                cost_per_1k_tokens=0.0008
            )
        ]
        super().__init__(profiles)
    
    def route(self, task_description: str, context: dict = None) -> str:
        """Custom routing logic based on task patterns"""
        task_lower = task_description.lower()
        
        # Code-related tasks
        if any(keyword in task_lower for keyword in ["code", "function", "debug", "program", "script"]):
            return "deepseek-chat"
        
        # Creative writing tasks
        if any(keyword in task_lower for keyword in ["story", "poem", "creative", "narrative", "essay"]):
            return "claude-3-5-sonnet"
        
        # Mathematical or analytical tasks
        if any(keyword in task_lower for keyword in ["calculate", "analyze", "math", "statistics"]):
            return "gpt-4o"
        
        # Quick responses for simple tasks
        if len(task_description) < 50 and "?" in task_description:
            return "llama-3.3-70b-versatile"
        
        # Check context for additional hints
        if context:
            if context.get("requires_tools"):
                # All our models support tools, but GPT-4 is most reliable
                return "gpt-4o"
            
            if context.get("long_context"):
                # Claude has the largest context window
                return "claude-3-5-sonnet"
            
            if context.get("budget_conscious"):
                # Groq Llama is most cost-effective
                return "llama-3.3-70b-versatile"
        
        # Default to GPT-4 for general tasks
        return "gpt-4o"

# Use custom router
custom_router = CustomRouter()

agent = Agent(
    name="Custom Routed Agent",
    instructions="You use custom routing logic for optimal model selection",
    model_router=custom_router
)

# Test different task types
print(agent.start("Write a Python function to implement quicksort"))  # Uses DeepSeek
print(agent.start("Write a haiku about spring"))                      # Uses Claude
print(agent.start("What is 2+2?"))                                   # Uses Llama
```

### 3. Load Balancing Strategy

```python
from praisonaiagents import Agent, ModelRouter
import random
from collections import defaultdict
import time

class LoadBalancingRouter:
    def __init__(self, models: list, weights: dict = None):
        self.models = models
        self.weights = weights or {model: 1.0 for model in models}
        self.request_counts = defaultdict(int)
        self.error_counts = defaultdict(int)
        self.last_reset = time.time()
        self.reset_interval = 3600  # Reset stats hourly
    
    def select_model(self, task: str = None) -> str:
        """Select model based on load balancing strategy"""
        # Reset stats if needed
        if time.time() - self.last_reset > self.reset_interval:
            self.reset_stats()
        
        # Calculate effective weights based on error rates
        effective_weights = {}
        for model in self.models:
            error_rate = self.error_counts[model] / max(self.request_counts[model], 1)
            # Reduce weight for models with high error rates
            effective_weights[model] = self.weights[model] * (1 - error_rate * 0.5)
        
        # Weighted random selection
        total_weight = sum(effective_weights.values())
        if total_weight == 0:
            return random.choice(self.models)
        
        rand = random.uniform(0, total_weight)
        cumulative = 0
        
        for model, weight in effective_weights.items():
            cumulative += weight
            if rand <= cumulative:
                self.request_counts[model] += 1
                return model
        
        return self.models[0]
    
    def record_error(self, model: str):
        """Record an error for a model"""
        self.error_counts[model] += 1
    
    def reset_stats(self):
        """Reset statistics"""
        self.request_counts.clear()
        self.error_counts.clear()
        self.last_reset = time.time()
    
    def get_stats(self):
        """Get current load balancing statistics"""
        stats = {}
        for model in self.models:
            requests = self.request_counts[model]
            errors = self.error_counts[model]
            stats[model] = {
                "requests": requests,
                "errors": errors,
                "error_rate": errors / max(requests, 1),
                "weight": self.weights[model]
            }
        return stats

# Create load-balanced agent
lb_router = LoadBalancingRouter(
    models=[
        "openai/gpt-4o-mini",
        "anthropic/claude-3-haiku",
        "groq/llama-3.3-70b-versatile"
    ],
    weights={
        "openai/gpt-4o-mini": 2.0,        # Higher weight for primary
        "anthropic/claude-3-haiku": 1.5,   # Medium weight
        "groq/llama-3.3-70b-versatile": 1.0  # Lower weight
    }
)

class LoadBalancedAgent(Agent):
    def __init__(self, router: LoadBalancingRouter, **kwargs):
        super().__init__(**kwargs)
        self.router = router
    
    def start(self, message: str):
        """Execute with load balancing"""
        model = self.router.select_model(message)
        self.model = model
        
        try:
            response = super().start(message)
            return response
        except Exception as e:
            # Record error and retry with different model
            self.router.record_error(model)
            
            # Select alternative model
            alternative = self.router.select_model(message)
            if alternative != model:
                self.model = alternative
                return super().start(message)
            else:
                raise e

# Use load-balanced agent
agent = LoadBalancedAgent(
    router=lb_router,
    name="Load Balanced Assistant",
    instructions="You are a helpful assistant"
)

# Simulate multiple requests
for i in range(20):
    response = agent.start(f"Question {i}: What is the capital of France?")
    
# Check load distribution
print("Load Balancing Stats:")
for model, stats in lb_router.get_stats().items():
    print(f"{model}: {stats['requests']} requests, {stats['error_rate']:.2%} error rate")
```

### 4. A/B Testing Strategy

```python
from praisonaiagents import Agent
import random
import json
from datetime import datetime

class ABTestingRouter:
    def __init__(self, experiments: dict):
        """
        experiments = {
            "experiment_name": {
                "models": ["model_a", "model_b"],
                "weights": [0.5, 0.5],  # Traffic split
                "metrics": ["response_time", "user_satisfaction", "cost"]
            }
        }
        """
        self.experiments = experiments
        self.results = defaultdict(lambda: defaultdict(list))
    
    def select_variant(self, experiment_name: str, user_id: str = None) -> str:
        """Select model variant for A/B test"""
        if experiment_name not in self.experiments:
            raise ValueError(f"Unknown experiment: {experiment_name}")
        
        exp = self.experiments[experiment_name]
        
        # Consistent assignment based on user_id if provided
        if user_id:
            # Hash user_id to consistently assign to same variant
            variant_index = hash(user_id) % len(exp["models"])
        else:
            # Random assignment based on weights
            variant_index = random.choices(
                range(len(exp["models"])),
                weights=exp["weights"]
            )[0]
        
        return exp["models"][variant_index]
    
    def record_metric(self, experiment_name: str, model: str, metric: str, value: float):
        """Record metric for analysis"""
        self.results[experiment_name][f"{model}_{metric}"].append(value)
    
    def get_results(self, experiment_name: str) -> dict:
        """Get A/B test results"""
        if experiment_name not in self.results:
            return {}
        
        results = {}
        exp = self.experiments[experiment_name]
        
        for model in exp["models"]:
            model_results = {}
            for metric in exp["metrics"]:
                key = f"{model}_{metric}"
                values = self.results[experiment_name].get(key, [])
                
                if values:
                    model_results[metric] = {
                        "mean": sum(values) / len(values),
                        "min": min(values),
                        "max": max(values),
                        "count": len(values)
                    }
            
            results[model] = model_results
        
        return results

# Set up A/B test
ab_router = ABTestingRouter({
    "response_quality_test": {
        "models": ["openai/gpt-4o", "anthropic/claude-3-5-sonnet"],
        "weights": [0.5, 0.5],
        "metrics": ["quality_score", "response_time", "cost"]
    },
    "speed_test": {
        "models": ["groq/llama-3.3-70b-versatile", "gemini/gemini-1.5-flash"],
        "weights": [0.6, 0.4],  # 60/40 split
        "metrics": ["response_time", "tokens_per_second"]
    }
})

# Create A/B testing agent
class ABTestAgent(Agent):
    def __init__(self, ab_router: ABTestingRouter, experiment: str, **kwargs):
        super().__init__(**kwargs)
        self.ab_router = ab_router
        self.experiment = experiment
    
    def start(self, message: str, user_id: str = None):
        """Execute with A/B testing"""
        # Select variant
        model = self.ab_router.select_variant(self.experiment, user_id)
        self.model = model
        
        # Track timing
        start_time = time.time()
        
        # Execute
        response = super().start(message)
        
        # Record metrics
        response_time = time.time() - start_time
        self.ab_router.record_metric(
            self.experiment, 
            model, 
            "response_time", 
            response_time
        )
        
        # Simulate quality score (in production, this would come from user feedback)
        quality_score = random.uniform(0.7, 1.0)
        self.ab_router.record_metric(
            self.experiment,
            model,
            "quality_score",
            quality_score
        )
        
        return response

# Run A/B test
agent = ABTestAgent(
    ab_router=ab_router,
    experiment="response_quality_test",
    name="A/B Test Assistant",
    instructions="You help users with various tasks"
)

# Simulate user interactions
for i in range(100):
    user_id = f"user_{i % 20}"  # 20 unique users
    response = agent.start("Help me understand machine learning", user_id)

# Analyze results
results = ab_router.get_results("response_quality_test")
print("A/B Test Results:")
for model, metrics in results.items():
    print(f"\n{model}:")
    for metric, stats in metrics.items():
        print(f"  {metric}: {stats['mean']:.3f} (n={stats['count']})")
```

## Provider-Specific Capabilities

### 1. Tool Support Routing

```python
from praisonaiagents import Agent, tool

# Define tools
@tool
def web_search(query: str) -> str:
    """Search the web for information"""
    return f"Search results for: {query}"

@tool
def code_execution(code: str) -> str:
    """Execute Python code"""
    return f"Code output: {eval(code)}"

@tool
def image_generation(prompt: str) -> str:
    """Generate an image from prompt"""
    return f"Image generated: {prompt}"

# Provider capability mapping
PROVIDER_CAPABILITIES = {
    "openai/gpt-4o": ["web_search", "code_execution"],
    "anthropic/claude-3-5-sonnet": ["web_search", "code_execution"],
    "gemini/gemini-1.5-pro": ["web_search"],
    "deepseek-chat": ["code_execution"],
}

class CapabilityRouter:
    def __init__(self, capabilities: dict):
        self.capabilities = capabilities
    
    def find_capable_model(self, required_tools: list) -> str:
        """Find model that supports required tools"""
        for model, supported_tools in self.capabilities.items():
            if all(tool in supported_tools for tool in required_tools):
                return model
        
        # No model supports all tools - return most capable
        best_model = None
        best_score = 0
        
        for model, supported_tools in self.capabilities.items():
            score = sum(1 for tool in required_tools if tool in supported_tools)
            if score > best_score:
                best_score = score
                best_model = model
        
        return best_model

# Route based on tool requirements
capability_router = CapabilityRouter(PROVIDER_CAPABILITIES)

def create_agent_for_task(task: str, required_tools: list):
    """Create agent with appropriate model for tools"""
    model = capability_router.find_capable_model(
        [tool.__name__ for tool in required_tools]
    )
    
    return Agent(
        name="Capability-Aware Agent",
        instructions=f"You help with: {task}",
        model=model,
        tools=required_tools
    )

# Examples
search_agent = create_agent_for_task(
    "Web research",
    [web_search]
)  # Can use any model

code_agent = create_agent_for_task(
    "Code assistance", 
    [code_execution]
)  # Will use OpenAI, Anthropic, or DeepSeek

multi_tool_agent = create_agent_for_task(
    "Complex tasks",
    [web_search, code_execution]
)  # Will use OpenAI or Anthropic
```

### 2. Context Window Routing

```python
from praisonaiagents import Agent
import tiktoken

class ContextAwareRouter:
    def __init__(self):
        self.model_context_windows = {
            "gpt-4o-mini": 128000,
            "gpt-4o": 128000,
            "claude-3-5-sonnet": 200000,
            "gemini-1.5-pro": 2000000,  # 2M tokens
            "llama-3.3-70b-versatile": 128000,
            "deepseek-chat": 64000
        }
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count"""
        return len(self.tokenizer.encode(text))
    
    def select_model_for_context(self, context: str, min_headroom: int = 4000) -> str:
        """Select model based on context size"""
        context_tokens = self.estimate_tokens(context)
        required_tokens = context_tokens + min_headroom
        
        # Find models that can handle the context
        capable_models = [
            model for model, window in self.model_context_windows.items()
            if window >= required_tokens
        ]
        
        if not capable_models:
            # Return model with largest context window
            return max(self.model_context_windows.items(), key=lambda x: x[1])[0]
        
        # Return cheapest capable model (simplified)
        cost_priority = [
            "llama-3.3-70b-versatile",
            "gpt-4o-mini",
            "deepseek-chat",
            "gpt-4o",
            "claude-3-5-sonnet",
            "gemini-1.5-pro"
        ]
        
        for model in cost_priority:
            if model in capable_models:
                return model
        
        return capable_models[0]

# Use context-aware routing
context_router = ContextAwareRouter()

def process_document(document: str):
    """Process document with appropriate model"""
    model = context_router.select_model_for_context(document)
    
    print(f"Document size: {context_router.estimate_tokens(document)} tokens")
    print(f"Selected model: {model}")
    
    agent = Agent(
        name="Document Processor",
        instructions="You analyze and summarize documents",
        model=model
    )
    
    return agent.start(f"Analyze this document: {document}")

# Example with different document sizes
small_doc = "This is a small document." * 100
large_doc = "This is a large document." * 10000
huge_doc = "This is a huge document." * 50000

process_document(small_doc)   # Uses cheap model
process_document(large_doc)   # Uses model with larger context
process_document(huge_doc)    # Uses Gemini with 2M context
```

## Reliability and Failover

### 1. Circuit Breaker Pattern

```python
from praisonaiagents import Agent
import time
from enum import Enum

class CircuitState(Enum):
    CLOSED = "closed"  # Normal operation
    OPEN = "open"      # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing recovery

class CircuitBreaker:
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    def call(self, func, *args, **kwargs):
        """Execute function with circuit breaker"""
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except self.expected_exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        return (
            self.last_failure_time and
            time.time() - self.last_failure_time >= self.recovery_timeout
        )
    
    def _on_success(self):
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN

# Multi-provider agent with circuit breakers
class ResilientMultiProviderAgent:
    def __init__(self, name: str, instructions: str, models: list):
        self.name = name
        self.instructions = instructions
        self.models = models
        self.circuit_breakers = {
            model: CircuitBreaker() for model in models
        }
    
    def start(self, message: str):
        """Execute with circuit breaker failover"""
        errors = []
        
        for model in self.models:
            try:
                agent = Agent(
                    name=self.name,
                    instructions=self.instructions,
                    model=model
                )
                
                # Try with circuit breaker
                response = self.circuit_breakers[model].call(
                    agent.start,
                    message
                )
                
                print(f"Success with {model}")
                return response
                
            except Exception as e:
                errors.append(f"{model}: {str(e)}")
                print(f"Failed with {model}: {e}")
                continue
        
        # All models failed
        raise Exception(f"All models failed: {'; '.join(errors)}")

# Use resilient agent
resilient_agent = ResilientMultiProviderAgent(
    name="Resilient Assistant",
    instructions="You are a helpful assistant",
    models=[
        "openai/gpt-4o",
        "anthropic/claude-3-5-sonnet",
        "groq/llama-3.3-70b-versatile"
    ]
)

# Will automatically failover if providers fail
response = resilient_agent.start("What is the meaning of life?")
```

### 2. Retry with Exponential Backoff

```python
import asyncio
from typing import Optional

class RetryConfig:
    def __init__(
        self,
        max_retries: int = 3,
        initial_delay: float = 1.0,
        max_delay: float = 60.0,
        exponential_base: float = 2.0,
        jitter: bool = True
    ):
        self.max_retries = max_retries
        self.initial_delay = initial_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter

async def retry_with_backoff(
    func,
    config: RetryConfig,
    *args,
    **kwargs
):
    """Execute function with retry and exponential backoff"""
    last_exception = None
    
    for attempt in range(config.max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            
            if attempt == config.max_retries - 1:
                break
            
            # Calculate delay
            delay = min(
                config.initial_delay * (config.exponential_base ** attempt),
                config.max_delay
            )
            
            # Add jitter
            if config.jitter:
                delay *= (0.5 + random.random())
            
            print(f"Attempt {attempt + 1} failed, retrying in {delay:.1f}s...")
            await asyncio.sleep(delay)
    
    raise last_exception

# Agent with retry logic
class RetryableAgent(Agent):
    def __init__(self, retry_config: RetryConfig = None, **kwargs):
        super().__init__(**kwargs)
        self.retry_config = retry_config or RetryConfig()
    
    async def start_async(self, message: str):
        """Execute with retry logic"""
        return await retry_with_backoff(
            super().start_async,
            self.retry_config,
            message
        )

# Use with multiple providers
models = [
    "openai/gpt-4o",
    "anthropic/claude-3-5-sonnet",
    "groq/llama-3.3-70b-versatile"
]

agents = [
    RetryableAgent(
        name=f"Agent-{model}",
        model=model,
        retry_config=RetryConfig(max_retries=3)
    )
    for model in models
]
```

## Performance Monitoring

### 1. Provider Performance Tracking

```python
from dataclasses import dataclass
from typing import Dict, List
import statistics

@dataclass
class PerformanceMetrics:
    response_times: List[float]
    error_count: int = 0
    total_requests: int = 0
    
    @property
    def avg_response_time(self) -> float:
        return statistics.mean(self.response_times) if self.response_times else 0
    
    @property
    def p95_response_time(self) -> float:
        if not self.response_times:
            return 0
        sorted_times = sorted(self.response_times)
        index = int(len(sorted_times) * 0.95)
        return sorted_times[index]
    
    @property
    def error_rate(self) -> float:
        return self.error_count / max(self.total_requests, 1)

class PerformanceMonitor:
    def __init__(self):
        self.metrics: Dict[str, PerformanceMetrics] = defaultdict(
            lambda: PerformanceMetrics(response_times=[])
        )
    
    def record_request(self, model: str, response_time: float, success: bool):
        """Record request metrics"""
        metrics = self.metrics[model]
        metrics.total_requests += 1
        
        if success:
            metrics.response_times.append(response_time)
            # Keep only last 1000 response times
            if len(metrics.response_times) > 1000:
                metrics.response_times.pop(0)
        else:
            metrics.error_count += 1
    
    def get_report(self) -> dict:
        """Get performance report for all models"""
        report = {}
        for model, metrics in self.metrics.items():
            report[model] = {
                "avg_response_time": f"{metrics.avg_response_time:.3f}s",
                "p95_response_time": f"{metrics.p95_response_time:.3f}s",
                "error_rate": f"{metrics.error_rate:.2%}",
                "total_requests": metrics.total_requests
            }
        return report
    
    def recommend_model(self) -> Optional[str]:
        """Recommend best performing model"""
        if not self.metrics:
            return None
        
        # Score models based on performance
        scores = {}
        for model, metrics in self.metrics.items():
            # Lower score is better
            score = (
                metrics.avg_response_time * 0.5 +
                metrics.p95_response_time * 0.3 +
                metrics.error_rate * 100 * 0.2
            )
            scores[model] = score
        
        return min(scores.items(), key=lambda x: x[1])[0]

# Agent with performance monitoring
monitor = PerformanceMonitor()

class MonitoredAgent(Agent):
    def __init__(self, monitor: PerformanceMonitor, **kwargs):
        super().__init__(**kwargs)
        self.monitor = monitor
    
    def start(self, message: str):
        """Execute with performance monitoring"""
        start_time = time.time()
        success = False
        
        try:
            response = super().start(message)
            success = True
            return response
        finally:
            response_time = time.time() - start_time
            self.monitor.record_request(self.model, response_time, success)

# Test multiple providers
models = ["openai/gpt-4o-mini", "anthropic/claude-3-haiku", "groq/llama-3.3-70b"]

for _ in range(50):
    for model in models:
        agent = MonitoredAgent(
            monitor=monitor,
            name="Test Agent",
            model=model
        )
        try:
            agent.start("Quick test query")
        except:
            pass

# Get performance report
print("Performance Report:")
for model, metrics in monitor.get_report().items():
    print(f"\n{model}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value}")

print(f"\nRecommended model: {monitor.recommend_model()}")
```

## Best Practices

### 1. Provider Abstraction Layer

```python
from abc import ABC, abstractmethod

class ProviderAdapter(ABC):
    """Abstract base for provider adapters"""
    
    @abstractmethod
    def complete(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    def stream(self, prompt: str, **kwargs):
        pass
    
    @abstractmethod
    def supports_tools(self) -> bool:
        pass

class OpenAIAdapter(ProviderAdapter):
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def complete(self, prompt: str, **kwargs) -> str:
        # OpenAI-specific implementation
        pass
    
    def stream(self, prompt: str, **kwargs):
        # OpenAI streaming implementation
        pass
    
    def supports_tools(self) -> bool:
        return True

class AnthropicAdapter(ProviderAdapter):
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def complete(self, prompt: str, **kwargs) -> str:
        # Anthropic-specific implementation
        pass
    
    def stream(self, prompt: str, **kwargs):
        # Anthropic streaming implementation
        pass
    
    def supports_tools(self) -> bool:
        return True

# Unified interface
class UnifiedProvider:
    def __init__(self):
        self.adapters = {
            "openai": OpenAIAdapter(os.getenv("OPENAI_API_KEY")),
            "anthropic": AnthropicAdapter(os.getenv("ANTHROPIC_API_KEY")),
            # Add more providers
        }
    
    def get_adapter(self, provider: str) -> ProviderAdapter:
        if provider not in self.adapters:
            raise ValueError(f"Unknown provider: {provider}")
        return self.adapters[provider]
```

### 2. Cost and Usage Tracking

```python
class UsageTracker:
    def __init__(self):
        self.usage = defaultdict(lambda: {
            "requests": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "total_cost": 0.0
        })
        
        # Cost per 1K tokens (simplified)
        self.costs = {
            "gpt-4o": {"input": 0.01, "output": 0.03},
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "claude-3-5-sonnet": {"input": 0.003, "output": 0.015},
            "llama-3.3-70b": {"input": 0.0008, "output": 0.0008}
        }
    
    def track_usage(self, model: str, input_tokens: int, output_tokens: int):
        """Track token usage and calculate cost"""
        self.usage[model]["requests"] += 1
        self.usage[model]["input_tokens"] += input_tokens
        self.usage[model]["output_tokens"] += output_tokens
        
        # Calculate cost
        if model in self.costs:
            input_cost = (input_tokens / 1000) * self.costs[model]["input"]
            output_cost = (output_tokens / 1000) * self.costs[model]["output"]
            self.usage[model]["total_cost"] += input_cost + output_cost
    
    def get_summary(self) -> dict:
        """Get usage summary"""
        summary = {
            "by_model": dict(self.usage),
            "total_cost": sum(u["total_cost"] for u in self.usage.values()),
            "total_requests": sum(u["requests"] for u in self.usage.values())
        }
        return summary

# Use with agents
usage_tracker = UsageTracker()

def track_agent_usage(agent: Agent, usage_tracker: UsageTracker):
    """Decorator to track agent usage"""
    original_start = agent.start
    
    def tracked_start(message: str):
        # Estimate tokens (simplified)
        input_tokens = len(message.split()) * 1.3
        
        response = original_start(message)
        
        output_tokens = len(response.split()) * 1.3
        
        usage_tracker.track_usage(
            agent.model,
            int(input_tokens),
            int(output_tokens)
        )
        
        return response
    
    agent.start = tracked_start
    return agent
```

## Conclusion

Multi-provider support in PraisonAI enables sophisticated strategies for optimizing AI applications across dimensions of cost, performance, reliability, and capabilities. By implementing advanced routing strategies, failover mechanisms, and performance monitoring, you can build robust AI systems that leverage the best of each provider while maintaining high availability and cost efficiency.

## Next Steps

- Explore [Session Persistence](./session-persistence.mdx) for stateful multi-provider applications
- Learn about [Quality-Based RAG](./quality-based-rag.mdx) for provider-optimized retrieval
- Check out [Router Agent Examples](./router-agent-cost-optimization.mdx) for cost optimization strategies